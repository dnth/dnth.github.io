<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Image-Classification on Dickson Neoh - Personal Portfolio</title><link>https://dicksonneoh.com/categories/image-classification/</link><description>Recent content in Image-Classification on Dickson Neoh - Personal Portfolio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 09 Sep 2024 09:00:00 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/categories/image-classification/index.xml" rel="self" type="application/rss+xml"/><item><title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime &amp; Optimizations</title><link>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</link><pubDate>Mon, 09 Sep 2024 09:00:00 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</guid><description>üöÄ Motivation Having real time inference is crucial for many computer vision applications. In some domain, a 1-second delay in inference could mean life or death.
Imagine you&amp;rsquo;re sitting in a self-driving car and the car takes one full second to detect an oncoming truck.
Just one second too late, and you could end up in the clouds üëºüëºüëº
Or if you&amp;rsquo;re lucky, on the ground.
I hope that shows you how crucial this problem is.</description></item><item><title>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android &amp; iOS</title><link>https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/</link><pubDate>Thu, 16 Mar 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/</guid><description>üåü Motivation For many data scientist (including myself), we pride ourselves in training a model, seeing the loss graph go down, and claim victory when the test set accuracy reaches 99.99235%.
Why not?
This is the after all the juiciest part of the job. &amp;ldquo;Solving&amp;rdquo; one dataset after another, it may seem like anything around you can be conquered with a simple model.fit.
That was me two years ago.
The naive version of me thought that was all about it with machine learning (ML).</description></item><item><title>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</title><link>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</link><pubDate>Tue, 07 Feb 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</guid><description>üî• Motivation With various high-level libraries like Keras, Transformer, and Fastai, the barrier to training SOTA models has never been lower.
On top of that with platforms like Google Colab and Kaggle, pretty much anyone can train a reasonably good model using an old laptop or even a mobile phone (with some patience).
The question is no longer &amp;ldquo;can we train a SOTA model?&amp;rdquo;, but &amp;ldquo;what happens after that?&amp;rdquo;
Unfortunately, after getting the model trained, most people wash their hands off at this point claiming their model works.</description></item><item><title>fastdup: A Powerful Tool to Manage, Clean &amp; Curate Visual Data at Scale on Your CPU - For Free.</title><link>https://dicksonneoh.com/portfolio/fastdup_manage_clean_curate/</link><pubDate>Tue, 03 Jan 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/fastdup_manage_clean_curate/</guid><description>‚è≥ Last Updated: March 27, 2023.
‚úÖ Motivation As a data scientist, you might be tempted to jump into modeling as soon as you can. I mean, that&amp;rsquo;s the fun part, right?
But trust me, if you skip straight to modeling without taking the time to really understand the problem and analyze the data, you&amp;rsquo;re setting yourself up for failure.
I&amp;rsquo;ve been there.
You might feel like a superstar, but you&amp;rsquo;ll have with a model that doesn&amp;rsquo;t work ü§¶‚Äç‚ôÇÔ∏è.</description></item></channel></rss>