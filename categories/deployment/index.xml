<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deployment on Dickson Neoh - Personal Portfolio</title><link>https://dicksonneoh.com/categories/deployment/</link><description>Recent content in Deployment on Dickson Neoh - Personal Portfolio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 30 Sep 2024 09:00:00 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/categories/deployment/index.xml" rel="self" type="application/rss+xml"/><item><title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime &amp; Optimizations</title><link>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</link><pubDate>Mon, 30 Sep 2024 09:00:00 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</guid><description>&lt;h3 id="-motivation">ðŸš€ Motivation&lt;/h3>
&lt;p>Having real-time inference is crucial for computer vision applications.
In some domains, a 1-second delay in inference could mean life or death.&lt;/p>
&lt;p>Imagine sitting in a self-driving car and the car takes &lt;strong>one full second&lt;/strong> to detect an oncoming speeding truck.&lt;/p>
&lt;p>Just one second too late, and you could end up in the clouds ðŸ‘¼ðŸ‘¼ðŸ‘¼&lt;/p>
&lt;p>Or if you&amp;rsquo;re lucky, you get a very up-close view of the pavement.&lt;/p></description></item><item><title>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android &amp; iOS</title><link>https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/</link><pubDate>Thu, 16 Mar 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/</guid><description>&lt;h3 id="-motivation">ðŸŒŸ Motivation&lt;/h3>
&lt;p>For many data scientist (including myself), we pride ourselves in training a model, seeing the loss graph go down, and claim victory when the test set accuracy reaches 99.99235%.&lt;/p>
&lt;p>Why not?&lt;/p>
&lt;p>This is the after all the juiciest part of the job. &amp;ldquo;Solving&amp;rdquo; one dataset after another, it may seem like anything around you can be &lt;em>conquered&lt;/em> with a simple &lt;code>model.fit&lt;/code>.&lt;/p>
&lt;p>That was me two years ago.&lt;/p>
&lt;p>The naive version of me thought that was all about it with machine learning (ML).
As long as we have a dataset, ML is the way to go.&lt;/p></description></item><item><title>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</title><link>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</link><pubDate>Tue, 07 Feb 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</guid><description>&lt;h3 id="-motivation">ðŸ”¥ Motivation&lt;/h3>
&lt;!-- You finally got into a Kaggle competition. You found a *getting-started notebook* written by a Kaggle Grandmaster and immediately trained a state-of-the-art (SOTA) image classification model.

After some fiddling, you found yourself in the leaderboard topping the charts with **99.9851247\% accuracy** on the test set ðŸ˜Ž!

Proud of your achievement you reward yourself to some rest and a good night's sleep. 
And tomorrow it's time to move on to the next dataset (again). -->
&lt;!-- And then..





 
 
 
 
 
 &lt;figure>
 &lt;a href="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_sleep.jpg" class="image-popup">
 &lt;img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_sleep.jpg"
 
 
 
 style="max-width: 100%; height: auto;"/>
 &lt;/a>
 
 &lt;/figure>
 
 -->
&lt;!-- I hope this doesn't keep you awake at night as it did for me. -->
&lt;p>With various high-level libraries like &lt;a href="https://keras.io/" target="_blank" rel="nofollow noopener noreferrer">Keras&lt;/a>, &lt;a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="nofollow noopener noreferrer">Transformer&lt;/a>, and &lt;a href="https://www.fast.ai/" target="_blank" rel="nofollow noopener noreferrer">Fastai&lt;/a>, the barrier to training SOTA models has never been lower.&lt;/p></description></item><item><title>Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU</title><link>https://dicksonneoh.com/portfolio/supercharging_yolov5_180_fps_cpu/</link><pubDate>Tue, 07 Jun 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/supercharging_yolov5_180_fps_cpu/</guid><description>&lt;h3 id="-motivation">ðŸ”¥ Motivation&lt;/h3>
&lt;p>After months of searching, you&amp;rsquo;ve finally found &lt;em>the one&lt;/em>.&lt;/p>
&lt;p>The one object detection library that just works.
No installation hassle, no package version mismatch, and no &lt;code>CUDA&lt;/code> errors.&lt;/p>
&lt;p>I&amp;rsquo;m talking about the amazingly engineered &lt;a href="https://github.com/ultralytics/yolov5" target="_blank" rel="nofollow noopener noreferrer">YOLOv5&lt;/a> object detection library by &lt;a href="https://ultralytics.com/yolov5" target="_blank" rel="nofollow noopener noreferrer">Ultralytics&lt;/a>.&lt;/p>
&lt;p>Elated, you quickly find an interesting dataset from &lt;a href="https://roboflow.com/" target="_blank" rel="nofollow noopener noreferrer">Roboflow&lt;/a> and finally trained a state-of-the-art (SOTA) YOLOv5 model to detect firearms from image streams.&lt;/p>
&lt;p>You ran through a quick checklist &amp;ndash;&lt;/p></description></item><item><title>Deploying GPT-J Models on a Telegram Bot with Hugging Face Hub - For Free</title><link>https://dicksonneoh.com/portfolio/deploy_gpt_hf_models_on_telegram/</link><pubDate>Thu, 19 May 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/deploy_gpt_hf_models_on_telegram/</guid><description>&lt;h3 id="-motivation">ðŸ’¥ Motivation&lt;/h3>
&lt;style type="text/css">
 .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
 p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
 0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
 .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
 .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
 .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
 .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
 img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
 svg{top:0.125em;position:relative}&lt;/style>
 &lt;div>&lt;svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
 &lt;symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
 &lt;/symbol>
 &lt;symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
 &lt;/symbol>
 &lt;/svg>&lt;/div>&lt;div class="notice tip" >
 &lt;p class="first notice-title">&lt;span class="icon-notice baseline">&lt;svg>
 &lt;use href="#tip-notice">&lt;/use>
 &lt;/svg>&lt;/span>tip&lt;/p></description></item><item><title>Squeezing the Best Performance Out of YOLOX with Weights and Biases</title><link>https://dicksonneoh.com/portfolio/comparing_yolox_models_weights_and_biases/</link><pubDate>Wed, 11 May 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/comparing_yolox_models_weights_and_biases/</guid><description>&lt;h3 id="-motivation">ðŸ”Ž Motivation&lt;/h3>
&lt;style type="text/css">
 .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
 p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
 0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
 .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
 .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
 .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
 .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
 img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
 svg{top:0.125em;position:relative}&lt;/style>
 &lt;div>&lt;svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
 &lt;symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
 &lt;/symbol>
 &lt;symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
 &lt;/symbol>
 &lt;/svg>&lt;/div>&lt;div class="notice tip" >
 &lt;p class="first notice-title">&lt;span class="icon-notice baseline">&lt;svg>
 &lt;use href="#tip-notice">&lt;/use>
 &lt;/svg>&lt;/span>tip&lt;/p></description></item><item><title>Faster than GPU: How to 10x your Object Detection Model and Deploy on CPU at 50+ FPS</title><link>https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</link><pubDate>Sat, 30 Apr 2022 15:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</guid><description>&lt;h3 id="-motivation">ðŸš¦ Motivation&lt;/h3>
&lt;style type="text/css">
 .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
 p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
 0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
 .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
 .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
 .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
 .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
 img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
 svg{top:0.125em;position:relative}&lt;/style>
 &lt;div>&lt;svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
 &lt;symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
 &lt;/symbol>
 &lt;symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
 &lt;/symbol>
 &lt;/svg>&lt;/div>&lt;div class="notice tip" >
 &lt;p class="first notice-title">&lt;span class="icon-notice baseline">&lt;svg>
 &lt;use href="#tip-notice">&lt;/use>
 &lt;/svg>&lt;/span>tip&lt;/p></description></item><item><title>How to Deploy Object Detection Models on Android with Flutter</title><link>https://dicksonneoh.com/portfolio/how_to_deploy_od_models_on_android_with_flutter/</link><pubDate>Sun, 17 Apr 2022 15:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/how_to_deploy_od_models_on_android_with_flutter/</guid><description>&lt;h3 id="-deployment-where-ml-models-go-to-die">ðŸš‘ Deployment: Where ML models go to die&lt;/h3>
&lt;p>In this post, I will outline the basic steps to deploy ML models onto lightweight mobile devices &lt;strong>easily, quickly and for free&lt;/strong>.&lt;/p>
&lt;style type="text/css">
 .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
 p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
 0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
 .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
 .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
 .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
 .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
 img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
 svg{top:0.125em;position:relative}&lt;/style>
 &lt;div>&lt;svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
 &lt;symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
 &lt;/symbol>
 &lt;symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
 &lt;/symbol>
 &lt;symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
 &lt;path
 d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
 &lt;/symbol>
 &lt;/svg>&lt;/div>&lt;div class="notice tip" >
 &lt;p class="first notice-title">&lt;span class="icon-notice baseline">&lt;svg>
 &lt;use href="#tip-notice">&lt;/use>
 &lt;/svg>&lt;/span>tip&lt;/p></description></item><item><title>Deploying Object Detection Models on Hugging Face Spaces</title><link>https://dicksonneoh.com/portfolio/deploy_icevision_models_on_huggingface_spaces/</link><pubDate>Thu, 17 Feb 2022 13:42:56 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/deploy_icevision_models_on_huggingface_spaces/</guid><description>&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>So, youâ€™ve trained a deep learning model that can detect objects from images.
Next, how can you share the awesomeness of your model with the rest of the world?
You might be a PhD student trying to get some ideas from your peers or supervisors, or a startup founder who wishes to share a minimum viable product to your clients for feedback.
But, at the same time you don&amp;rsquo;t wish to go through the hassle of dealing with MLOps.
This blog post is for you. In this post I will walk you through how to deploy your model and share them to the world for free!&lt;/p></description></item></channel></rss>