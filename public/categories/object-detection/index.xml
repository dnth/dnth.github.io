<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Object-Detection on Dickson Neoh - Personal Portfolio</title>
    <link>http://localhost:1313/categories/object-detection/</link>
    <description>Recent content in Object-Detection on Dickson Neoh - Personal Portfolio</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jun 2022 11:00:15 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/object-detection/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU</title>
      <link>http://localhost:1313/portfolio/supercharging_yolov5_180_fps_cpu/</link>
      <pubDate>Tue, 07 Jun 2022 11:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/supercharging_yolov5_180_fps_cpu/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;🔥 Motivation&lt;/h3&gt;&#xA;&lt;p&gt;After months of searching, you&amp;rsquo;ve finally found &lt;em&gt;the one&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The one object detection library that just works.&#xA;No installation hassle, no package version mismatch, and no &lt;code&gt;CUDA&lt;/code&gt; errors.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;m talking about the amazingly engineered &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34; target=&#34;_blank&#34; rel=&#34;nofollow noopener noreferrer&#34;&gt;YOLOv5&lt;/a&gt; object detection library by &lt;a href=&#34;https://ultralytics.com/yolov5&#34; target=&#34;_blank&#34; rel=&#34;nofollow noopener noreferrer&#34;&gt;Ultralytics&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Elated, you quickly find an interesting dataset from &lt;a href=&#34;https://roboflow.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow noopener noreferrer&#34;&gt;Roboflow&lt;/a&gt; and finally trained a state-of-the-art (SOTA) YOLOv5 model to detect firearms from image streams.&lt;/p&gt;&#xA;&lt;p&gt;You ran through a quick checklist &amp;ndash;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Squeezing the Best Performance Out of YOLOX with Weights and Biases</title>
      <link>http://localhost:1313/portfolio/comparing_yolox_models_weights_and_biases/</link>
      <pubDate>Wed, 11 May 2022 11:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/comparing_yolox_models_weights_and_biases/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;🔎 Motivation&lt;/h3&gt;&#xA;&lt;style type=&#34;text/css&#34;&gt;&#xA;    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice&#xA;    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0&#xA;    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning&#xA;    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info&#xA;    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note&#xA;    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip&#xA;    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice&#xA;    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline&#xA;    svg{top:0.125em;position:relative}&lt;/style&gt;&#xA;    &lt;div&gt;&lt;svg width=&#34;0&#34; height=&#34;0&#34; display=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&#xA;            &lt;symbol id=&#34;tip-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;note-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;warning-notice&#34; viewBox=&#34;0 0 576 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;info-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;        &lt;/svg&gt;&lt;/div&gt;&lt;div class=&#34;notice tip&#34; &gt;&#xA;        &lt;p class=&#34;first notice-title&#34;&gt;&lt;span class=&#34;icon-notice baseline&#34;&gt;&lt;svg&gt;&#xA;                    &lt;use href=&#34;#tip-notice&#34;&gt;&lt;/use&gt;&#xA;                &lt;/svg&gt;&lt;/span&gt;tip&lt;/p&gt;</description>
    </item>
    <item>
      <title>Faster than GPU: How to 10x your Object Detection Model and Deploy on CPU at 50&#43; FPS</title>
      <link>http://localhost:1313/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</link>
      <pubDate>Sat, 30 Apr 2022 15:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;🚦 Motivation&lt;/h3&gt;&#xA;&lt;style type=&#34;text/css&#34;&gt;&#xA;    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice&#xA;    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0&#xA;    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning&#xA;    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info&#xA;    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note&#xA;    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip&#xA;    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice&#xA;    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline&#xA;    svg{top:0.125em;position:relative}&lt;/style&gt;&#xA;    &lt;div&gt;&lt;svg width=&#34;0&#34; height=&#34;0&#34; display=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&#xA;            &lt;symbol id=&#34;tip-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;note-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;warning-notice&#34; viewBox=&#34;0 0 576 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;info-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;        &lt;/svg&gt;&lt;/div&gt;&lt;div class=&#34;notice tip&#34; &gt;&#xA;        &lt;p class=&#34;first notice-title&#34;&gt;&lt;span class=&#34;icon-notice baseline&#34;&gt;&lt;svg&gt;&#xA;                    &lt;use href=&#34;#tip-notice&#34;&gt;&lt;/use&gt;&#xA;                &lt;/svg&gt;&lt;/span&gt;tip&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Deploy Object Detection Models on Android with Flutter</title>
      <link>http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/</link>
      <pubDate>Sun, 17 Apr 2022 15:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/</guid>
      <description>&lt;h3 id=&#34;-deployment-where-ml-models-go-to-die&#34;&gt;🚑 Deployment: Where ML models go to die&lt;/h3&gt;&#xA;&lt;p&gt;In this post, I will outline the basic steps to deploy ML models onto lightweight mobile devices &lt;strong&gt;easily, quickly and for free&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;style type=&#34;text/css&#34;&gt;&#xA;    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice&#xA;    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0&#xA;    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning&#xA;    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info&#xA;    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note&#xA;    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip&#xA;    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice&#xA;    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline&#xA;    svg{top:0.125em;position:relative}&lt;/style&gt;&#xA;    &lt;div&gt;&lt;svg width=&#34;0&#34; height=&#34;0&#34; display=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&#xA;            &lt;symbol id=&#34;tip-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;note-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;warning-notice&#34; viewBox=&#34;0 0 576 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;info-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;        &lt;/svg&gt;&lt;/div&gt;&lt;div class=&#34;notice tip&#34; &gt;&#xA;        &lt;p class=&#34;first notice-title&#34;&gt;&lt;span class=&#34;icon-notice baseline&#34;&gt;&lt;svg&gt;&#xA;                    &lt;use href=&#34;#tip-notice&#34;&gt;&lt;/use&gt;&#xA;                &lt;/svg&gt;&lt;/span&gt;tip&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</title>
      <link>http://localhost:1313/portfolio/training_dl_model_for_cell_counting/</link>
      <pubDate>Mon, 11 Apr 2022 15:07:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/training_dl_model_for_cell_counting/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;🕶️ Motivation&lt;/h3&gt;&#xA;&lt;p&gt;Many biology and medical procedures involve counting cells from images taken with a microscope.&#xA;Counting cells reveals the concentration of bacteria and viruses and gives vital information on the progress of a disease.&lt;/p&gt;&#xA;&lt;p&gt;To accomplish the counting, researchers painstakingly count the cells by hand with the assistance of a device called &lt;a href=&#34;https://www.youtube.com/watch?v=WWS9sZbGj6A&amp;amp;ab_channel=ThermoFisherScientific&#34; target=&#34;_blank&#34; rel=&#34;nofollow noopener noreferrer&#34;&gt;hemocytometer&lt;/a&gt;.&#xA;This process is repetitive, tedious, and prone to errors.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;What if we could automate the counting by using an intelligent deep learning algorithm instead?&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying Object Detection Models on Hugging Face Spaces</title>
      <link>http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/</link>
      <pubDate>Thu, 17 Feb 2022 13:42:56 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;So, you’ve trained a deep learning model that can detect objects from images.&#xA;Next, how can you share the awesomeness of your model with the rest of the world?&#xA;You might be a PhD student trying to get some ideas from your peers or supervisors, or a startup founder who wishes to share a minimum viable product to your clients for feedback.&#xA;But, at the same time you don&amp;rsquo;t wish to go through the hassle of dealing with MLOps.&#xA;This blog post is for you. In this post I will walk you through how to deploy your model and share them to the world for free!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
