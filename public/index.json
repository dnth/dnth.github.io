


  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

{"items":[{"content":"üëã Introduction On October 28th, 2024 I made it to GitHub Trending! This wasn\u0026rsquo;t something I expected to happen, and I\u0026rsquo;m still in disbelief.\nGitHub Trending Developer for 28th October 2024 But after all the dopamine rush, I\u0026rsquo;m back to reality and I want to share my journey on how I made it.\nThis is mostly a note to myself and I hope it can help you too.\nüöÄ The Project I created an open source repository called x.infer.\nIt\u0026rsquo;s a framework agnostic computer vision inference library. I created this to solve my problem of using difference computer vision frameworks without the hassle of rewriting the inference code.\nCheck out the project here dnth/x.infer - - Star With x.infer you can load any model from different frameworks and use them with a unified API.\nimport xinfer model = xinfer.create_model(\u0026#34;vikhyatk/moondream2\u0026#34;) model.infer(image, prompt) # Run single inference model.infer_batch(images, prompts) # Run batch inference model.launch_gradio() # Launch Gradio interface It already has support for many popular models and frameworks like Transformers, vLLM, Ollama, Ultralytics, etc. Combined it\u0026rsquo;s over 1000 models that you can use with the above 4 lines of code.\nüõ£Ô∏è The Road to Trending First a disclaimer, I think that building a project with the goal of getting it trending is NOT a good starting point.\nAnchoring your project on an extrinsic reward (getting trending) will not make you happier in the long run and you\u0026rsquo;ll likely to give up when things are tough.\nInstead, start with a goal that is much bigger.\nüí° Start with a goal to solve a problem, learn something, or build something that you\u0026rsquo;re passionate about. This will push you to work on your project for way longer. Now let\u0026rsquo;s talk about how I did it.\nüéØ Solving a Pain Point The first step to getting your project trending is to solve a pain point.\nThis is not something that you\u0026rsquo;ll know from the get-go. Stick around and you\u0026rsquo;ll start to notice some patterns. And also simple solutions for the problem.\nFor me, one pattern emerged, there are many frameworks, and each framework has its own way of loading the model, inference, and post-processing, etc. Every time I change framework, I have to rewrite the inference code. This is really annoying.\nPutting this into a single unified API would be super useful.\nüìù The README File This is probably the most important part of the project aside from solving a pain point.\nNobody would know about what pain point you\u0026rsquo;re solving if you don\u0026rsquo;t tell them in a way that\u0026rsquo;s easy to understand.\nI spent a good amount of time writing the README file. I wanted to make sure that it\u0026rsquo;s easy to understand and that it showcases the project\u0026rsquo;s value proposition.\nAt the very top, I included 4 things:\nShields.io badges to showcase the project\u0026rsquo;s popularity and features. Simple logo and a short description of what the project does. A GIF to showcase the project\u0026rsquo;s value proposition. Quick links. This is the first thing people see when they visit the project. It\u0026rsquo;s important to make it good. Like it or not, people judge a book by its cover. So make it good.\nThis also conveys that you\u0026rsquo;re serious about your project.\nüåü Key Features Most people are busy, don\u0026rsquo;t make them read a long paragraph to convey something simple. Some better alternatives.\nBullet points GIFs Videos For example, I used a GIF to showcase the project\u0026rsquo;s value proposition.\nAnd then follow up with a bullet point list to showcase the project\u0026rsquo;s key features.\n‚úÖ Run inference with \u0026gt;1000+ models in 3 lines of code.\n‚úÖ List and search models interactively.\n‚úÖ Launch a Gradio interface to interact with a model.\n‚úÖ Serve model as a REST API endpoint with Ray Serve and FastAPI.\n‚úÖ OpenAI chat completions API compatible.\n‚úÖ Customize and add your own models with minimal code changes.\nüé• The Demo Video To support the README file, I also created a demo video to showcase the project. This made it even clearer to understand what the project does.\nPeople are visual creatures they said and I find this to be true. And also visuals convey more information than words and in a shorter time.\nInstead of trying to explain the project in words I let the video do the talking. Bonus point, the video is only 20 seconds but it conveys much more information than I could explain in words using a long paragraph. Plus, people are more likely to watch a video than to read a long paragraph.\nüë• The Community I did not post about this project on social media in one go. Instead I posted about it in a niche community to get some initial feedback.\nI posted on Reddit and got some good feedback.\nx.infer - Framework agnostic computer vision inference.\nbyu/WatercressTraining incomputervision üéÅ Wrapping Up There you have it. These are the steps I took to get my project trending. Hope you\u0026rsquo;ll find it useful.\nOnce again, I want to emphasize that building a project with the goal of getting it trending is NOT a good starting point.\nStart with a goal to solve a problem, learn something, or build something that you\u0026rsquo;re passionate about. This will push you to work on your project for a longer without getting discouraged.\nLastly here\u0026rsquo;s a link to my project again in case you missed it. Drop a star if you like it!\ndnth/x.infer - - Star Thank you for reading!\n","description":"I made it to GitHub Trending! Here's how I did it.","permalink":"http://localhost:1313/blog/i_made_it_github_trending/","title":"I Made It to GitHub Trending - My Open Source Journey","type":"Blog"},{"content":"üöÄ Motivation Having real-time inference is crucial for computer vision applications. In some domains, a 1-second delay in inference could mean life or death.\nImagine sitting in a self-driving car and the car takes one full second to detect an oncoming speeding truck.\nJust one second too late, and you could end up in the clouds üëºüëºüëº\nOr if you\u0026rsquo;re lucky, you get a very up-close view of the pavement.\nI hope that shows how crucial real-time inference is.\nüìå In many high-stake applications, it\u0026rsquo;s not just about being right - it\u0026rsquo;s about being right, right now. Thus, having real-time inference capability is paramount and will determine whether a model gets deployed or not. In many cases, you can pick one or the other:\nA fast model with low accuracy A slow model with high accuracy But can we have the best of both worlds? I.e. a fast and accurate model?\nThat\u0026rsquo;s what this post is about.\ntip\nBy the end of the post you\u0026rsquo;ll learn how to supercharge the inference speed of any image models from TIMM with optimized ONNX Runtime and TensorRT.\nIn short:\nüì• Load any pre-trained model from TIMM. üîÑ Convert the model to ONNX format. üñ•Ô∏è Run inference with ONNX Runtime (CPU \u0026amp; CUDA Provider). üéÆ Run inference with TensorRT provider and optimized runtime parameters. üß† Bake the pre-processing into the ONNX model for faster inference. You can find the code for this post on my GitHub repository here.\ninfo\nAre you non-technical?\nListen to this 10 mins conversation podcast that breaks down the content of this post in an ELI5 manner.\nDeep Dive - Explain Like I\u0026#39;m 5 Supercharge Your PyTorch Image Models Your browser does not support the audio element. Note: Conversation generated using NotebookLM.\nIf you\u0026rsquo;re technical, and this sounds exciting, then let\u0026rsquo;s dive in! üèä‚Äç‚ôÇÔ∏è\nüíª Installation Let\u0026rsquo;s begin with the installation. I will be using a conda environment to install the packages required for this post. Feel free to the environment of your choice.\nconda create -n supercharge_timm_tensorrt python=3.11 conda activate supercharge_timm_tensorrt We\u0026rsquo;ll be using the timm library to load a pre-trained model and run inference. So let\u0026rsquo;s install timm.\npip install timm At the time of writing, there are over 1370 models available in timm. Any of which can be used in this post.\nüîß Load and Infer Let\u0026rsquo;s load a top performing model from the timm leaderboard - the eva02_large_patch14_448.mim_m38m_ft_in22k_in1k model.\nThe plot above shows the accuracy vs inference speed for the EVA02 model.\nLook closely, the EVA02 model achieves top ImageNet accuracy (90.05% top-1, 99.06% top-5) but is lags in speed. Check out the model on the timm leaderboard here.\nSo let\u0026rsquo;s get the EVA02 model on our local machine\nimport timm model_name = \u0026#39;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\u0026#39; model = timm.create_model(model_name, pretrained=True).eval() Get the data config and transformations for the model\ndata_config = timm.data.resolve_model_data_config(model) transforms = timm.data.create_transform(**data_config, is_training=False) And run an inference to get the top 5 predictions\nimport torch from PIL import Image from urllib.request import urlopen img = Image.open(urlopen(\u0026#39;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\u0026#39;)) with torch.inference_mode(): output = model(transforms(img).unsqueeze(0)) top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5) Next, decode the predictions into class names as a sanity check\nfrom imagenet_classes import IMAGENET2012_CLASSES im_classes = list(IMAGENET2012_CLASSES.values()) class_names = [im_classes[i] for i in top5_class_indices[0]] for name, prob in zip(class_names, top5_probabilities[0]): print(f\u0026#34;{name}: {prob:.2f}%\u0026#34;) Top 5 predictions:\nx \u0026ndash; + espresso:\u0026nbsp;26.78% eggnog:\u0026nbsp;2.88% cup:\u0026nbsp;2.60% chocolate\u0026nbsp;sauce,\u0026nbsp;chocolate\u0026nbsp;syrup:\u0026nbsp;2.39% bakery,\u0026nbsp;bakeshop,\u0026nbsp;bakehouse:\u0026nbsp;1.48% Looks like the model is doing it\u0026rsquo;s job.\nNow let\u0026rsquo;s benchmark the inference latency.\n‚è±Ô∏è Baseline Latency We will run the inference 10 times and average the inference time.\nimport time def run_benchmark(model, device, num_images=10): model = model.to(device) with torch.inference_mode(): start = time.perf_counter() for _ in range(num_images): input_tensor = transforms(img).unsqueeze(0).to(device) model(input_tensor) end = time.perf_counter() ms_per_image = (end - start) / num_images * 1000 fps = num_images / (end - start) print(f\u0026#34;PyTorch model on {device}: {ms_per_image:.3f} ms per image, FPS: {fps:.2f}\u0026#34;) Let\u0026rsquo;s benchmark on CPU and GPU.\n# CPU Benchmark run_benchmark(model, torch.device(\u0026#34;cpu\u0026#34;)) # GPU Benchmark if torch.cuda.is_available(): run_benchmark(model, torch.device(\u0026#34;cuda\u0026#34;)) Alright the benchmarks are in\nx \u0026ndash; + PyTorch\u0026nbsp;model\u0026nbsp;on\u0026nbsp;cpu:\u0026nbsp;1584.379\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;0.63 PyTorch\u0026nbsp;model\u0026nbsp;on\u0026nbsp;cuda:\u0026nbsp;77.226\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;12.95 Although the performance on the GPU is not bad, 12+ FPS is still not fast enough for real-time inference. On my reasonably modern CPU, it took over 1.5 seconds to run an inference.\nDefinitely not self-driving car material ü§∑\nnote\nI\u0026rsquo;m using the following hardware for the benchmarks:\nGPU - NVIDIA RTX 3090 CPU - 11th Gen Intel¬Æ Core‚Ñ¢ i9-11900 @ 2.50GHz √ó 16 Now let\u0026rsquo;s start to improve the inference time.\nüîÑ Convert to ONNX ONNX is an open and interoperable format for deep learning models. It lets us deploy models across different frameworks and devices.\nThe key advantage of using ONNX is that it lets us deploy models across different frameworks and devices, and offers some performance gains.\nTo convert the model to ONNX format, let\u0026rsquo;s first install onnx.\npip install onnx And export the model to ONNX format\nimport timm import torch model = timm.create_model( \u0026#34;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\u0026#34;, pretrained=True ).eval() onnx_filename = \u0026#34;eva02_large_patch14_448.onnx\u0026#34; torch.onnx.export( model, torch.randn(1, 3, 448, 448), onnx_filename, export_params=True, opset_version=20, do_constant_folding=True, input_names=[\u0026#34;input\u0026#34;], output_names=[\u0026#34;output\u0026#34;], dynamic_axes={\u0026#34;input\u0026#34;: {0: \u0026#34;batch_size\u0026#34;}, \u0026#34;output\u0026#34;: {0: \u0026#34;batch_size\u0026#34;}}, ) note\nHere are the descriptions for the arguments you can pass to the torch.onnx.export function:\nParameter Description torch.randn(1, 3, 448, 448) A dummy input tensor with the appropriate shape export_params Whether to export the model parameters do_constant_folding Whether to do constant folding for optimization input_names The name of the input node output_names The name of the output node dynamic_axes Dynamic axes for the input and output nodes If there are no errors, you will end up with a file called eva02_large_patch14_448.onnx in your working directory.\ntip\nInspect and visualize the ONNX model using the Netron webapp. üñ•Ô∏è ONNX Runtime on CPU To run the and inference on the ONNX model, we need to install onnxruntime. This is the \u0026rsquo;engine\u0026rsquo; that will run the ONNX model.\npip install onnxruntime One (major) benefit of using ONNX Runtime is the ability to run the model without PyTorch as a dependency. This is great for deployment and for running inference in environments where PyTorch is not available.\nThe ONNX model we exported earlier only includes the model weights and the graph structure. It does not include the pre-processing transforms. To run the inference using onnxruntime, we need to replicate the PyTorch transforms. To find out the transforms that was used, you can print out the transforms.\nprint(transforms) x \u0026ndash; + Compose( \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Resize(size=(448,\u0026nbsp;448),\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;interpolation=bicubic,\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;max_size=None,\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;antialias=True) \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;CenterCrop(size=(448,\u0026nbsp;448)) \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;MaybeToTensor() \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Normalize(mean=tensor([0.4815,\u0026nbsp;0.4578,\u0026nbsp;0.4082]),\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;std=tensor([0.2686,\u0026nbsp;0.2613,\u0026nbsp;0.2758])) ) Now let\u0026rsquo;s replicate the transforms using numpy.\ndef transforms_numpy(image: PIL.Image.Image): image = image.convert(\u0026#39;RGB\u0026#39;) image = image.resize((448, 448), Image.BICUBIC) img_numpy = np.array(image).astype(np.float32) / 255.0 img_numpy = img_numpy.transpose(2, 0, 1) mean = np.array([0.485, 0.456, 0.406]).reshape(-1, 1, 1) std = np.array([0.229, 0.224, 0.225]).reshape(-1, 1, 1) img_numpy = (img_numpy - mean) / std img_numpy = np.expand_dims(img_numpy, axis=0) img_numpy = img_numpy.astype(np.float32) return img_numpy Using the numpy, transforms let\u0026rsquo;s run inference with ONNX Runtime.\nimport onnxruntime as ort # Create ONNX Runtime session with CPU provider onnx_filename = \u0026#34;eva02_large_patch14_448.onnx\u0026#34; session = ort.InferenceSession( onnx_filename, providers=[\u0026#34;CPUExecutionProvider\u0026#34;] # Run on CPU ) # Get input and output names input_name = session.get_inputs()[0].name output_name = session.get_outputs()[0].name # Run inference output = session.run([output_name], {input_name: transforms_numpy(img)})[0] If we inspect the output shape, we can see that it\u0026rsquo;s the same as the number of classes in the ImageNet dataset.\nLet\u0026rsquo;s inspect the output.shape: x \u0026ndash; + (1,\u0026nbsp;1000) And printing the top 5 predictions:\nx \u0026ndash; + espresso:\u0026nbsp;28.65% cup:\u0026nbsp;2.77% eggnog:\u0026nbsp;2.28% chocolate\u0026nbsp;sauce,\u0026nbsp;chocolate\u0026nbsp;syrup:\u0026nbsp;2.13% bakery,\u0026nbsp;bakeshop,\u0026nbsp;bakehouse:\u0026nbsp;1.42% We get the same results as the PyTorch model with ONNX Runtime. That\u0026rsquo;s a good sign!\nNow let\u0026rsquo;s benchmark the inference latency on ONNX Runtime with a CPU provider (backend).\nimport time num_images = 10 start = time.perf_counter() for i in range(num_images): output = session.run([output_name], {input_name: transforms_numpy(img)})[0] end = time.perf_counter() time_taken = end - start ms_per_image = time_taken / num_images * 1000 fps = num_images / time_taken print(f\u0026#34;Onnxruntime CPU: {ms_per_image:.3f} ms per image, FPS: {fps:.2f}\u0026#34;) x \u0026ndash; + Onnxruntime\u0026nbsp;CPU:\u0026nbsp;2002.446\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;0.50 Ouch! That\u0026rsquo;s slower than the PyTorch model. What a bummer! It may seem like a step back, but we are only getting started.\nRead on.\ninfo\nThe remainder of this post assumes that you have a compatible NVDIA GPU. If you don\u0026rsquo;t, you can still use the CPU for inference by switch to the Intel OpenVINO or AMD backend.\nThere are more backends available including for mobile devices like Apple, Android, etc. Check them out here\nThese will be covered in a future post.\nüñºÔ∏è ONNX Runtime on CUDA Other than the CPU, ONNX Runtime offers other backends for inference. We can easily swap to a different backend by changing the provider. In this case we will use the CUDA backend.\nTo use the CUDA backend, we need to install the onnxruntime-gpu package. warning\nYou must uninstall the onnxruntime package before installing the onnxruntime-gpu package.\nRun the following to uninstall the onnxruntime package.\npip uninstall onnxruntime Then install the onnxruntime-gpu package.\npip install onnxruntime-gpu==1.19.2 The onnxruntime-gpu package requires a compatible CUDA and cuDNN version. I\u0026rsquo;m running on onnxruntime-gpu==1.19.2 at the time of writing this post. This version is compatible with CUDA 12.x and cuDNN 9.x.\nSee the compatibility matrix here.\nYou can install all the CUDA dependencies using conda with the following command.\nconda install -c nvidia cuda=12.2.2 \\ cuda-tools=12.2.2 \\ cuda-toolkit=12.2.2 \\ cuda-version=12.2 \\ cuda-command-line-tools=12.2.2 \\ cuda-compiler=12.2.2 \\ cuda-runtime=12.2.2 Once done, replace the CPU provider with the CUDA provider.\nonnx_filename = \u0026#34;eva02_large_patch14_448.onnx\u0026#34; session = ort.InferenceSession( onnx_filename, providers=[\u0026#34;CUDAExecutionProvider\u0026#34;] # change the provider ) The rest of the code is the same as the CPU inference.\nJust with one line of code change, the benchmarks are as follows:\nx \u0026ndash; + Onnxruntime\u0026nbsp;CUDA\u0026nbsp;numpy\u0026nbsp;transforms:\u0026nbsp;56.430\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;17.72 But that\u0026rsquo;s kinda expected. Running on the GPU, we should expect a speedup.\ninfo\nIf you encounter the following error:\nFailed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory It means that the CUDA library is not in the library path. You need to export the library path to include the CUDA library.\nexport LD_LIBRARY_PATH=\u0026#34;/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib:$LD_LIBRARY_PATH\u0026#34; Replace the /home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib with the path to your CUDA library.\nTheres is one more trick we can use to squeeze out more performance - using CuPy for the transforms instead of NumPy.\nCuPy is a library that lets us run NumPy code on the GPU. It\u0026rsquo;s a drop-in replacement for NumPy, so you can just replace numpy with cupy in your code and it will run on the GPU.\nLet\u0026rsquo;s install CuPy compatible with our CUDA version.\npip install cupy-cuda12x And we can use it to run the transforms.\ndef transforms_cupy(image: PIL.Image.Image): # Convert image to RGB and resize image = image.convert(\u0026#34;RGB\u0026#34;) image = image.resize((448, 448), Image.BICUBIC) # Convert to CuPy array and normalize img_cupy = cp.array(image, dtype=cp.float32) / 255.0 img_cupy = img_cupy.transpose(2, 0, 1) # Apply mean and std normalization mean = cp.array([0.485, 0.456, 0.406], dtype=cp.float32).reshape(-1, 1, 1) std = cp.array([0.229, 0.224, 0.225], dtype=cp.float32).reshape(-1, 1, 1) img_cupy = (img_cupy - mean) / std # Add batch dimension img_cupy = cp.expand_dims(img_cupy, axis=0) return img_cupy With CuPy, we got a tiny bit of performance improvement:\nx \u0026ndash; + Onnxruntime\u0026nbsp;CUDA\u0026nbsp;cupy\u0026nbsp;transforms:\u0026nbsp;54.267\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;18.43 Using ONNX Runtime with CUDA is a little better than the PyTorch model on the GPU, but still not fast enough for real-time inference.\nWe have one more trick up our sleeve.\nüìä ONNX Runtime on TensorRT Similar to the CUDA provider, we have the TensorRT provider on ONNX Runtime. This lets us run the model using the TensorRT high performance inference engine by NVIDIA.\nFrom the compatibility matrix, we can see that onnxruntime-gpu==1.19.2 is compatible with TensorRT 10.1.0.\nTo use the TensorRT provider, you need to have TensorRT installed on your system.\npip install tensorrt==10.1.0 \\ tensorrt-cu12==10.1.0 \\ tensorrt-cu12-bindings==10.1.0 \\ tensorrt-cu12-libs==10.1.0 Next you need to export library path to include the TensorRT library.\nexport LD_LIBRARY_PATH=\u0026#34;/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/python3.11/site-packages/tensorrt_libs:$LD_LIBRARY_PATH\u0026#34; Replace the /home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/python3.11/site-packages/tensorrt_libs with the path to your TensorRT library.\nOtherwise you\u0026rsquo;ll encounter the following error:\nFailed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory Next we need so set the TensorRT provider options in ONNX Runtime inference code.\nproviders = [ ( \u0026#34;TensorrtExecutionProvider\u0026#34;, { \u0026#34;device_id\u0026#34;: 0, \u0026#34;trt_max_workspace_size\u0026#34;: 8589934592, \u0026#34;trt_fp16_enable\u0026#34;: True, \u0026#34;trt_engine_cache_enable\u0026#34;: True, \u0026#34;trt_engine_cache_path\u0026#34;: \u0026#34;./trt_cache\u0026#34;, \u0026#34;trt_force_sequential_engine_build\u0026#34;: False, \u0026#34;trt_max_partition_iterations\u0026#34;: 10000, \u0026#34;trt_min_subgraph_size\u0026#34;: 1, \u0026#34;trt_builder_optimization_level\u0026#34;: 5, \u0026#34;trt_timing_cache_enable\u0026#34;: True, }, ), ] onnx_filename = \u0026#34;eva02_large_patch14_448.onnx\u0026#34; session = ort.InferenceSession(onnx_filename, providers=providers) The rest of the code is the same as the CUDA inference.\nnote\nHere are the parameters and description for the TensorRT provider:\nParameter Description device_id The GPU device ID to use. Using the first GPU in the system. trt_max_workspace_size Maximum workspace size for TensorRT in bytes (8GB). Allows TensorRT to use up to 8GB of GPU memory for operations. trt_fp16_enable Enables FP16 (half-precision) mode. Speeds up inference on supported GPUs while reducing memory usage. trt_engine_cache_enable Enables caching of TensorRT engines. Speeds up subsequent runs by avoiding engine rebuilding. trt_engine_cache_path Directory where TensorRT engine cache files will be stored. trt_force_sequential_engine_build Allows parallel building of TensorRT engines for different subgraphs. trt_max_partition_iterations Maximum number of iterations for TensorRT to attempt partitioning the graph. trt_min_subgraph_size Minimum number of nodes required for a subgraph to be considered for conversion to TensorRT. trt_builder_optimization_level Optimization level for the TensorRT builder. Level 5 is highest, can result in longer build times but potentially better performance. trt_timing_cache_enable Enables timing cache. Helps speed up engine building by reusing layer timing information from previous builds. Refer to the TensorRT ExecutionProvider documentation for more details on the parameters.\nAnd now let\u0026rsquo;s run the benchmark:\nx \u0026ndash; + TensorRT\u0026nbsp;+\u0026nbsp;numpy:\u0026nbsp;18.852\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;53.04 TensorRT\u0026nbsp;+\u0026nbsp;cupy:\u0026nbsp;16.892\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;59.20 Running with TensorRT and cupy give us a 4.5x speedup over the PyTorch model on the GPU and 93x speedup over the PyTorch model on the CPU!\nThank you for reading this far. That\u0026rsquo;s the end of this post.\nOr is it?\nYou could stop here and be happy with the results. After all we already got a 93x speedup over the PyTorch model.\nBut.. if you\u0026rsquo;re like me and you wonder how much more performance we can squeeze out of the model, there\u0026rsquo;s one final trick up our sleeve.\nüéÇ Bake pre-processing into ONNX If you recall, we did our pre-processing transforms outside of the ONNX model in CuPy or NumPy.\nThis incurs some data transfer overhead. We can avoid this overhead by baking the transforms operations into the ONNX model.\nOkay so how do we do this?\nFirst, we need to write the preprocessing code as a PyTorch module.\nimport torch.nn as nn class Preprocess(nn.Module): def __init__(self, input_shape: List[int]): super(Preprocess, self).__init__() self.input_shape = tuple(input_shape) self.mean = torch.tensor([0.4815, 0.4578, 0.4082]).view(1, 3, 1, 1) self.std = torch.tensor([0.2686, 0.2613, 0.2758]).view(1, 3, 1, 1) def forward(self, x: torch.Tensor): x = torch.nn.functional.interpolate( input=x, size=self.input_shape[2:], ) x = x / 255.0 x = (x - self.mean) / self.std return x And now export the Preprocess module to ONNX.\ninput_shape = [1, 3, 448, 448] output_onnx_file = \u0026#34;preprocessing.onnx\u0026#34; model = Preprocess(input_shape=input_shape) torch.onnx.export( model, torch.randn(input_shape), output_onnx_file, opset_version=20, input_names=[\u0026#34;input_rgb\u0026#34;], output_names=[\u0026#34;output_prep\u0026#34;], dynamic_axes={ \u0026#34;input_rgb\u0026#34;: { 0: \u0026#34;batch_size\u0026#34;, 2: \u0026#34;height\u0026#34;, 3: \u0026#34;width\u0026#34;, }, }, ) Let\u0026rsquo;s visualize the exported preprocessing.onnx model on Netron.\nnote\nNote the name of the output node of the preprocessing.onnx model - output_preprocessing. Next, let\u0026rsquo;s visualize the original eva02_large_patch14_448 model on Netron.\nNote the name of the input node of the eva02_large_patch14_448 model. We will need this for the merge. The name of the input node is input.\nNow, we merge the preprocessing.onnx model with the eva02_large_patch14_448 model. To achieve this, we need to merge the output of the preprocessing.onnx model with the input of the eva02_large_patch14_448 model.\nTo merge the models, we use the compose function from the onnx library.\nimport onnx # Load the models model1 = onnx.load(\u0026#34;preprocessing.onnx\u0026#34;) model2 = onnx.load(\u0026#34;eva02_large_patch14_448.onnx\u0026#34;) # Merge the models merged_model = onnx.compose.merge_models( model1, model2, io_map=[(\u0026#34;output_preprocessing\u0026#34;, \u0026#34;input\u0026#34;)], prefix1=\u0026#34;preprocessing_\u0026#34;, prefix2=\u0026#34;model_\u0026#34;, doc_string=\u0026#34;Merged preprocessing and eva02_large_patch14_448 model\u0026#34;, producer_name=\u0026#34;dickson.neoh@gmail.com using onnx compose\u0026#34;, ) # Save the merged model onnx.save(merged_model, \u0026#34;merged_model_compose.onnx\u0026#34;) Note the io_map parameter. This lets us map the output of the preprocessing model to the input of the original model. You must ensure that the input and output names of the models are correct.\nIf there are no errors, you will end up with a file called merged_model_compose.onnx in your working directory. Let\u0026rsquo;s visualize the merged model on Netron.\ntip\nThe merged model expects an input of size [batch_size, 3, height, width]. This means that the model can take arbitrary input of size height, width and batch size. Now using this merged model, let\u0026rsquo;s run the inference benchmark again using the TensorRT provider.\nWe\u0026rsquo;ll need to make a small change to how the input tensor is passed to the model.\ndef read_image(image: Image.Image): image = image.convert(\u0026#34;RGB\u0026#34;) img_numpy = np.array(image).astype(np.float32) img_numpy = img_numpy.transpose(2, 0, 1) img_numpy = np.expand_dims(img_numpy, axis=0) return img_numpy Notice we are no longer doing the resize and normalization inside the function. This is because the merged model already includes these operations.\nAnd the results are in!\nx \u0026ndash; + TensorRT\u0026nbsp;with\u0026nbsp;pre-processing:\u0026nbsp;12.875\u0026nbsp;ms\u0026nbsp;per\u0026nbsp;image,\u0026nbsp;FPS:\u0026nbsp;77.67 That\u0026rsquo;s a 8x improvement over the original PyTorch model on the GPU and a whopping 123x improvement over the PyTorch model on the CPU! üöÄ\nLet\u0026rsquo;s do a final sanity check on the predictions.\nx \u0026ndash; + espresso:\u0026nbsp;34.48% cup:\u0026nbsp;2.16% chocolate\u0026nbsp;sauce,\u0026nbsp;chocolate\u0026nbsp;syrup:\u0026nbsp;1.53% bakery,\u0026nbsp;bakeshop,\u0026nbsp;bakehouse:\u0026nbsp;1.01% eggnog:\u0026nbsp;0.98% Looks like the predictions tally!\ninfo\nThere are small value differences in the confidence values which is likely due to the precision difference between FP32 and FP16 and the normalization difference between the PyTorch model and the ONNX model. üéÆ Video Inference Just for fun, let\u0026rsquo;s see how fast the merged model runs on a video.\nThe video inference code is also provided in the repo.\nüöß Conclusion In this post we have seen how we can supercharge our TIMM models for faster inference using ONNX Runtime and TensorRT.\ntip\nIn this post you\u0026rsquo;ve learned how to:\nüì• Load any pre-trained model from TIMM üîÑ Convert the model to ONNX format üñ•Ô∏è Run inference with ONNX Runtime (CPU \u0026amp; GPU) üéÆ Run inference with TensorRT (GPU) üõ†Ô∏è Tweak the TensorRT parameters for better performance üß† Bake the pre-processing into the ONNX model You can find the code for this post on my GitHub repository here.\nü§ó I uploaded the final model to Hugging Face. So if you want to try it out, you can get it here. Or simply check out the Hugging Face Spaces demo below.\nnote\nThere are other things that we\u0026rsquo;ve not explored in this post that will likely improve the inference speed. For example,\nQuantization - reducing the precision of the model weights from FP32 to FP8, INT8 or even lower. Pruning and Sparsity - removing the redundant components of the model to reduce the model size and improve the inference speed. Knowledge distillation - training a smaller and faster model to mimic the original model. I will leave these as an exercise for the reader. And let me know if you\u0026rsquo;d like me to write a follow-up post on these topics.\nThank you for reading! I hope this has been helpful. If you\u0026rsquo;d like to find out how to deploy this model on Android check out the following post.\nFebruary 7, 2023 PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter ","description":"Supercharge your PyTorch image models with ONNX Runtime and TensorRT. Learn step-by-step techniques to achieve up to 8x faster inference speeds, enabling real-time performance for computer vision applications. Optimize TIMM models for deployment using advanced acceleration methods.","permalink":"http://localhost:1313/portfolio/supercharge_your_pytorch_image_models/","title":"Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime \u0026 Optimizations","type":"Portfolio"},{"content":"I\u0026rsquo;m really excited to tell you that I\u0026rsquo;m one of the top 2% of scientists in the world for 2023, according to Stanford University. It\u0026rsquo;s been ten years of hard work in research and learning, and this award means a lot.\nüõ§Ô∏è The Journey When I started, it was easy.\nI had been in academic for some time as a student and research assistant. And I had the resources, for a good start. I thought it\u0026rsquo;s too good to be true. But reality hits mid journey.\nAs a researcher and student, I can recall the moment there was time pressure to publish papers because it\u0026rsquo;s been 3 years and I had no publications under my name.\nI could not graduate.\nI doubted myself a lot. Sleepless nights are a routine. But I\u0026rsquo;m glad I persevered.\nIf you\u0026rsquo;re researching or studying and it feels tough, keep going. The hard work is worth it because of all the good things you learn and do along the way. The skills that I have today, I owe it to the times I spent in research labs, sifting through papers and code.\nIn 2022, I graduated with Ph.D of Engineering. My thesis was about using deep learning techniques to estimate the amount of charge in a Lithium ion battery commonly found in electric vehicles.\nI had the privilege (and luck) to be published in Nature.\nüöÄ Transition to Industry By now, I\u0026rsquo;ve been in academia for 10 years and I decided to move into the industry because I see huge potential in the deep learning field from the academic stand point.\nNeedless to say, it was a big change.\nI didn\u0026rsquo;t have much industry experience outside of school. But I learned that the rigor and mindset from the research world is very helpful in business too.\nBeing able to think through problems and figure out how to solve them is a big deal in any job.\nThese skills from university have helped me a lot in the business world, and they help you keep learning new things too.\nüôè Acknowledgments I want to say a big thank you to the people I work with, my mentors, and my teachers, especially at Universiti Tenaga Nasional. They\u0026rsquo;ve all helped me get here.\nGetting this award shows that sticking with it and working hard can lead to great things.\nI hope this encourages you to keep on with your own work and chase your goals.\nI hope you‚Äôve enjoyed reading this blog post. If you have any questions, comments, or feedback, please leave them on the following LinkedIn post.\n","description":"Join me in reflecting on a decade-long journey from academia to industry recognition as one of the world's leading scientists.","permalink":"http://localhost:1313/blog/world_top_scientist/","title":"Celebrating a Milestone in the Top 2% of Global Scientists","type":"Blog"},{"content":"üåü Motivation For many data scientist (including myself), we pride ourselves in training a model, seeing the loss graph go down, and claim victory when the test set accuracy reaches 99.99235%.\nWhy not?\nThis is the after all the juiciest part of the job. \u0026ldquo;Solving\u0026rdquo; one dataset after another, it may seem like anything around you can be conquered with a simple model.fit.\nThat was me two years ago.\nThe naive version of me thought that was all about it with machine learning (ML). As long as we have a dataset, ML is the way to go.\nAlmost nobody talked about what happens to the model after that.\nLike a painting not shown in an artist\u0026rsquo;s studio, a machine learning model not deployed is a missed opportunity to enrich and enhance the lives of those it was intended to serve.\nChatGPT Without deployment the model you\u0026rsquo;ve trained only benefits you.\nSo how do we maximize the number of people you can serve with the model?\nMobile device.\nIt\u0026rsquo;s 2023, if you\u0026rsquo;re reading this, chances are you own a mobile device.\nHands down, having a model that can work on mobile is going to reach many.\nAccessibility - Most people carry their mobile phones with them. A model accessible on mobile devices lets users use models on the go. Built-in hardware - Mobile devices comes packaged with on board camera and various sensors. Not worry about integrating new hardware. User experience - Enables new form of interaction between apps and sensors on the phone. E.g. computer vision models can be used in an image editing app on the phone. In this blog post, I will show you how you can make a model accessible through your mobile phone with Hugging Face and Flutter.\n‚úÖ Yes, for free.\ntip\n‚ö° By the end of this post you will learn how to:\nUpload a state-of-the-art image classification model to Hugging Face Spaces and get an inference endpoint. Create a Flutter mobile app that runs on Android and iOS to call the inference endpoint. Display the inference results on the screen with a beautiful UI. üí° NOTE: Code and data for this post are available on my GitHub repo here.\nDemo on iOS iPhone 14 Pro\nDemo on Android - Google Pixel 3 XL.\nI\u0026rsquo;ve also uploaded the app to Google Playstore. Download and try it out here.\nIf that looks interesting, let\u0026rsquo;s start!\nü§ó Hugging Face x TIMM Making computer vision models (especially large ones) available on mobile devices sounds interesting in theory.\nBut in practice there are many hurdles -\nHardware limitation - Mobile devices usually run on portable hardware with limited processing power, RAM, and battery life. Models needs to be optimized and efficient catering to these limitations. Optimization - To put computer vision models on mobile hardware, they usually need to be optimized to run on specific hardware and software environment on the device. This requires specialized knowledge in computer vision and mobile development. Practicality - User experience is a big factor in whether your app will be used by anyone. Nobody wants to use a bloated, slow and inefficient mobile app. I know that sounds complicated. Don\u0026rsquo;t worry because we are NOT going to deal with any of that in this blog post!\nEnter üëá\nHugging Face is a platform that allows users to host and share machine learning models and dataset. It\u0026rsquo;s most notable for its Transformers model for Natural Language Processing (NLP).\nRecently Hugging Face has been expanding its territory beyond NLP and venturing into computer vision.\nRoss Wightman, the creator of the wildly popular PyTorch Image Model (TIMM) repo joins forces.\nTIMM is a open-source computer vision repo used in research and commercial application. I boasts close to a thousand (and counting) state-of-the-art PyTorch image models, pretrained weights and scripts for training, validation and inference.\nTIMM joins Hugging Face. tip\nCheck out the TIMM repo here. What does it mean for you?\nNow you can use any models from TIMM with Hugging Face on platforms of your choice. The Hugging Face docs shows how you can do it using Python.\nüì• Hosting a Model on Hugging Face Spaces Spaces are one of the most popular ways to share ML applications and demos with the world.\nDetails on the Hardware specifications and pricing here.\nHardware specs on Spaces. tip\nDetails on how I trained the model using fastai here. Here\u0026rsquo;s the model that I trained using Fastai ahd hosted on Hugging Face Space.\nTry it out üëá\ntip\nView on the Hugging Face webpage here. The inference endpoind is deployed using Gradio in just a few lines of code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import os import gradio as gr from fastai.vision.all import * import skimage learn = load_learner(\u0026#34;learner.pkl\u0026#34;) labels = learn.dls.vocab def predict(img): img = PILImage.create(img) pred, pred_idx, probs = learn.predict(img) return {labels[i]: float(probs[i]) for i in range(len(labels))} title = \u0026#34;Paddy Disease Classifier with EdgeNeXt\u0026#34; description = \u0026#34;9 Diseases + 1 Normal class\u0026#34; interpretation = \u0026#34;default\u0026#34; examples = [\u0026#34;sample_images/\u0026#34; + file for file in files] enable_queue = True gr.Interface( fn=predict, inputs=gr.inputs.Image(shape=(224, 224)), outputs=gr.outputs.Label(num_top_classes=3), title=title, description=description, examples=examples, interpretation=interpretation, enable_queue=enable_queue ).launch() If we want to use other language then we\u0026rsquo;ll need an API endpoint.\nüîÑ Inference API Endpoint All applications deployed using Gradio has an API endpoint.\nTIMM joins Hugging Face. View the API endpoint here\nüì≤ Flutter Build user interface. Don\u0026rsquo;t want to get user lost in the detail implementation. Calling the endpoint in Flutter.\nimport \u0026#39;dart:convert\u0026#39;; import \u0026#39;package:http/http.dart\u0026#39; as http; Future\u0026lt;Map\u0026gt; classifyRiceImage(String imageBase64) async { final response = await http.post( Uri.parse( \u0026#39;https://dnth-edgenext-paddy-disease-classifie-dc60651.hf.space/run/predict\u0026#39;), headers: \u0026lt;String, String\u0026gt;{ \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json; charset=UTF-8\u0026#39;, }, body: jsonEncode(\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt;{ \u0026#39;data\u0026#39;: [imageBase64] }), ); if (response.statusCode == 200) { // If the server did return a 200 CREATED response, // then decode the image and return it. final classificationResult = jsonDecode(response.body)[\u0026#34;data\u0026#34;][0]; return classificationResult; } else { // If the server did not return a 200 OKAY response, // then throw an exception. throw Exception(\u0026#39;Failed to classify image.\u0026#39;); } } GitHub repo here.\nü§ñ Demo Demo on iOS iPhone 14 Pro\nDemo on Android - Google Pixel 3 XL.\nUse image picker or camera.\nI\u0026rsquo;ve also uploaded the app to Google Playstore. Download and try it out here.\nüôè Comments \u0026amp; Feedback That\u0026rsquo;s a wrap! In this post, I\u0026rsquo;ve shown you how you can start from a model, train it, and deploy it on a mobile device for edge inference.\ntip\n‚ö° By the end of this post you will learn how to:\nUpload a SOTA classification model to Hugging Face Spaces and get an inference endpoint. Create a Flutter mobile app that runs on Android and iOS to call the inference endpoint. Display the inference results on the screen with a beautiful UI. üí° NOTE: Code and data for this post are available on my GitHub repo here.\nWhat\u0026rsquo;s next? If you\u0026rsquo;d like to learn about how I deploy a cloud based object detection model on Android, check it out here.\nI hope you\u0026rsquo;ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or drop me a message.\n","description":"Discover how Hugging Face's TIMM library brings state-of-the-art computer vision models to iOS and Android.","permalink":"http://localhost:1313/portfolio/bringing_high_quality_image_models_to_mobile/","title":"Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android \u0026 iOS","type":"Portfolio"},{"content":"‚úÖ Motivation In today\u0026rsquo;s world of selfies and Instagram, we all take tons of photos on our phones, cameras, and other gadgets.\nBut let\u0026rsquo;s be real, it\u0026rsquo;s easy for our photo collections to become a chaotic mess, making it impossible to find that one special memory.\nI mean, I\u0026rsquo;ve got gigabytes of photos on my Google Photo app filled with dark shots, overly exposed shots, blurry shots, and tons of duplicate stills.\nAnd let\u0026rsquo;s face it, what we post on Instagram vs what\u0026rsquo;s behind the scenes can be wildly different.\nMe vs my photo album. I know, you\u0026rsquo;ll say that there\u0026rsquo;s no harm in keeping those extra selfies in your phone. Right?\nNot in the short term. But over time, these photos will just clutter your devices taking up valuable disk space and slowing down your device.\nAlso, think about these -\nIt\u0026rsquo;s difficult to find specific photos when your collection is in a mess. Organizing your collection saves you time spent searching for photos. An organized photo collection can be a source of pride especially when you share them. Digital clutter not only affects your device but also impacts you psychologically. So consider cleaning up your digital clutter, because it pays in the long run.\nIf you\u0026rsquo;re convinced, now comes the next hurdle.\nSpending hours sorting through your photos and cleaning them is a pain. Nobody has time for that. We‚Äôre busy people.\nDon\u0026rsquo;t fret, that\u0026rsquo;s what this post is about. In this post, I\u0026rsquo;ll show you how to tidy up your digital life by organizing your photo collection and not spending an entire weekend doing it.\ntip\nüí´ Here\u0026rsquo;s what you\u0026rsquo;ll learn by the end -\nHow to isolate corrupted images in your photo album. How to identify duplicates in your photo album. How to filter out photos that are too dark, too bright, or blurry. How to cluster similar-looking shots together. How to bulk-delete photos. üìù NOTE: All codes used in the post are on my Github repository.\n‚ö° fastdup fastdup is a tool that let you gain insights from a large image/video collection.\nYou can manage, clean, and curate your images at scale on your local machine event with a single CPU. fastdup lets you clean visual data with ease, freeing up valuable resources and time.\nHere are some superpowers you get with fastdup - it lets you identify:\nfastdup superpowers. Source: fastdup GitHub. In short, fastdup is üëá\nUnsupervised: fits any visual dataset. Scalable: handles 400M images on a single machine. Efficient: works on CPU (even on Google Colab with only 2 CPU cores!). Low Cost: can process 12M images on a $1 cloud machine budget. üåü The best part? fastdup is free.\ninfo\nfastdup also offers an Enterprise edition of the tool that lets you do more. Find out here. If that looks interesting, let\u0026rsquo;s get started with.. üëá\n‚òï Messy Images As we are going to clean up messy albums, the first step is to download the photos from your Google Photos, Onedrive, or whatever cloud service you use into your local drive.\nI don\u0026rsquo;t have a massive photo collection, so I‚Äôll be using an image collection from Kaggle that was scraped off Google Images.\nThe contributor Debadri Dutta has a knack for photography and traveling. A lot of the images from the collection are uploaded by users on social media. So I thought it would be a good fit to use it for this post.\nHere are a few sample images. Sample images scraped from Google. With the images downloaded locally let\u0026rsquo;s organize them in a folder. Here\u0026rsquo;s how the folders look on my computer.\n‚îú‚îÄ‚îÄ images | ‚îú‚îÄ‚îÄ image001.jpg | ‚îú‚îÄ‚îÄ image002.jpg | ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ fastdup_report ‚îî‚îÄ‚îÄ fastdup_analyze.ipynb note\nDescription -\nimages/ \u0026ndash; Folder to store the images. fastdup_report/ \u0026ndash; Directory to save the output generated by fastdup. fastdup_analyze.ipynb \u0026ndash; Jupyter notebook to run fastdup. üìù NOTE: If you\u0026rsquo;d like to follow along with the example on this post download the images to your desktop from Kaggle here into the images/ directory.\nWith the folders in place let\u0026rsquo;s get working.\nüßÆ Install and Run First, let\u0026rsquo;s install fastdup with:\npip install fastdup I\u0026rsquo;m running fastdup==0.903 and Python 3.10 for this post. Feel free to use the latest version available.\nAfter the installation completes, you can now import fastdup in your Python console and start the run.\nimport fastdup work_dir = \u0026#34;./fastdup_report\u0026#34; images_dir = \u0026#34;./images\u0026#34; fd = fastdup.create(work_dir, images_dir) fd.run() note\nimages_dir \u0026ndash; Path to the folder containing images. work_dir \u0026ndash; Path to save the outputs from the run. üìù NOTE: More info on other parameters on the docs page.\nThis starts the process of detecting all issues on the images in images_dir. Depending on your CPU power, this may take a few seconds to a few minutes to complete.\nOn my machine, with an Intel Core‚Ñ¢ i9-11900 it takes under 1-minute to check through (approx. 35,000) images in the folder ü§Ø.\nOnce the run completes, you\u0026rsquo;ll find the work_dir populated with all files from the run.\ntip\nfastdup recommends running the commands in a Python console and NOT in a Jupyter notebook. Personally, I find no issues running the commands in a notebook. But beware that the notebook size can be large especially if there are lots of images rendered.\nOnce the run is complete, we can visualize the issues.\nFor a summary, run\nfd.summary() Here are some useful information from the summary.\nDataset contains 35136 images. Valid images are 99.83% (35,077) of the data, invalids are 0.17% (59) of the data' 2.15% (756) belong to 12 similarity clusters (components). Largest cluster has 16 (0.05%) images. 6.16% (2,163) of images are possible outliers, and fall in the bottom 5.00% of similarity values. There are a few issues we can already spot there but let\u0026rsquo;s start with üëá\nüö´ Invalid Images Invalid images are files that cannot be read by fastdup. Chances are, they are corrupted images.\nWe have 59 of them according to the summary. To get the list of invalid images, run:\nfd.invalid_instances() which outputs\nI tried to open these images on my machine, but they could not be viewed.\nInvalid images can\u0026rsquo;t be used but take up disk space. There\u0026rsquo;s only one way to deal with it - Delete.\nTo delete corrupted images with fastdup, let\u0026rsquo;s collect the images into a list:\ninvalid_images = fd.invalid_instances() list_of_invalid_images = invalid_images[\u0026#39;img_filename\u0026#39;].to_list() images_to_delete now contains a list of file directories to be deleted.\n[\u0026#39;art and culture/145.jpg\u0026#39;, \u0026#39;art and culture/148 (9).jpg\u0026#39;, \u0026#39;art and culture/155 (3).jpg\u0026#39;, \u0026#39;art and culture/156 (5).jpg\u0026#39;, ... ... ... \u0026#39;art and culture/98 (5).jpg\u0026#39;, \u0026#39;food and d rinks/1.jpg\u0026#39;, \u0026#39;food and d rinks/28 (2).jpg\u0026#39;, \u0026#39;food and d rinks/325 (3).jpg\u0026#39;, \u0026#39;food and d rinks/424.jpg\u0026#39;] What\u0026rsquo;s left to do next is to write a function to delete images in list_of_invalid_images.\nwarning\nThe following code will DELETE ALL corrupted images specified in list_of_invalid_images. I recommend making a backup of your existing dataset before proceeding. from pathlib import Path def delete_images(file_paths): for file_path in file_paths: path = images_dir / Path(file_path) if path.is_file(): print(f\u0026#34;Deleting {path}\u0026#34;) path.unlink() And call the function:\ndelete_images(list_of_invalid_images) Just like that, we\u0026rsquo;ve deleted all corrupted images from our dataset!\ntip\nYou can optionally choose to move the images to another folder instead of deleting them like what we did above. We can do that with the following function:\nimport shutil from pathlib import Path def move_images_to_folder(file_paths, folder_name=\u0026#34;invalid_images\u0026#34;): corrupted_images_dir = Path(folder_name) corrupted_images_dir.mkdir(exist_ok=True) # create the directory if it doesn\u0026#39;t exist for file_path in file_paths: path = images_dir / Path(file_path) if path.is_file(): new_path = corrupted_images_dir / Path(file_path) new_path.parent.mkdir(parents=True, exist_ok=True) # create the parent directory if it doesn\u0026#39;t exist print(f\u0026#34;Moving {path} to {new_path}\u0026#34;) shutil.move(str(path), str(new_path)) And call the function:\nmove_images_to_folder(list_of_invalid_images) This should move the invalid images into the folder_name directory.\nüëØ‚Äç‚ôÇÔ∏è Duplicate Images To view the duplicate photos run:\nfd.vis.duplicates_gallery() If you\u0026rsquo;re running this in a Jupyter notebook, you\u0026rsquo;ll see something like the following. A generated gallery of duplicates. note\nYou can optionally specify num_images \u0026ndash; The max number of images to display. Defaults to 20.\nüìù NOTE: More info on other parameters here.\nIn the visualization above we see that there are exact copies residing in different folders within the images_dir.\nSo what do we do about it? You can either refer to the file name and delete the duplicate images by hand.\nOr\nUse a convenient function in fastdup to bulk delete images that are EXACT copies.\nTo do that, let\u0026rsquo;s first get the connected components DataFrame:\ncc_df, _ = fd.connected_components() Next, we will group the connected components DataFrame to show only the duplicates:\ndef get_clusters_of_duplicates(df, sort_by=\u0026#39;count\u0026#39;, min_count=2, ascending=False): agg_dict = {\u0026#39;img_filename\u0026#39;: list, \u0026#39;mean_distance\u0026#39;: max, \u0026#39;count\u0026#39;: len} df = df[df[\u0026#39;count\u0026#39;] \u0026gt;= min_count] df = df[df[\u0026#34;mean_distance\u0026#34;]==1.0] grouped_df = df.groupby(\u0026#39;component_id\u0026#39;).agg(agg_dict).sort_values(by=[sort_by], ascending=ascending) return grouped_df duplicates_df=get_clusters_of_duplicates(cc_df) In duplicates_df you\u0026rsquo;ll now find:\nNow let\u0026rsquo;s turn the contents of duplicates_df into a list of images using the function:\ndef get_list_of_duplicate_images(df): df[\u0026#39;img_filename\u0026#39;] = df[\u0026#39;img_filename\u0026#39;].apply(lambda row: row[1:]) # Get a list of images to delete from the df list_of_duplicate_images=duplicates_df[\u0026#39;img_filename\u0026#39;].to_list() # Flatten list list_of_duplicate_images = [item for sublist in list_of_duplicate_images for item in sublist] return list_of_duplicate_images Calling the function\nlist_of_duplicate_images = get_list_of_duplicate_images(duplicates_df) We end up with list_of_duplicate_images\n[\u0026#39;978580450_e862715aba.jpg.jpg\u0026#39;, \u0026#39;architecure/14217992353_2b5120f5b8_m.jpg\u0026#39;, \u0026#39;food and d rinks/z3jQCkYBoXtDrw8mxnkH.jpg\u0026#39;, \u0026#39;food and d rinks/PYFXUZZDGzcsoUAEWLhH.png\u0026#39;, \u0026#39;food and d rinks/kbcgoFeL1BXZzWKSEwfU.png\u0026#39;, \u0026#39;food and d rinks/pIX6YKvYX2sJcgAk5aCo.jpg\u0026#39;, .. .. .. \u0026#39;architecure/11543398565_7a25482b20.jpg\u0026#39;, \u0026#39;food and d rinks/uO6H0sqpkRdg20J3QvzX.jpg\u0026#39;, \u0026#39;food and d rinks/zxN445iYMYExleeeKhA6.jpg\u0026#39;] With this you can use the functions move_images_to_folder or delete_images we defined earlier.\nJust like that, we\u0026rsquo;ve eliminated duplicates from the album! In this post, I found a total of 1929 fully identical images!\nNow on to the next common problem in photo albums üëá\nü§≥ Dark, Bright, and Blurry Shots Let\u0026rsquo;s be real, even pros have overly dark bright, and blurry shots in their albums. These shots are probably not going to be used and hog your storage space.\nWith fastdup you can filter them out with:\nfd.vis.stats_gallery(metric=\u0026#39;dark\u0026#39;) The above snippet sorts all the photos in your folder following ascending mean values. So the darker images (lower mean value) should appear at the top.\nA generated gallery of dark images. The first 3 images (totally black) are classic. I always find these somewhere in my albums due to accidental press when the phone is in my pocket.\nI leave it to you to judge if you\u0026rsquo;d keep or discard the rest of the images.\nConversely, get the brightest images on top with:\nfd.vis.stats_gallery(metric=\u0026#39;bright\u0026#39;) A generated gallery of bright images. Again, see the first 3 images (totally white) which happens sometimes when your shots are overexposed.\nAnd next, let\u0026rsquo;s sort our album with the blur metric. You\u0026rsquo;ve guessed it, this sorts our album with the most blurry image on top.\nfd.vis.stats_gallery(metric=\u0026#39;blur\u0026#39;) A generated gallery of blur images. There are more ways we can view our photos using statistical metrics. So you can change the metric argument to:\nblur \u0026ndash; Sort by blurriness. mean \u0026ndash; Sort by mean value. min \u0026ndash; Sort by minimum value. max \u0026ndash; Sort by maximum value. stdv \u0026ndash; Sort by standard deviation value. View other examples here.\ntip\nTry running with metric='stdv'. You\u0026rsquo;ll find images that lie outside of the standard deviation and potentially find anomalies in them. üóÇ Clustering Similar Shots This is one of my favorite functions in fastdup.\nWith all the thousands of photos in one album, it will be interesting to group similar shots to assess them as a whole.\nIt\u0026rsquo;s also easier to identify patterns and trends in these similar shots. Or you may find that these shots are just redundant shots that will not be used.\nTo group similar shots together run:\nfd.vis.component_gallery() And you\u0026rsquo;ll find something like the following.\nAbove, I\u0026rsquo;ve shown you three examples of similar-looking shots grouped together with the file path of each image. It\u0026rsquo;s up to you to decide what to do with the similar-looking shots. Not going to use them? Delete. Otherwise, you can also keep them organized in a folder of some sort.\ntip\nCheck out the full output of the above code in the notebook. üîì Conclusion Cleaning up your digital photo collection is an important step towards simplifying your digital life.\nDisorganized photos can take up valuable storage space, slow down your device\u0026rsquo;s performance, and make it difficult to find specific photos when you need them.\nIn this blog post, I‚Äôve shown you how to use fastdup to programmatically clean your photo collections without spending a lot of time.\ntip\nüí´ Here\u0026rsquo;s what we learned -\nHow to identify duplicates in your photo album using Python code. How to filter out photos that are too dark, too bright, or blurry. How to group similar-looking shots together. How to bulk-delete photos. üìù NOTE: All codes used in the post are on my Github repository.\nBy using fastdup to identify and delete duplicate and unwanted photos, and clustering similar photos for easy organization, you can save time and energy and enjoy a well-organized digital photo collection.\nI hope you\u0026rsquo;ve enjoyed and learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or drop me a message.\nCleaning image datasets is painful, esp if your dataset is huge.\nIf you find yourself sifting through images for duplicates and anomalies, you\u0026#39;re not alone. I\u0026#39;ve been there. üò≠\nI\u0026#39;ll show how you can inspect large image dataset efficiently for free.\nA üßµ pic.twitter.com/2J4W6GF4fy\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) January 26, 2023 ","description":"Even pros have dark, blurry \u0026 duplicate shots. But disorganization can make it hard to find those special memories. Let's fix that.","permalink":"http://localhost:1313/portfolio/clean_up_your_digital_life/","title":"Clean Up Your Digital Life: How I Found 1929 Fully Identical Images, Dark, Bright and Blurry Shots in Minutes, For Free.","type":"Portfolio"},{"content":"üî• Motivation With various high-level libraries like Keras, Transformer, and Fastai, the barrier to training SOTA models has never been lower.\nOn top of that with platforms like Google Colab and Kaggle, pretty much anyone can train a reasonably good model using an old laptop or even a mobile phone (with some patience).\nThe question is no longer \u0026ldquo;can we train a SOTA model?\u0026rdquo;, but \u0026ldquo;what happens after that?\u0026rdquo;\nUnfortunately, after getting the model trained, most people wash their hands off at this point claiming their model works. But, what good would SOTA models do if it\u0026rsquo;s just in notebooks and Kaggle leaderboards?\nUnless the model is deployed and put to use, it\u0026rsquo;s of little benefit to anyone out there.\nBut deployment is painful. Running a model on a mobile phone?\nForget it ü§∑‚Äç‚ôÇÔ∏è.\nThe frustration is real. I remember spending nights exporting models into ONNX and it still failed me. Deploying models on mobile for edge inference used to be complex.\nNot anymore.\nIn this post, I\u0026rsquo;m going to show you how you can pick from over 900+ SOTA models on TIMM, train them using best practices with Fastai, and deploy them on Android using Flutter.\n‚úÖ Yes, for free.\ntip\n‚ö° By the end of this post you will learn how to:\nLoad a SOTA classification model from TIMM and train it with Fastai. Export the trained model with TorchScript for inference. Create a functional Android app and run the inference on your device. üî• The inference time is at 100ms and below on my Pixel 3 XL! The lowest I got was 37ms!\nüí° NOTE: Code and data for this post are available on my GitHub repo here.\nHere\u0026rsquo;s a TLDR üëá\nIf that looks interesting, read on üëá\nüåø Dataset We will be working with the Paddy Disease Classification dataset from Kaggle. The dataset consists of 10,407 labeled images across ten classes (9 diseases and 1 normal):\nbacterial_leaf_blight bacterial_leaf_streak bacterial_panicle_blight blast brown_spot dead_heart downy_mildew hispa tungro normal The task is to classify the paddy images into 1 of the 9 diseases or normal. Few sample images shown below. Next, I download the data locally and organize them in a folder structure. Here\u0026rsquo;s the structure I have on my computer.\n‚îú‚îÄ‚îÄ data ‚îÇ ‚îú‚îÄ‚îÄ test_images ‚îÇ ‚îî‚îÄ‚îÄ train_images ‚îÇ ‚îú‚îÄ‚îÄ bacterial_leaf_blight ‚îÇ ‚îú‚îÄ‚îÄ bacterial_leaf_streak ‚îÇ ‚îú‚îÄ‚îÄ bacterial_panicle_blight ‚îÇ ‚îú‚îÄ‚îÄ blast ‚îÇ ‚îú‚îÄ‚îÄ brown_spot ‚îÇ ‚îú‚îÄ‚îÄ dead_heart ‚îÇ ‚îú‚îÄ‚îÄ downy_mildew ‚îÇ ‚îú‚îÄ‚îÄ hispa ‚îÇ ‚îú‚îÄ‚îÄ models ‚îÇ ‚îú‚îÄ‚îÄ normal ‚îÇ ‚îî‚îÄ‚îÄ tungro ‚îî‚îÄ‚îÄ train ‚îî‚îÄ‚îÄ train.ipynb note\nDescriptions of the folders:\ndata/ - A folder to store train and test images. train/ - A folder to store training-related files and notebooks. View the full structure by browsing my GitHub repo.\ntip\nüîî If you\u0026rsquo;d like to explore the dataset and excel in the competition, I\u0026rsquo;d encourage you to check out a series of Kaggle notebooks by Jeremy Howard.\nFirst Steps. - Setting up, looking at the data and training your first model. Small Models. - Iterate faster with small models, test time augmentation, and then scale up. Scaling Up. - Testing various models, Vision Transformers, and Ensembles. Multi-target. - Train a multi-target model with Fastai. I\u0026rsquo;ve personally learned a lot from the notebooks. Part of the codes in the post is adapted from the notebooks.\nNow that we\u0026rsquo;ve got the data, let\u0026rsquo;s see how to start building a model out of it\nFor that we need üëá\nü•á PyTorch Image Models There are many libraries to model computer vision tasks but PyTorch Image Models or TIMM by Ross Wightman is arguably the most prominent one today.\nThe TIMM repository hosts hundreds of recent SOTA models maintained by Ross. At this point (January 2023) we have 964 pre-trained models on TIMM and increasing as we speak.\nYou can install TIMM by simply:\npip install timm One line of code, and we\u0026rsquo;d have access to all models on TIMM!\nWith such a massive collection, it can be disorienting which model to start from. Worry not, TIMM provides a function to search for model architectures with a wildcard.\nSince we will be running the model on a mobile device, let\u0026rsquo;s search for model names that contain the word edge:\nimport timm timm.list_models(\u0026#39;*edge*\u0026#39;) This outputs all models that match the wildcard.\n[\u0026#39;cs3edgenet_x\u0026#39;, \u0026#39;cs3se_edgenet_x\u0026#39;, \u0026#39;edgenext_base\u0026#39;, \u0026#39;edgenext_small\u0026#39;, \u0026#39;edgenext_small_rw\u0026#39;, \u0026#39;edgenext_x_small\u0026#39;, \u0026#39;edgenext_xx_small\u0026#39;] Looks like we have something related to the EdgeNeXt model.\nWith a simple search and reading through the preprint EdgeNeXt - Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications, looks like it\u0026rsquo;s a fitting model for our application!\nWith the model name, you can now start training. The TIMM repo provides various utility functions and training scripts. Feel free to use them.\nIn this post, I\u0026rsquo;m going to show you an easy way to train a TIMM model using Fastai üëá\nüèãÔ∏è‚Äç‚ôÄÔ∏è Training with Fastai Fastai is a deep learning library that provides practitioners with high high-level components that can quickly provide SOTA results. Under the hood Fastai uses PyTorch but it abstracts away the details and incorporates various best practices in training a model.\nInstall Fastai with:\npip install fastai Since, we\u0026rsquo;d run our model on a mobile device, let\u0026rsquo;s select the smallest model we got from the previous section - edgenext_xx_small.\nLet\u0026rsquo;s import all the necessary packages with:\nfrom fastai.vision.all import * Next, load the images into a DataLoader.\ntrn_path = Path(\u0026#39;../data/train_images\u0026#39;) dls = ImageDataLoaders.from_folder(trn_path, seed=316, valid_pct=0.2, bs=128, item_tfms=[Resize((224, 224))], batch_tfms=aug_transforms(min_scale=0.75)) note\nParameters for the from_folder method:\ntrn_path \u0026ndash; A Path to the training images. valid_pct \u0026ndash; The percentage of dataset to allocate as the validation set. bs \u0026ndash; Batch size to use during training. item_tfms \u0026ndash; Transformation applied to each item. batch_tfms \u0026ndash; Random transformations applied to each batch to augment the dataset. Read more here. üìù NOTE: Check out the Fastai docs for more information on the parameters.\nYou can show a batch of the train images loaded into the DataLoader with:\ndls.train.show_batch(max_n=8, nrows=2) Next create a Learner object which stores the model, dataloaders, and loss function to train a model. Read more about the Learner here.\nFor vision classification tasks we can create a Learner by calling the vision_learner function and providing the necessary parameters:\nlearn = vision_learner(dls, \u0026#39;edgenext_xx_small\u0026#39;, metrics=accuracy).to_fp16() note\nParameters for vision_learner:\ndls - The Dataloader object. edgenext_xx_small - Model name from TIMM. üìù NOTE: Read more on vision_learner here.\nIn Fastai, you can easily incorporate Mixed Precision Training by adding the .to_fp16() method. This little trick reduces memory usage and trains your model faster at the cost of precision.\nOne of my favorite features in Fastai is the learning rate finder. It lets you estimate the range of learning rate to train the model for the best results.\nFind the best learning rate with:\nlearn.lr_find() tip\nThe orange dot üü† shows the suggested learning rate which is approximately at 2e-3.\nA good learning rate lies at the point where the loss is decreasing most rapidly. On the plot, it\u0026rsquo;s anywhere from the orange dot üü† to the point where the loss starts increasing again approximately at 1e-1. I\u0026rsquo;ll pick 1e-2 as my learning rate.\nRead a post by Zach Mueller on how to pick a good learning rate.\nNow train the model for 5 epochs and a base learning rate of 0.002 with the 1cycle policy. The ShowGraphCallback callback plots the progress of the training.\nlearn.fine_tune(5, base_lr=1e-2, cbs=[ShowGraphCallback()]) With just a few lines of code, we can train a reasonably good model with Fastai. For completeness, here are the few lines of codes you need to load and train the model:\n1 2 3 4 5 6 7 8 from fastai.vision.all import * trn_path = Path(\u0026#39;../data/train_images\u0026#39;) dls = ImageDataLoaders.from_folder(trn_path, seed=316, valid_pct=0.2, bs=128, item_tfms=[Resize((224, 224))], batch_tfms=aug_transforms(min_scale=0.75)) learn = vision_learner(dls, \u0026#39;edgenext_xx_small\u0026#39;, metrics=accuracy).to_fp16() learn.fine_tune(5, base_lr=1e-2, cbs=[ShowGraphCallback()]) tip\nFor demonstration purposes, I\u0026rsquo;ve only with only 5 epochs. You can train for longer to obtain better accuracy and model performance.\nüìù NOTE: View my training notebook here.\nYou can optionally export the Learner object and import it from another script or notebook with:\nlearn.export(\u0026#34;../../train/export.pkl\u0026#34;) Once done, now it\u0026rsquo;s time we transform the model into a form we can use for mobile inference.\nFor that, we\u0026rsquo;ll need üëá\nüìÄ Exporting with TorchScript In this section, we export the model into a form suitable for a mobile device. We can do that easily with TorchScript.\nTorchScript is a way to create serializable and optimizable models from PyTorch code on a variety of platforms, including desktop and mobile devices, without requiring a Python runtime.\nTorchScript Docs With TorchScript, the model\u0026rsquo;s code is converted into a static graph that can be optimized for faster performance, and then saved and loaded as a serialized representation of the model.\nThis allows for deployment to a variety of platforms and acceleration with hardware such as GPUs, TPUs, and mobile devices.\nAll the models on TIMM can be exported with TorchScript using the following code snippet.\n1 2 3 4 5 6 7 8 9 import torch from torch.utils.mobile_optimizer import optimize_for_mobile learn.model.cpu() learn.model.eval() example = torch.rand(1, 3, 224, 224) traced_script_module = torch.jit.trace(learn.model, example) optimized_traced_model = optimize_for_mobile(traced_script_module) optimized_traced_model._save_for_lite_interpreter(\u0026#34;torchscript_edgenext_xx_small.pt\u0026#34;) note\nFrom the snippet above we need to specify a few things:\nLine 6: The shape of the input image tensor. Line 9: \u0026ldquo;torchscript_edgenext_xx_small.pt\u0026rdquo; is the name of the resulting TorchScript serialized model. If you already have your own model.pt file, replace Line 4 and Line 5 with:\nmodel = torch.load(\u0026#39;model.pt\u0026#39;, map_location=\u0026#34;cpu\u0026#34;) model.eval() üìù NOTE: View the full notebook from training to exporting the model on my GitHub repo here.\nOnce completed, you\u0026rsquo;ll have a file torchscript_edgenext_xx_small.pt that can be ported to other devices for inference. In this post, I will be porting it to Android using a framework known as Flutter.\nüì≤ Inference in Flutter Flutter is an open-source framework by Google for building beautiful, natively compiled, multi-platform applications from a single codebase.\nFlutter Webpage We can load the torchscript_edgenext_xx_small.pt and use if for inference. To do so, we will use the pytorch_lite Flutter package. The pytorch_lite package supports image classification and detection with TorchScript.\nThe following code snippet shows a function to load our serialized model torchscript_edgenext_xx_small.pt.\n1 2 3 4 5 6 7 8 9 10 Future loadModel() async { String pathImageModel = \u0026#34;assets/models/torchscript_edgenext_xx_small.pt\u0026#34;; try { _imageModel = await PytorchLite.loadClassificationModel( pathImageModel, 224, 224, labelPath: \u0026#34;assets/labels/label_classification_paddy.txt\u0026#34;); } on PlatformException { print(\u0026#34;only supported for Android\u0026#34;); } } note\nFrom the snippet above we need to specify a few things:\nLine 2: Path to the serialized model. Line 5: The input image size - 224 by 224 pixels. Line 6: A text file with labels associated with each class. View the full code on my GitHub repo.\nThe following code snippet shows a function to run the inference.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Future runClassification() async { //pick an image final XFile? image = await _picker.pickImage(source: ImageSource.gallery); if (image != null) { // run inference var result = await _imageModel! .getImagePredictionResult(await File(image.path).readAsBytes()); setState(() { _imagePrediction = result[\u0026#39;label\u0026#39;]; _predictionConfidence = (result[\u0026#39;probability\u0026#39;] * 100).toStringAsFixed(2); _image = File(image.path); }); } } Those are the two important functions to load and run the TorchScript model.\nThe following screen capture shows the Flutter app in action. The clip runs in real-time and is NOT sped up!\nThe compiled .apk file is about 77MB in size and the inference time is at 100 ms or below on my Pixel 3 XL!\nTry it out and install the pre-built .apk file on your Android phone here.\nüôè Comments \u0026amp; Feedback That\u0026rsquo;s a wrap! In this post, I\u0026rsquo;ve shown you how you can start from a model, train it, and deploy it on a mobile device for edge inference.\ntip\n‚ö° In short we learned how to:\nLoad a SOTA classification model from TIMM and train it with Fastai. Export the trained model with TorchScript for inference. Create a functional Android app and run the model inference on your device. üìù NOTE: View the codes for the entire post on my GitHub repo here.\nWhat\u0026rsquo;s next? If you\u0026rsquo;d like to learn about how I deploy a cloud based object detection model on Android, check it out here.\nI hope you\u0026rsquo;ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or drop me a message. Alternatively you can also comment on this Hacker News thread.\nTired of training models that never see the light of day? Don\u0026#39;t let your hard work go to waste! In this üßµ, I\u0026#39;ll show you how to pick from over 900+ models from TIMM by @wightmanr , train them with Fastai by @jeremyphoward , and deploy them on Android ‚Äì all for free. pic.twitter.com/25pgunaJNM\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) February 14, 2023 ","description":"Learn and deploy over 900+ cutting edge PyTorch classification models on Android. ","permalink":"http://localhost:1313/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/","title":"PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter","type":"Portfolio"},{"content":"‚è≥ Last Updated: March 27, 2023.\n‚úÖ Motivation As a data scientist, you might be tempted to jump into modeling as soon as you can. I mean, that\u0026rsquo;s the fun part, right?\nBut trust me, if you skip straight to modeling without taking the time to really understand the problem and analyze the data, you\u0026rsquo;re setting yourself up for failure.\nI\u0026rsquo;ve been there.\nYou might feel like a superstar, but you\u0026rsquo;ll have with a model that doesn\u0026rsquo;t work ü§¶‚Äç‚ôÇÔ∏è.\nBut how do we even begin inspecting large datasets of images effectively and efficiently? And can we really do it on a local computer quickly, for free?\nSounds too good to be true eh?\nIt\u0026rsquo;s not, with üëá\n‚ö° fastdup fastdup is a tool that let us gain insights from a large image/video collection. You can manage, clean, and curate your images at scale on your local machine with a single CPU. It\u0026rsquo;s incredibly easy to use and highly efficient.\nAt first, I was skeptical. How could a single tool handle my data cleaning and curation needs on a single CPU machine, especially if the dataset is huge? But I was curious, so I decided to give it a try.\nAnd I have to say, I was pleasantly surprised.\nfastdup lets me clean my visual data with ease, freeing up valuable resources and time.\nHere are some superpowers you get with fastdup. It lets you identify:\nfastdup superpowers. Source: fastdup GitHub. In short, fastdup is üëá\nFast: Efficient C++ engine processes up to 7000 images in less than 3 minutes with a 2-core CPU (Google Colab). Scalable: Handles up to 400M images on a single CPU machine. Unsupervised: Runs on unlabeled (or labeled) image/video data. Cost: Basic functions are free to use. Process up to 12M images on a $1 cloud machine budget. The best part? fastdup is free.\nIt\u0026rsquo;s easy to get started and use. The authors of fastdup even used it to uncover over 1.2M duplicates and 104K data train/validation leaks in the ImageNet-21K dataset here.\ntip\n‚ö° By the end of this post, you will learn how to:\nInstall fastdup and run it on your local machine. Find duplicate and anomalies in your dataset. Identify wrong/confusing labels in your dataset. Uncover data leak in your dataset. üìù NOTE: All codes used in the post are on my Github repo. Alternatively, you can run this example in Colab.\nIf that looks interesting, let\u0026rsquo;s dive in.\nüìñ Installation To start, run:\npip install fastdup==0.909 I\u0026rsquo;m using version 0.909 for this post but, feel free to use the latest version available if they are compatible.\nüñº Dataset I will be using an openly available image classification dataset from Intel. The dataset contains 25,000 images (150 x 150 pixels) of natural scenes from around the world in 6 categories:\nbuildings forest glacier mountain sea tree Samples from dataset. tip\nI encourage you to pick a dataset of your choice in running this example. You can find some inspiration here. üèãÔ∏è‚Äç‚ôÄÔ∏è fastdup in Action: Discovering Data Issues Next, download the data locally and organize them in a folder structure. Here\u0026rsquo;s the structure I have on my computer.\n‚îú‚îÄ‚îÄ scene_classification ‚îú‚îÄ‚îÄ data ‚îÇ ‚îú‚îÄ‚îÄ train_set ‚îÇ | ‚îú‚îÄ‚îÄ buildings ‚îÇ | | ‚îú‚îÄ‚îÄ image1.jpg ‚îÇ | | ‚îú‚îÄ‚îÄ image2.jpg ‚îÇ | | ‚îú‚îÄ‚îÄ ... ‚îÇ | ‚îú‚îÄ‚îÄ mountain ‚îÇ | | ‚îú‚îÄ‚îÄ image10.jpg ‚îÇ | | ‚îú‚îÄ‚îÄ image11.jpg ‚îÇ | | ‚îú‚îÄ‚îÄ ... | ‚îú‚îÄ‚îÄ valid_set | | ‚îú‚îÄ‚îÄ buildings | | | ‚îú‚îÄ‚îÄ image100.jpg ‚îÇ | | ‚îú‚îÄ‚îÄ ... | ‚îî‚îÄ‚îÄ test_set ‚îî‚îÄ‚îÄ report ‚îú‚îÄ‚îÄ train ‚îú‚îÄ‚îÄ train_valid ‚îî‚îÄ‚îÄ valid note\nDescription of folders:\ndata/ \u0026ndash; Folder to store all datasets. report/ \u0026ndash; Directory to save the output generated by fastdup. üìù NOTE: For simplicity, I\u0026rsquo;ve also included the datasets in my Github repo.\nTo start checking through the images, create a Jupyter notebook and run:\nwork_dir = \u0026#34;scene_classification/report/\u0026#34; images_dir = \u0026#34;scene_classification/data/\u0026#34; fd = fastdup.create(work_dir, images_dir) fd.run() note\nParameters for the run method:\nwork_dir \u0026ndash; Path to save the output artifacts from the run. input_dir \u0026ndash; Path to the folder containing images. üìù NOTE: More info on other parameters here.\nfastdup will run through all images in the folder to check for issues. How long it takes depends on how powerful is your CPU. On my machine, with an Intel Core‚Ñ¢ i9-11900 it takes about 1 minute to check through (approx. 25,000) images in the folder ü§Ø.\nOnce complete, a run summary will be printed out.\nFound a total of 118 fully identical images (d\u0026gt;0.990), which are 0.16 % Found a total of 108 nearly identical images(d\u0026gt;0.980), which are 0.15 % Found a total of 11908 above threshold images (d\u0026gt;0.900), which are 16.31 % Found a total of 2433 outlier images (d\u0026lt;0.050), which are 3.33 % You\u0026rsquo;ll also find a bunch of output files in the work_dir folder. We can now visualize them accordingly.\nThe upcoming sections show how you can visualize duplicates, anomalies, confusing labels and data leakage.\nüßë‚Äçü§ù‚Äçüßë Duplicates First, let\u0026rsquo;s see if there are duplicates in the dataset with:\nfd.vis.duplicates_gallery() note\nOther parameters for duplicates_gallery method:\nsave_path \u0026ndash; Path to save the visualization. Defaults work_dir. num_images \u0026ndash; The max number of images to display. Defaults to 20. üìù NOTE: More info on other parameters here.\nYou\u0026rsquo;d see something like the following üëá\nHere, we can already spot a few issues in our dataset. As shown below, 10234.jpg and 7654.jpg are exact duplicates. We know that through the Distance score of 1.0.\nBut that\u0026rsquo;s not the only problem. They are labeled differently! One if labeled glacier and the other mountain. If you look further there are a bunch of other duplicates too.\nIn this example both the duplicates belong to the train_set (see the path of the images above). In the event they do not belong to the same set, then we have üëá\nüö∞ Data Leakage In machine learning data leakage is when data from outside the training dataset is used to train the model. Read more about data leakage in machine learning here.\nCan you spot any data leakage in the gallery above?\nThe gallery above shows that we indeed discovered a data leakage:\nTrain-validation leak - Image from the training set is found in the validation set. Train-test leak - Image from the training set is found in the test set. If you train a model on this data, you\u0026rsquo;d get a model that performs extremely well on the test set. Along the process, you also convinced yourself that the model is robust. When in reality, it\u0026rsquo;s not.\nThis is why models fail in production.\nIt\u0026rsquo;s because the model might just memorize the training set without generalizing to unseen data. That\u0026rsquo;s why it\u0026rsquo;s important to make sure the training and validation/test sets do not contain duplicates!\ntip\nA validation set consists of representative and non-overlapping samples from the train set and is used to evaluate models during training. Overlapping images in the train and validation set may lead to poor performance on new data. The way we craft our validation set is extremely important to ensure the model does not overfit. Spending time crafting your validation set takes a little effort, but will pay off well in the future. Rachel Thomas from Fastai wrote a good piece on how to craft a good validation set.\nThat\u0026rsquo;s how easy it is to find duplicate images and remove them from your dataset! Let\u0026rsquo;s see if we can find more issues.\nü¶Ñ Anomalies Similar to duplicates, it\u0026rsquo;s easy to visualize anomalies in your dataset:\nfd.vis.outliers_gallery() You\u0026rsquo;d see something like the following üëá\nWell, what do we find here? The image below is certainly a mistake.\nIt\u0026rsquo;s not a broken image (it\u0026rsquo;s still a valid image file) but there\u0026rsquo;s no useful information on the image for it to be in the test_set.\nThis type of image is common in large dataset such as LAION and ImageNet.\nAll the other images above don\u0026rsquo;t look too convincing to me either. Take a look at the images labeled as forest and glacier below.\ntip\nNote that the lower the Distance value, the more likely it will be an outlier. I guess you can evaluate the rest if they belong to the right classes as labeled.\nüíÜ Wrong or Confusing Labels One of my favorite capabilities of fastdup is finding wrong or confusing labels. Similar to previous sections, we can simply run:\nfd.vis.similarity_gallery() You\u0026rsquo;d see something like üëá\nThat looks like a lot of information. Let\u0026rsquo;s break it down a little.\nWhat\u0026rsquo;s happening here is that, under the hood, fastdup finds images that are similar to one another at the embedding level but are assigned different labels.\nFor instance, the glacier image below is a duplicate of another image labeled mountain and another image of glacier.\nIt is important to address these confusing labels because if the training data contains confusing or incorrect labels, it can negatively impact the performance of the model.\nüôè Comments \u0026amp; Feedback That\u0026rsquo;s a wrap!\ntip\nIn this post I\u0026rsquo;ve shown you how to:\nInstall fastdup and run it on your local machine. Find duplicate and anomalies in your dataset. Identify wrong/confusing labels in your dataset. Uncover data leak in your dataset. üìù NOTE: Find all codes follow along in my GitHub repo here.\nIf you\u0026rsquo;d like to remove problematic files programatically refer to my other blog post here.\nBy using fastdup and cleaning your dataset, you saved yourself:\nUnnecessary labeling cost. Long computation/training time. Headaches from debugging model predictions due to problems in data. I believe fastdup is one of the easiest tools to get started for data cleaning. It‚Äôs a low-hanging fruit and ought to be in your toolkit if you‚Äôre working with image datasets.\nIf you\u0026rsquo;re interested to learn more, I\u0026rsquo;ve trained a deep learning classification model on the clean version of the data using Fastai. View the training notebook here. The accuracy on the validation set is approximately 94.9% - comparable to the winning solutions of the competition (96.48% with ensembling).\nI hope you\u0026rsquo;ve enjoyed and learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or drop me a message.\nCleaning image datasets is painful, esp if your dataset is huge.\nIf you find yourself sifting through images for duplicates and anomalies, you\u0026#39;re not alone. I\u0026#39;ve been there. üò≠\nI\u0026#39;ll show how you can inspect large image dataset efficiently for free.\nA üßµ pic.twitter.com/2J4W6GF4fy\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) January 26, 2023 ","description":"Learn how to use fastdup to find image duplicates, anomalies, wrong labels and data leakage. Say goodbye to cluttered folders.","permalink":"http://localhost:1313/portfolio/fastdup_manage_clean_curate/","title":"fastdup: A Powerful Tool to Manage, Clean \u0026 Curate Visual Data at Scale on Your CPU - For Free.","type":"Portfolio"},{"content":"üí´ AI/ML Data Talks Podcast In this podcast episode I share about my journey and transition from academia to industry and the lessons I learned along the way.\nDuring our chat, we talk about some of the hottest topics in machine learning, like What is MLOps? Data Drift vs Concept Drift, and Monitoring Machine Learning Model.\nWe also talked about some insights into the latest AI and ML trends and ecosystem. Specifically how important it is for various tools to be able to communicate with each other, and how ZenML helps with that.\nAdditionally, if you\u0026rsquo;re interested in pursuing a career in AI or ML, I also discussed what worked for me during my transition and potential pitfalls.\nStarting projects and writing about them publicly is a great way to solidify your understanding of the material.\nSo if you\u0026rsquo;re curious about the current state of AI and ML, and you\u0026rsquo;re looking for some practical advice on how to get started in the field, this episode is definitely for you!\nCatch my discussion with Poo Kuan Hoong, PhD - Google Developer Expert and Lead Data Scientist at BAT üëá\nüëç Tips to Get Started with ML If you\u0026rsquo;re interested in AI or ML engineering but don\u0026rsquo;t know where to start, now is a great time to jump in. There are many resources available to help you learn and get started with these technologies.\nOne important tip is to start writing about your projects and experiences. Writing can help you solidify your understanding and connect the dots between different concepts. Don\u0026rsquo;t be afraid to learn in public and share your work, even if you don\u0026rsquo;t think anyone else will read it.\nRemember, writing is for you, and it can do more good for your own learning than for anyone else. So do yourself this favor.\nIf you don\u0026rsquo;t know what to write about, start with writing to yourself six months ago. What you wish you knew? What are the lessons learned? What advice you\u0026rsquo;d give to someone trying to walk the same path you took?\nAnother tip is to practice coding and work through different tutorials and exercises. Don\u0026rsquo;t get stuck in \u0026ldquo;tutorial hell\u0026rdquo; by only following tutorials without applying your knowledge to real-world problems.\nFinally, don\u0026rsquo;t be discouraged by the steep learning curve of AI and ML especially if you\u0026rsquo;re coming from another field. With persistence and dedication, you can develop the skills to become a successful AI or ML engineer.\nüìñ Lessons Learned One of the key lessons learned is the importance of staying up-to-date with the latest industry trends and tools. While academic research focuses on developing new theories and algorithms, the industry is more focused on practical applications that can generate business value.\nAnother important lesson is the need to develop strong communication skills, as working with cross-functional teams requires the ability to effectively communicate complex technical concepts to non-technical stakeholders.\nAdditionally, it is important to be adaptable and willing to learn new skills as the field of machine learning is constantly evolving. Finally, it is essential to maintain a passion for the field, as it is the driving force behind success in both academia and industry.\n‚úÖ Conclusion Transitioning from academia to industry as an AI or ML engineer can be a challenging but rewarding experience.\nIt is important to keep learning and adapting to new technologies and methodologies, as well as to continuously improve communication and collaboration skills.\nWriting and blogging about projects and experiences can also be a valuable tool for personal growth and development. By studying case studies and success stories, one can gain insights into the real-world applications of AI and ML in different industries.\nWith the right mindset and approach, the transition can be a fulfilling journey towards a successful and fulfilling career.\n","description":"Learn from an experienced AI/ML Engineer about their journey from academia to industry and get tips on how to become an AI/ML Engineer yourself.","permalink":"http://localhost:1313/blog/podcast_ai_ml_data_talks_episode_fifteen/","title":"From Academia to Industry: Insights from an AI/ML Engineer","type":"Blog"},{"content":"‚ú® Introduction Running AI on the Edge has become a widely discussed topic in the world of artificial intelligence. At its core, the concept of Edge AI involves putting AI algorithms as close to the user or data as possible. This is in contrast to the traditional server-based approach, where computations are carried out in remote servers, leading to slow response times and the need to transfer data back and forth.\nIn this talk, we will delve into what it means to run AI on the Edge, the benefits of this approach and how it differs from the traditional server-based one. We will also look at the various reasons why industries are adopting Edge AI, including latency, robustness, cost reduction, privacy, and more. By the end, you will have a better understanding of Edge AI and its significance in the world of technology.\nHere\u0026rsquo;s my conversation Sage Elliot from Whylabs.\nü§ñ What is Edge AI? Edge AI is the implementation of Artificial Intelligence (AI) models directly on the device where data is generated, rather than relying on a remote server.\nThis approach brings the AI models closer to the data source, making the process faster and reducing the latency. Edge AI has become increasingly popular due to its ability to reduce the amount of data being sent to remote servers, improve privacy, and save cost.\nüèãÔ∏è‚Äç‚ôÄÔ∏è Why run AI on the Edge? There are several reasons why one might want to run AI on the Edge. Here are some of the most significant reasons:\nLatency: Edge AI can deliver real-time results, making it ideal for applications that require immediate responses such as self-driving cars.\nRobustness: By having the AI models on the device, Edge AI can provide results even in the absence of an internet connection, which makes it more robust and reliable.\nCost reduction: Sending large data to remote servers for processing can be expensive, especially in industries with limited access to Wi-Fi or landline. Edge AI eliminates this cost by processing the data directly on the device.\nPrivacy: Keeping data within the device protects sensitive information from being transferred to remote servers, thus improving privacy.\nüí´ Use Cases of Edge AI Edge AI has found applications in several industries, including:\nComputer Vision: Edge AI can be used to implement face recognition or image classification on devices like smartphones. Read some of my recent projects on running object detection models real-time on a CPU using OpenVINO and DeepSparse.\nAgriculture: Edge AI can be used in agriculture to monitor crops and provide real-time predictions on their growth.\nHealthcare: Edge AI can be used in medical devices to process data and provide results without relying on remote servers.\nAutomotive: Self-driving cars require real-time predictions to make critical decisions. Edge AI provides a solution to this by processing data directly on the device.\nIn conclusion, Edge AI is a powerful technology that has revolutionized how data is processed and analyzed. It provides faster, more reliable, and more secure results than traditional server-based AI solutions.\n","description":"Deploy and share your models to iterate quickly.","permalink":"http://localhost:1313/blog/talk_rsqrd_applications_edge_ai/","title":"Applications of Edge AI with Sage Elliot at Whylabs","type":"Blog"},{"content":"üí´ Takeaways In this talk, I discussed why ML pipelines should be built from the get-go. Here are some of the key takeaways:\n1Ô∏è‚É£ Despite the hype around machine learning, 55% of companies have not deployed a single ML model yet, and those who have are struggling to maintain and scale them.\n2Ô∏è‚É£ Putting ML in production is not just about ML, but also about engineering. Many companies are doing more engineering than ML to solve various issues that arise in the pipeline.\n3Ô∏è‚É£ There are many ML Ops tools and platforms available, but this can be overwhelming and disorienting for beginners. It\u0026rsquo;s important to start with a clear plan and a focus on the most critical parts of the pipeline.\n4Ô∏è‚É£ Data cleaning and preprocessing can take up to 80% of the data scientist\u0026rsquo;s time, so it\u0026rsquo;s important to have efficient processes and tools to handle this.\nWatch the full video to learn more about these challenges and how to tackle them! üé•üë®‚Äçüè´\nThe talk was hosted with FuzzyLabs and Skillsmatter. View the talk on Skillsmatter webpage here.\nüî• Motivation Machine Learning (ML) has become an integral part of various sectors, including finance, healthcare, e-commerce, and more. Despite its popularity, many companies struggle to deploy and maintain ML models in production, with 55% of companies yet to deploy a single ML model.\nML pipelines offer a solution to this problem. They\u0026rsquo;re a sequence of steps that enable organizations to manage, deploy, and maintain their ML models in a production environment.\nIn this talk, we discuss why ML pipelines should be built from the get-go and how they can help organizations overcome the challenges of deploying and maintaining their ML models. We will also explore the different stages of an ML pipeline, the benefits of using ML pipelines, and best practices for building them. So, let\u0026rsquo;s get started!\n‚úÖ Why ML Pipelines The process of implementing machine learning models can be complex and challenging. While developing a model is an important aspect, it\u0026rsquo;s just one piece of the puzzle. There are several other steps involved, such as data cleaning, feature engineering, training, testing, deployment, and monitoring. Therefore, to ensure a smooth and efficient process, it\u0026rsquo;s essential to build a machine learning pipeline from the get-go.\nA machine learning pipeline is a set of processes that help in managing and automating machine learning workflows. It involves several stages, such as data collection, data cleaning, data pre-processing, feature extraction, model training, model evaluation, model deployment, and monitoring. Building a pipeline allows you to automate most of these processes, saving you time and effort in the long run.\nMoreover, an ML pipeline helps in maintaining consistency, accuracy, and reproducibility of results. With a well-built pipeline, you can ensure that every time you run the model, you\u0026rsquo;ll get the same results. This is crucial, especially in the industry, where minor discrepancies can result in significant losses.\nü§ñ Challenges of Implementing ML Pipelines Despite the growing popularity of machine learning, many companies have yet to deploy any machine learning models, and even those who have deployed them are struggling to maintain them. In fact, according to recent studies, more than 55% of companies have not deployed a single machine learning model yet. For those that have, their models are not deployed to a majority of their products, but instead comprise only a small portion of their products. This raises the question of what went wrong.\nThe dream of machine learning in production is that data collection, training the model, and putting it into production are three steps that happen in sequence. However, the reality is that the process is not that straightforward. In actuality, it looks more like an infinite cycle of back and forth between data collection, training the model, testing, and deployment. This cycle can take a long time, and some people continue looping until their model is good enough to put into production.\nThe majority of the problem with putting ML in production is not in the ML itself, but it\u0026rsquo;s more of the engineering side of things. Most of the time, people are doing more engineering than ML. To solve various engineering issues, it takes a lot of time, and there are now a lot of companies that are jumping in to help with all of these issues. This has resulted in hundreds of tools, and if you\u0026rsquo;re new to ML Ops, it can be overwhelming, and you don\u0026rsquo;t know where to start or which one to pick.\nThe problem with inefficiency is also a challenge in ML in the industry. According to a survey conducted in 2012, the 80/20 data science dilemma reveals that most data scientists spend most of their time finding, cleaning, and reorganizing huge amounts of data, which is an inefficient strategy when the real thing they should be focusing on is building the model.\nüéÑ How to Implement an ML Pipeline There are many existing tools available to implement machine learning pipelines. Some of the most popular ones are Apache Airflow, Kubeflow, TensorFlow Extended, and ZenML.\nAn advantage of ZenML is that its an open-source MLOps framework to unify all the components in your pipeline. With ZenML you can streamline your ML workflows, improve collaboration, and accelerate your results. Check out the GitHub repo here.\nHere is a full list of all stack components currently supported in ZenML, with a description of that components role in the MLOps process:\nComponent Description Orchestrator Orchestrating the runs of your pipeline Artifact Store Storage for the artifacts created by your pipelines Container Registry Store for your containers Secrets Manager Centralized location for the storage of your secrets Step Operator Execution of individual steps in specialized runtime environments Model Deployer Services/platforms responsible for online model serving Feature Store Management of your data/features Experiment Tracker Tracking your ML experiments Alerter Sending alerts through specified channels Annotator Labeling and annotating data Data Validator Data and model validation Image Builder Builds container images. Regardless of which tool you pick, it is important to make sure you are clear with each step that takes place in the pipeline. Here are some of the important points mentioned in the talk:\nDefine your objectives: Before you start building your ML pipelines, it\u0026rsquo;s essential to define your objectives. What are you trying to achieve with your model? What kind of data will you need? How will you evaluate the performance of your model?\nData collection: Once you have defined your objectives, you need to collect the data that you will use to train your model. It\u0026rsquo;s crucial to ensure that your data is clean, accurate, and representative of the problem you\u0026rsquo;re trying to solve.\nPreprocessing: Once you have your data, you need to preprocess it to prepare it for your model. This may involve cleaning, normalization, feature engineering, and more. It\u0026rsquo;s important to ensure that your preprocessing steps are well-documented and reproducible.\nModel training: After preprocessing, you can train your model using your data. There are many different algorithms and techniques you can use, depending on your objectives and the nature of your data. Again, it\u0026rsquo;s essential to keep track of your experiments and document your results.\nModel evaluation: Once you have trained your model, you need to evaluate its performance. This may involve testing your model on a holdout dataset or using cross-validation techniques. It\u0026rsquo;s important to use appropriate evaluation metrics and to be aware of any potential biases in your data.\nDeployment: Once you have a model that performs well, you need to deploy it in a production environment. This may involve integrating your model with other systems, creating APIs, and ensuring scalability and reliability. It\u0026rsquo;s important to test your model thoroughly before deployment and to monitor its performance in production.\nBy following these steps, you can build ML pipelines from the get-go that are robust, efficient, and effective.\nIn addition to the points mentioned above, it\u0026rsquo;s important to emphasize the need for model monitoring. Once a model is deployed into production, it\u0026rsquo;s crucial to keep an eye on its performance and make sure it\u0026rsquo;s still providing accurate predictions. This is where model monitoring comes in.\nBy monitoring the model\u0026rsquo;s performance metrics, such as accuracy and precision, we can quickly detect if the model is not performing as expected. This can be done through various tools, such as dashboards and alerts, that notify the team if the model\u0026rsquo;s performance drops below a certain threshold.\nMoreover, model monitoring allows for continuous improvement of the model. By analyzing the performance data and identifying areas for improvement, the team can retrain the model with new data to improve its accuracy and reliability.\nüåø Conclusion Machine learning in production goes beyond training models. Having an ML pipeline from the get go ensures reproducibility and reliability of the model in production.\nThere are various MLOps tools to build a pipeline. Pick the one you\u0026rsquo;re comfortable with considering various aspects such as interoperability, ease of use and collaboration support.\n","description":"Deploy and share your models to iterate quickly.","permalink":"http://localhost:1313/blog/talk_ml_pipelines_from_the_get_go/","title":"ML Pipelines from the Get Go (Without Tears)","type":"Blog"},{"content":"üî• Introduction The rapid advancement in computer vision technology has led to the development of sophisticated models that can perform complex tasks, such as object detection and segmentation, with high accuracy. However, these models often require high computational resources and can be slow when running on a CPU. This can pose a challenge for real-time applications, such as surveillance and security, where quick detection and analysis of objects is critical.\nIn this talk, we will discuss a demonstration of detecting pistols in real-time using a CPU. We will also outline the methodology and lessons learned from this project and provide some advice for those looking to implement similar computer vision models on a CPU.\nüõó Methodology The methodology used in this project involved optimizing the computer vision model and the hardware configuration to ensure fast inference speeds on a CPU. This involved several key steps, including:\nModel selection: We selected a state-of-the-art object detection model that was well-suited for real-time applications and had good accuracy.\nModel optimization: We optimized the model architecture and parameters to reduce the computational requirements and improve the inference speed.\nHardware optimization: We configured the hardware to ensure that the CPU was being utilized efficiently and that the memory and storage requirements were optimized.\nTesting and validation: We tested and validated the system using real-world data to ensure that it was working as expected and that the accuracy and inference speed met the desired requirements.\n‚úÖ Lessons Learned Here are some of the lessons I learned:\nChoose the right model: Select a state-of-the-art object detection model that is well-suited for real-time applications and has good accuracy.\nOptimize the model: Optimize the model architecture and parameters to reduce the computational requirements and improve the inference speed.\nConfigure the hardware properly: Configure the hardware to ensure that the CPU is being utilized efficiently and that the memory and storage requirements are optimized.\nTest and validate using real-world data: Test and validate the system using real-world data to ensure that it is working as expected and that the accuracy and inference speed meet the desired requirements.\nüí´ Conclusion Running computer vision models on a CPU for fast inference speeds is a challenging but achievable goal. By following the steps outlined in this post, such as optimizing the model and hardware configuration and testing and validating the system using real-world data, you can achieve high accuracy and minimal latency in real-time applications. This can have significant practical implications for industries such as surveillance and security, where quick detection and analysis of objects is critical.\n","description":"Deploy and share your models to iterate quickly.","permalink":"http://localhost:1313/blog/talk_cool_data_science_cpu_pistol_detection/","title":"Cool Data Projects Show with Kristen - Gun Detection on CPU","type":"Blog"},{"content":"üí° Introduction This talk was given to the Tensorflow Deep Learning Malaysia Facebook group during the June 2022 online meetup. The group had over 7.5k members consisting of audience from various background related to artificial intelligence in Malaysia.\nThe goal of the talk is to introduce the members to existing open-source tools they can use to deploy models on the cloud and edge.\nHalf of the audience has no experience with deep learning. Hence, the talk was tailored to beginners in the field.\nü™Ç The Deep Gap I started the talk by introducing my background as an academic and my experience in the field.\nI started exploring the field of deep learning (DL) in 2013. Having been in the field for over 9+ years now, I shared my stories on how I arrived at this point and my observation of the DL field over the years.\nI also shared that being in academia, we are incentivized for publications more than anything else. As a result, many \u0026ldquo;groundbreaking\u0026rdquo; works in DL stopped at the point of publication - which is a pity. Had the works continue beyond that, they could have the potential to change the industry.\nThe consequence?\nMore than 85% of machine learning models fail to make it into production.\nGartner Survey I unveiled that the deep gap is that not enough attention is placed on productionizing/deploying DL models in real world applications.\n‚õè Technical Walkthrough I transition the talk to share on some of my recent projects on deploying DL models.\nI elaborated on two general categories of deployment environments:\nCloud Deployment. Edge Deployment. üåß Cloud Deployment Cloud deployment is a setting where the trained DL model is hosted on the cloud infrastructure.\nI shared how I trained a state-of-the-art VFNet model with IceVision and deploy them on an Android phone using the Hugging Face Hub ecosystem.\nThe details can be found in the following posts:\nTraining a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images.\nHow to Deploy Object Detection Models on Android with Flutter.\nüì± Edge Deployment Edge deployment is a setting where the trained DL model is placed on a physical computing hardware (also known as edge device) where the data is collected.\nI shared how I trained a state-of-the-art object detection model, YOLOX to accurately detect license plates on Malaysian vehicles. I also shared how I optimize the model to run 10x faster (at 50 FPS) on a CPU using the OpenVINO toolkit.\nI briefly talked about an alternative to the OpenVINO toolkit which can accelerate inference up to 180 FPS using DeepSparse and SparseML library by Neural Magic.\nThe details can be found in the following posts:\nFaster than GPU: How to 10x your Object Detection Model and Deploy on CPU at 50+ FPS.\nSupercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU.\nüçß Takeaways Here are the takeaways from the brief talk\ntip\nBegin with deployment in mind as the end goal. The gap is deeper at the deployment side. Many open-source tools make it easy to deploy models. MLOps - hot topic worth exploring. üìΩ Video \u0026amp; Presentation Deck Recorded video üëá My presentation deck üëá\n","description":"Deploy and share your models to iterate quickly.","permalink":"http://localhost:1313/blog/talk_tfdl_deploying_dl_without_tears/","title":"Leveraging Open Source Tools to Deploy Models (Without üò•)","type":"Blog"},{"content":"üî• Motivation After months of searching, you\u0026rsquo;ve finally found the one.\nThe one object detection library that just works. No installation hassle, no package version mismatch, and no CUDA errors.\nI\u0026rsquo;m talking about the amazingly engineered YOLOv5 object detection library by Ultralytics.\nElated, you quickly find an interesting dataset from Roboflow and finally trained a state-of-the-art (SOTA) YOLOv5 model to detect firearms from image streams.\nYou ran through a quick checklist \u0026ndash;\nInference results, checked ‚úÖ COCO mAP, checked ‚úÖ Live inference latency, checked ‚úÖ You\u0026rsquo;re on top of the world.\nYou can finally pitch the results to your clients next Monday. At the back of your mind, you can already see your clients\u0026rsquo; impressed look on the astonishing feat.\nOn the pitching day, just when you thought things are going in the right direction. One of the clients asked,\n\u0026ldquo;Does your model run on our existing CPU?\u0026rdquo;\nYou flinched.\nThat wasn\u0026rsquo;t something you anticipated. You tried to convince them that GPUs are \u0026ldquo;the way forward\u0026rdquo; and it\u0026rsquo;s \u0026ldquo;the best way\u0026rdquo; to run your model in real-time.\nYou scanned the room and begin to notice the looks on their faces every time you said the word GPU and CPU.\nNeedless to say it didn\u0026rsquo;t go well. I hope nobody will have to face this awkward situation in a pitching session, ever. You don\u0026rsquo;t have to learn it the hard way, like I did.\nYou may wonder, can we really use consumer grade CPUs to run models in real-time?\nü¶æYES we can!\nI wasn\u0026rsquo;t a believer, but now I am, after discovering Neural Magic.\nIn this post, I show you how you can supercharge your YOLOv5 inference performance running on CPUs using free and open-source tools by Neural Magic.\ntip\nBy the end of this post, you will learn how to:\nTrain a SOTA YOLOv5 model on your own data. Sparsify the model using SparseML quantization aware training, sparse transfer learning, and one-shot quantization. Export the sparsified model and run it using the DeepSparse engine at insane speeds. P/S: The end result - YOLOv5 on CPU at 180+ FPS using only 4 CPU cores! üöÄ\ninfo\nIf you\u0026rsquo;re completely new to YOLOv5, get up to speed by reading this tutorial by Exxact. If that sounds exciting let\u0026rsquo;s dive in üßô\nüî© Setting Up üî´ Dataset The recent gun violence news had me thinking deeply about how we can prevent incidents like these again. This is the worst gun violence since 2012, and 21 innocent lives were lost.\nI\u0026rsquo;m deeply saddened, and my heart goes out to all victims of the violence and their loved ones.\nI\u0026rsquo;m not a lawmaker, so there is little I can do there. But, I think I know something in computer vision that might help. That\u0026rsquo;s when I came across the Pistols Dataset from Roboflow.\nThis dataset contains 2986 images and 3448 labels across a single annotation class: pistols. Images are wide-ranging: pistols in hand, cartoons, and staged studio-quality images of guns. The dataset was originally released by the University of Grenada.\nü¶∏ Installation Now let\u0026rsquo;s put the downloaded Pistols Dataset into the appropriate folder first. I will put the downloaded images and labels into the datasets/ folder.\nLet\u0026rsquo;s also put the sparsification recipes from SparseML into the recipes/ folder. More on recipes later.\nHere\u0026rsquo;s a high-level overview of my directory.\n‚îú‚îÄ‚îÄ req.txt ‚îú‚îÄ‚îÄ datasets ‚îÇ ‚îú‚îÄ‚îÄ pistols ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ train | | ‚îú‚îÄ‚îÄ valid ‚îú‚îÄ‚îÄ recipes ‚îÇ ‚îú‚îÄ‚îÄ yolov5s.pruned.md ‚îÇ ‚îú‚îÄ‚îÄ yolov5.transfer_learn_pruned.md ‚îÇ ‚îú‚îÄ‚îÄ yolov5.transfer_learn_pruned_quantized.md | ‚îî‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ yolov5-train ‚îú‚îÄ‚îÄ data | ‚îú‚îÄ‚îÄ hyps | | ‚îú‚îÄ‚îÄ hyps.scratch.yaml | | ‚îî‚îÄ‚îÄ ... | ‚îú‚îÄ‚îÄ pistols.yaml | ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ models_v5.0 | ‚îú‚îÄ‚îÄ yolov5s.yaml | ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ train.py ‚îú‚îÄ‚îÄ export.py ‚îú‚îÄ‚îÄ annotate.py ‚îî‚îÄ‚îÄ ... note\nreq.txt - Requirement file to install all packages used in this post.\ndatasets/ - Contains the train and validation images/labels downloaded from Roboflow.\nrecipes/ - Contains sparsification recipes from the SparseML repo.\nyolov5-train/ - Cloned directory from Neural Magic\u0026rsquo;s YOLOv5 fork.\nNOTE: You can explore further into the folder structure on my Github repo.\nwarning\nIMPORTANT: The sparsification recipes will only work with Neural Magic\u0026rsquo;s YOLOv5 fork and will NOT WORK with the original YOLOv5 by Ultralytics. For this post, we are going to use a forked version of the YOLOv5 library that will allow us to do custom optimizations in the upcoming section.\nTo install, all packages in this blog post, run the following commands\ngit clone https://github.com/dnth/yolov5-deepsparse-blogpost cd yolov5-deepsparse-blogpost/ pip install torch==1.9.0 torchvision==0.10.0 --extra-index-url https://download.pytorch.org/whl/cu111 pip install -r req.txt Or, if you\u0026rsquo;re just getting started, I\u0026rsquo;d recommend üëá\ntip\nüî• Run everything in Colab with a notebook I made here. ‚õ≥ Baseline Performance üî¶ PyTorch Now that everything\u0026rsquo;s in place, let\u0026rsquo;s start by training a baseline model with no optimization.\nFor that, run the train.py script in the yolov5-train/ folder.\npython train.py --cfg ./models_v5.0/yolov5s.yaml \\ --data pistols.yaml \\ --hyp data/hyps/hyp.scratch.yaml \\ --weights yolov5s.pt --img 416 --batch-size 64 \\ --optimizer SGD --epochs 240 \\ --project yolov5-deepsparse --name yolov5s-sgd note\n--cfg \u0026ndash; Path to the configuration file which stores the model architecture.\n--data \u0026ndash; Path to the .yaml file that stores the details of the Pistols dataset.\n--hyp \u0026ndash; Path to the .yaml file that stores the training hyperparameter configurations.\n--weights \u0026ndash; Path to a pretrained weight.\n--img \u0026ndash; Input image size.\n--batch-size \u0026ndash; Batch size used in training.\n--optimizer \u0026ndash; Type of optimizer. Options include SGD, Adam, AdamW.\n--epochs \u0026ndash; Number of training epochs.\n--project \u0026ndash; Wandb project name.\n--name \u0026ndash; Wandb run id.\nAll metrics are logged to Weights \u0026amp; Biases (Wandb) here.\nOnce training\u0026rsquo;s done, let\u0026rsquo;s run inference on a video with the annotate.py script.\npython annotate.py yolov5-deepsparse/yolov5s-sgd/weights/best.pt \\ --source data/pexels-cottonbro-8717592.mp4 \\ --engine torch \\ --image-shape 416 416 \\ --device cpu \\ --conf-thres 0.7 note\nThe first argument points to the .pt saved checkpoint.\n--source - The input to run inference on. Options: path to video/images or just specify 0 to infer on your webcam.\n--engine - Which engine to use. Options: torch, deepsparse, onnxruntime.\n--image-size \u0026ndash; Input resolution.\n--device \u0026ndash; Device to use for inference. Options: cpu or 0 (GPU).\n--conf-thres \u0026ndash; Confidence threshold for inference.\nNOTE: The inference output will be saved in the annotation_results/ folder.\nHere\u0026rsquo;s how it looks like running the baseline YOLOv5-S on an Intel i9-11900 using all 8 CPU cores.\nAverage FPS : 21.91 Average inference time (ms) : 45.58 Actually, the FPS looks quite decent already and might suit some applications even without further optimization.\nBut why settle when you can get something better? After all, that\u0026rsquo;s why you\u0026rsquo;re here, right? üòâ\nMeet üëá\nüï∏ DeepSparse Engine DeepSparse is an inference engine by Neural Magic that runs optimally on CPUs. It\u0026rsquo;s incredibly easy to use. Just give it an ONNX model and you\u0026rsquo;re ready to roll.\nLet\u0026rsquo;s export our .pt file into ONNX using the export.py script.\npython export.py --weights yolov5-deepsparse/yolov5s-sgd/weights/best.pt \\ --include onnx \\ --imgsz 416 \\ --dynamic \\ --simplify note\n--weight \u0026ndash; Path to the .pt checkpoint.\n--include \u0026ndash; Which format to export to. Options: torchscript, onnx, etc.\n--imgsz \u0026ndash; Image size.\n--dynamic \u0026ndash; Dynamic axes.\n--simplify \u0026ndash; Simplify the ONNX model.\nAnd now, run the inference script again, this time using the deepsparse engine and with only 4 CPU cores in the --num-cores argument.\npython annotate.py yolov5-deepsparse/yolov5s-sgd/weights/best.onnx \\ --source data/pexels-cottonbro-8717592.mp4 \\ --image-shape 416 416 \\ --conf-thres 0.7 \\ --engine deepsparse \\ --device cpu \\ --num-cores 4 Average FPS : 29.48 Average inference time (ms) : 33.91 Just like that, we improved the average FPS from 21+ (PyTorch engine on CPU using 8 cores) to 29+ FPS. All we did was use the ONNX model with the DeepSparse engine.\nP/S: We are done with just the baselines here! The real action only happens next - when we run sparsification with üëá\nüë®‚Äçüç≥ SparseML and Recipes Image from SparseML documentation page. Sparsification is the process of removing redundant information from a model. The result is a smaller and faster model.\nThis is how we speed up our YOLOv5 model, by a lot!\ntip\nIn general, there are 2 methods to sparsify a model - Pruning and Quantization.\nPruning - Removing unused weights in the model.\nQuantization - Forcing a model to use a less accurate storage format i.e. from 32-bit floating point (FP32) to 8-bit integer (INT8).\n‚ö° P/S: Used together or separately both pruning and quantization result in a smaller and faster model!\nHow do we sparsify models?\nUsing SparseML - an open-source library by Neural Magic. With SparseML you can sparsify neural networks by applying pre-made recipes to the model. You can also modify the recipes to suit your needs.\nnote\nThere are 3 methods to sparsify models with SparseML:\n1Ô∏è‚É£ Post-training (One-shot).\n2Ô∏è‚É£ Sparse Transfer Learning.\n3Ô∏è‚É£ Training Aware.\nNOTE: 1Ô∏è‚É£ does not require re-training but only supports dynamic quantization. 2Ô∏è‚É£ and 3Ô∏è‚É£ requires re-training and supports pruning and quantization which may give better results.\nYou may wonder, this sounds too good to be true!\nWhat\u0026rsquo;s the caveat?\nGood question!\nWith sparsification, you can expect a slight loss in accuracy depending on the degree of sparsification. Highly sparse models are usually less accurate than the original model but gains significant boost in speed and latency.\nWith the recipes from SparseML, the loss of accuracy ranges from 2% to 6%. In other words the recovery is 94% to 98% compared to the performance of the original model. In exchange, we gain phenomenal speedups, ranging from 2x to 10x faster!\nIn most situations, this is not a big deal. If the accuracy loss is something you can tolerate, then let\u0026rsquo;s sparsify some models already! ü§è.\n‚òùÔ∏è One-Shot The one-shot method is the easiest way to sparsify an existing model as it doesn\u0026rsquo;t require re-training.\nBut this only works well for dynamic quantization, for now. There are ongoing works in making one-shot work well for pruning.\nLet\u0026rsquo;s run the one-shot method on the baseline model we trained earlier. All you need to do is add a --one-shot argument to the training script, and specify a pruning --recipe. Remember to specify --weights to the location of the best checkpoint from the training.\npython train.py --cfg ./models_v5.0/yolov5s.yaml \\ --recipe ../recipes/yolov5s.pruned.md \\ --data pistols.yaml --hyp data/hyps/hyp.scratch.yaml \\ --weights yolov5-deepsparse/yolov5s-sgd/weights/best.pt \\ --img 416 --batch-size 64 --optimizer SGD --epochs 240 \\ --project yolov5-deepsparse --name yolov5s-sgd-one-shot \\ --one-shot It should generate another .pt in the directory specified in --name. This .pt file stores the quantized weights in int8 format instead of fp32 resulting in a reduction in model size and inference speedups.\nNext, let\u0026rsquo;s export the quantized .pt file into ONNX format.\npython export.py --weights yolov5-deepsparse/yolov5s-sgd-one-shot/weights/checkpoint-one-shot.pt \\ --include onnx \\ --imgsz 416 \\ --dynamic \\ --simplify And run an inference\npython annotate.py yolov5-deepsparse/yolov5s-sgd-one-shot/weights/checkpoint-one-shot.onnx \\ --source data/pexels-cottonbro-8717592.mp4 \\ --image-shape 416 416 \\ --conf-thres 0.7 \\ --engine deepsparse \\ --device cpu \\ --num-cores 4 Average FPS : 32.00 Average inference time (ms) : 31.24 At no re-training cost, we are performing 10 FPS better than the original model. We maxed out at about 40 FPS!\nThe one-shot method only took seconds to complete. If you\u0026rsquo;re looking for the easiest method for performance gain, one-shot is the way to go.\nBut, if you\u0026rsquo;re willing to re-train the model to double its performance and speed, read on üëá\nü§π‚Äç‚ôÇÔ∏è Sparse Transfer Learning With SparseML you can take an already sparsified model (pruned and quantized) and fine-tune it on your own dataset. This is known as Sparse Transfer Learning.\nThis can be done by running\npython train.py --data pistols.yaml --cfg ./models_v5.0/yolov5s.yaml --weights zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned_quant-aggressive_94?recipe_type=transfer --img 416 --batch-size 64 --hyp data/hyps/hyp.scratch.yaml --recipe ../recipes/yolov5.transfer_learn_pruned_quantized.md --optimizer SGD --project yolov5-deepsparse --name yolov5s-sgd-pruned-quantized-transfer The above command loads a sparse YOLOv5-S from Neural Magic\u0026rsquo;s SparseZoo and runs the training on your dataset.\nThe --weights argument points to a model from the SparseZoo. There are more sparsified models available in SparseZoo. I will leave it to you to explore which model works best.\nRunning inference with annotate.py results in Average FPS : 51.56 Average inference time (ms) : 19.39 We almost 2x the FPS from the previous one-shot method! Judging from the FPS value and mAP scores, Sparse Transfer Learning makes a lot of sense for most applications.\nBut, if you scrutinize further into the mAP metric on the Wandb dashboard, you\u0026rsquo;ll notice it\u0026rsquo;s slightly lower than the next method üí™.\n‚úÇ Pruned YOLOv5-S Here, instead of taking an already sparsified model, we are going to sparsify our model by pruning it ourselves.\nTo do that we will use a pre-made recipe on the SparseML repo. This recipe tells the training script how to prune the model during training.\nFor that, we slightly modify the arguments of train.py\npython train.py --cfg ./models_v5.0/yolov5s.yaml \\ --recipe ../recipes/yolov5s.pruned.md --data pistols.yaml \\ --hyp data/hyps/hyp.scratch.yaml \\ --weights yolov5s.pt --img 416 --batch-size 64 --optimizer SGD \\ --project yolov5-deepsparse --name yolov5s-sgd-pruned The only change here is the --recipe and the --name argument. Also, there is no need to specify the --epoch argument because the number of training epochs is specified in the recipe.\n--recipe tells the training script which recipe to use for the YOLOv5-S model. In this case, we are using the yolov5s.pruned.md recipe which only prunes the model as it trains. You can change how aggressive your model is pruned by modifying the yolov5s.pruned.md recipe.\nwarning\nIMPORTANT: The sparsification recipes are model dependent. Eg. YOLOv5-S recipes will not work with YOLOv5-L.\nSo make sure you get the right recipe for the right model. Check out other YOLOv5 pre-made recipes here.\nRunning inference, we find\nAverage FPS : 35.50 Average inference time (ms) : 31.73 The drop in FPS is expected compared to the Sparse Transfer Learning method because this model is only pruned and not quantized. But we gain higher mAP values.\nüî¨ Quantized YOLOv5-S We\u0026rsquo;ve seen the effects of pruning, what about quantization? Let\u0026rsquo;s run quantization for the YOLOv5-S model and see how it behaves.\nWe could run the quantization without training (one-shot). But for better effects let\u0026rsquo;s train the model for 2 epochs. Re-training for 2 epochs allow the weights to re-adjust to the quantized values and hence produce better results.\nThe number of training epochs is specified in the yolov5s.quantized.md file.\nLet\u0026rsquo;s run the train.py\npython train.py --cfg ./models_v5.0/yolov5s.yaml \\ --recipe ../recipes/yolov5s.quantized.md \\ --data pistols.yaml \\ --hyp data/hyps/hyp.scratch.yaml \\ --weights yolov5-deepsparse/yolov5s-sgd/weights/best.pt --img 416 \\ --batch-size 64 --project yolov5-deepsparse --name yolov5s-sgd-quantized Inferencing with annotate.py\nAverage FPS : 43.29 Average inference time (ms) : 23.09 We have a bump in the FPS compared to the pruned model. With careful observation you\u0026rsquo;d notice a misdetection at the 0:03 second.\nHere, we see that the quantized model is faster than the pruned model at the cost of detection accuracy. But note, in this model we\u0026rsquo;ve only trained for 2 epochs compared to 240 epochs with the pruned model. Re-training for longer may solve the misdetection issue.\nWe\u0026rsquo;ve seen how the YOLOv5-S model performs when it is\nOnly pruned Only quantized But, can we run both pruning and quantization?\nOf course, why not? ü§ñ\nü™ö Pruned + Quantized YOLOv5-S Now, let\u0026rsquo;s take it to the next level by running both pruning and quantization. Note the difference --recipe I\u0026rsquo;m using.\npython train.py --cfg ./models_v5.0/yolov5s.yaml \\ --recipe ../recipes/yolov5.transfer_learn_pruned_quantized.md \\ --data pistols.yaml \\ --hyp data/hyps/hyp.scratch.yaml \\ --weights yolov5s.pt --img 416 \\ --batch-size 64 --optimizer SGD \\ --project yolov5-deepsparse --name yolov5s-sgd-pruned-quantized Export with export.py and run inference with annotate.py. We get\nAverage FPS : 58.06 Average inference time (ms) : 17.22 On our Wandb dashboard this model scores the highest mAP and is also the fastest.\nIt\u0026rsquo;s getting the best of both! üéØ\nI wanted to end the post here. But there is still this nagging thought that I can\u0026rsquo;t ignore. It\u0026rsquo;s keeping me awake at night. So I had to do this ü§∑‚Äç‚ôÇÔ∏è.\nEvery night I wonder how fast can we run YOLOv5 on CPUs? I mean the maximum possible FPS with SparseML + DeepSparse.\nThis led me to üëá\nüöÄ Supercharging with Smaller Models In the YOLOv5 series, the YOLOv5-Nano is the smallest model of all. In theory, this should be the fastest.\nSo I\u0026rsquo;m putting my bets on this model. Let\u0026rsquo;s apply the same steps again with the YOLOv5-Nano model.\nAnd\n..\n..\nüöÄüöÄüöÄ\nAverage FPS : 101.52 Average inference time (ms) : 9.84 ü§Ø This is mindblowing! The max FPS hit the 180+ range. I never imagine these numbers are possible, especially using only 4 CPU cores.\nSeeing this, I can now sleep peacefully at night üò¥\nüöß Conclusion What a journey this has been.\nGone are the days when we need GPUs to run models in real-time. With DeepSparse and SparseML, you can get GPU-class performance on commodity CPUs.\ntip\nIn this post I\u0026rsquo;ve shown you how to:\nTrain a SOTA YOLOv5 model with your own data. Sparsify the model using SparseML quantization aware training and one-shot quantization. Export the sparsified model and run it using the DeepSparse engine at insane speeds. P/S: Check out codes on my GitHub repo.\nIf you ever get lost in the commands that I used for this post, fear not. I listed all commands I used to train all models on the README of the repo.\nAlso, feel to use the repo with your own dataset and give it a ‚≠ê if it helps your work.\nIf you enjoyed this post, you might also like the following post where I show how to accelerate your PyTorch Image Models (TIMM) 8x faster with ONNX Runtime and TensorRT.\nSeptember 30, 2024 Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime \u0026amp; Optimizations üôè Comments \u0026amp; Feedback I hope you\u0026rsquo;ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or drop me a message. Anyone can train a YOLOv5 (@ultralytics) nowadays. But deploying it on a CPU is such a PAIN.\nThe pain ends here.\nIn thisüßµ I\u0026#39;ll show you how I got insane speeds (180+ FPS) running YOLOv5 on a consumer CPU using 4 only coresü§Ø\nüî• P/S: I use open-source tools by @neuralmagic. pic.twitter.com/b2vFOf57Ax\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) June 8, 2022 ","description":"Learn the tips and tricks to accelerate YOLOv5 inference up to 180+ FPS on a CPU! For free!","permalink":"http://localhost:1313/portfolio/supercharging_yolov5_180_fps_cpu/","title":"Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU","type":"Portfolio"},{"content":"üí• Motivation tip\nBy the end of this post you will learn how to:\nSet up a Telegram bot with a Python wrapper library. Use the Gradio API to access the GPT-J model prediction. Host the Telegram bot on Hugging Face Spaces. By the end of this post, you\u0026rsquo;ll have your own Telegram bot that has access to the GPT-J-6B model. All for free.\nDeploying a state-of-the-art (SOTA) GPT-like language model on a chatbot can be tricky.\nYou might wonder how to access to the GPT model? Or which infrastructure to host the bot and the model? Should it be serverless? AWS? Kubernetes? ü§í\nYada.. yada.. yada..\nI get it. Things get complicated quickly. It\u0026rsquo;s not worth going down the rabbit hole especially if you\u0026rsquo;re only experimenting or prototyping a feature.\nIn this post, I will show you how I deploy a SOTA GPT-J model by EleutherAI on a Telegram bot.\nFor FREEüöÄ.\nBy the end of this post, you\u0026rsquo;ll have your very own Telegram bot that can query the GPT-J model with any text you send it üëá\nIf that looks interesting, let\u0026rsquo;s begin üë©‚Äçüíª\nü§ñ Token From the Mighty BotFather We shall start by appeasing the mighty BotFather who holds the key to the world of bots ü§ñ\nFirst, you must have a Telegram account. Create one here. It\u0026rsquo;s free.\nNext, set up a bot that is associated with your Telegram account. For that, let\u0026rsquo;s consult the mighty BotFather and initiate the bot creation.\nThis link brings you to the BotFather. Alternatively, type BotFather in the Telegram search bar. The first result leads you to the BotFather.\nNext, send /start to the BotFather to start a conversation. Follow the instructions given by the BotFather until you obtain a token for your bot.\nwarning\nKeep this token private. Anyone with this token has access to your bot. This video provides a good step-by-step visual guide on how to obtain a token from the BotFather. üêç Python Telegram Bot Telegram wasn\u0026rsquo;t written with Python. But we ‚ù§Ô∏è Python! Can we still use Python to code our bot?\n‚úÖ Yes! With a wrapper library like python-telegram-bot. python-telegram-bot provides a pure Python, asynchronous interface for the Telegram Bot API. It\u0026rsquo;s incredibly user-friendly too. You can start running your Telegram bot with only 8 lines of code üëá\n1 2 3 4 5 6 7 8 9 10 from telegram import Update from telegram.ext import Updater, CommandHandler, CallbackContext def hello(update: Update, context: CallbackContext) -\u0026gt; None: update.message.reply_text(f\u0026#39;Hello {update.effective_user.first_name}.\u0026#39;) updater = Updater(\u0026#39;YOUR-TOKEN\u0026#39;) updater.dispatcher.add_handler(CommandHandler(\u0026#39;start\u0026#39;, hello)) updater.start_polling() updater.idle() The above code snippet creates a Telegram bot that recognizes the /start command (specified on line 8). Upon receiving the /start command it calls the hello function on line 4 which replies to the user.\nHere\u0026rsquo;s how it looks like if you run the code üëá\nYes! It\u0026rsquo;s that simple! ü§ì\nNow all you have to do is specify other commands to call any other functions of your choice.\nBefore we do that, let\u0026rsquo;s first install python-telegram-bot via\npip install python-telegram-bot==13.11 warning\npython-telegram-bot is under active development. There are breaking changes starting with version 20 and onward. For this post, I\u0026rsquo;d recommend sticking with version \u0026lt;20. To run the bot, save the 8-line code snippet above into a .py file and run it on your computer. Remember to replace 'YOUR-TOKEN' on line 7 with your own token from the BotFather.\nI will save the codes as bot.py on my machine and run the script with\npython bot.py Voila!\nYour bot is now live and ready to chat. Search for your bot on the Telegram search bar, and send it the /start command. It should respond by replying a text back to you, just like in the screen recording above.\nüí° GPT-J and the Gradio API We\u0026rsquo;ve configured our Telegram bot. What about the GPT-J model? Unless you have a powerful computer that runs 24/7, I wouldn\u0026rsquo;t recommend running the GPT-J model on your machine (although you can).\nI recently found a better solution that you can use to host the GPT-J model. Anyone can use it, it runs 24/7, and best of all it\u0026rsquo;s free!\nEnter üëâ Hugging Face Hub.\nHugging Face Hub is a central place where anyone can share their models, dataset, and app demos. The 3 main repo types of the Hugging Face Hub include:\nModels - hosts models. Datasets - stores datasets. Spaces - hosts demo apps. The GPT-J-6B model is generously provided by EleutherAI on the Hugging Face Hub as a model repository. It\u0026rsquo;s publicly available for use. Check them out here.\nYou can interact with the model directly on the GPT-J-6B model repo, or create a demo on your Space. In this post, I will show you how to set up a Gradio app as a demo on Hugging Face Space to interact with the GPT-J-6B model.\nFirst, create a Space with your Hugging Face account. If you\u0026rsquo;re unsure how to do that, I wrote a guide here. Next, create an app.py file in your Space repo.\nHere\u0026rsquo;s the content of app.py üëá\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import gradio as gr title = \u0026#34;GPT-J-6B\u0026#34; description = \u0026#34;Gradio Demo for GPT-J 6B, a transformer model trained \\ using Ben Wang\u0026#39;s Mesh Transformer JAX. \u0026#39;GPT-J\u0026#39; refers to the class of \\ model, while \u0026#39;6B\u0026#39; represents the number of trainable parameters. \\ To use it, simply add your text, or click one of the examples to load them. \\ I\u0026#39;ve used the API on this Space to deploy the GPT-J-6B model on a Telegram bot. \\ Link to blog post below üëá\u0026#34; article = \u0026#34;\u0026lt;p style=\u0026#39;text-align: center\u0026#39;\u0026gt; \\ \u0026lt;a href=\u0026#39;https://dicksonneoh.com/portfolio/deploy_gpt_hf_models_on_telegram/\u0026#39; \\ target=\u0026#39;_blank\u0026#39;\u0026gt;Blog post\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#34; examples = [ [\u0026#39;The tower is 324 metres (1,063 ft) tall,\u0026#39;], [\u0026#34;The Moon\u0026#39;s orbit around Earth has\u0026#34;], [\u0026#34;The smooth Borealis basin in the Northern Hemisphere covers 40%\u0026#34;] ] gr.Interface.load(\u0026#34;huggingface/EleutherAI/gpt-j-6B\u0026#34;, inputs=gr.inputs.Textbox(lines=5, label=\u0026#34;Input Text\u0026#34;), title=title,description=description, article=article, examples=examples, enable_queue=True).launch() On line 22 we load the GPT-J-6B model from the EleutherAI model hub and serve the predictions on the Space with a Gradio app.\nCheck out my Gradio demo app on my Space. Or try them out üëá\nOther than having a user interface, hosting a Gradio app on Space also allows you to use the API endpoint to access the app from elsewhere. For example, I\u0026rsquo;ve used this feature to get model predictions on my Android app here.\nTo view the API, click on \u0026ldquo;view the api\u0026rdquo; button at the bottom of the Space. It brings you to the API page that shows you how to use the endpoint.\nAll we need to do now is send a POST request from our Telegram bot to access the GPT-J model prediction.\ndef get_gpt_response(text): r = requests.post( url=\u0026#34;https://hf.space/embed/dnth/gpt-j-6B/+/api/predict/\u0026#34;, json={\u0026#34;data\u0026#34;: [text]}, ) response = r.json() return response[\u0026#34;data\u0026#34;][0] Let\u0026rsquo;s add this function into the bot.py file we created earlier. Here\u0026rsquo;s mine\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from telegram import Update from telegram.ext import Updater, CommandHandler, CallbackContext, MessageHandler, Filters import requests from telegram import ChatAction import os def hello(update: Update, context: CallbackContext) -\u0026gt; None: intro_text = \u0026#34;\u0026#34;\u0026#34; ü§ñ Greetings human! \\n ü§ó I\u0026#39;m a bot hosted on Hugging Face Spaces. \\n ü¶æ I can query the mighty GPT-J-6B model and send you a response here. Try me.\\n ‚úâÔ∏è Send me a text to start and I shall generate a response to complete your text!\\n\\n ‚ÄºÔ∏è PS: Responses are not my own (everything\u0026#39;s from GPT-J-6B). I\u0026#39;m not conscious (yet).\\n Blog post: https://dicksonneoh.com/portfolio/deploy_gpt_hf_models_on_telegram/ \u0026#34;\u0026#34;\u0026#34; update.message.reply_text(intro_text) def get_gpt_response(text): r = requests.post( url=\u0026#34;https://hf.space/embed/dnth/gpt-j-6B/+/api/predict/\u0026#34;, json={\u0026#34;data\u0026#34;: [text]}, ) response = r.json() return response[\u0026#34;data\u0026#34;][0] def respond_to_user(update: Update, context: CallbackContext): update.message.chat.send_action(action=ChatAction.TYPING) response_text = get_gpt_response(update.message.text) update.message.reply_text(response_text) updater = Updater(\u0026#39;YOUR-TOKEN\u0026#39;) updater.dispatcher.add_handler(CommandHandler(\u0026#34;start\u0026#34;, hello)) updater.dispatcher.add_handler(MessageHandler(Filters.text \u0026amp; ~Filters.command, respond_to_user)) updater.start_polling() updater.idle() I\u0026rsquo;m gonna save this as app.py on my computer and run it via\npython app.py Now, your bot will respond to /start command by calling the hello function (Configured on line 32). Additionally, it will also respond to all non-command texts by calling the respond_to_user function (Configured on line 33).\nThat is how we get GPT-J\u0026rsquo;s response through the Telegram bot ü§ñ. If you\u0026rsquo;ve made it to this point, congratulations! We\u0026rsquo;re almost done!\ntip\nIf you wish to run the Telegram bot on your machine you can stop here. Bear in mind you need to keep your machine alive 24/7 for your bot to work. But, if you wish to take your bot to the next level üöÄ then read on üëá\nü§ó Hosting Your Telegram Bot A little-known feature that I discovered recently is that you can host your Telegram bot on Hugging Face Spaces ü§´.\nIf you create a new Space, upload the app.py script and a requirement.txt file, it will work out of the box!\nThe contents of requirements.txt are\npython-telegram-bot==13.11 requests==2.27.1 If all is well, the Space will start building, and your bot now functional. Now you don\u0026rsquo;t have to keep your computer alive 24/7 to run the bot.\nI\u0026rsquo;m not sure if this is a feature or a bug, but this is pretty neat eh? Free hosting for your bots! Now let\u0026rsquo;s create Skynet ü§ñ\nwarning\nJokes aside, make sure you don\u0026rsquo;t expose your Telegram token by putting them in the source code. To hide your token, create an environment variable for it. On your Space, click on the Settings tab and enter the Name and Value of the environment variable. Let\u0026rsquo;s put the name as telegram_token and the value, your Telegram token. On your app.py change line 31 to the following\nupdater = Updater(os.environ[\u0026#39;telegram_token\u0026#39;]) Now, you can freely share your codes without exposing your Telegram token! For completeness, you can view my final app.py here.\nüéâ Conclusion In this post, I\u0026rsquo;ve shown you how easily you can leverage SOTA models such as the GPT-J-6B and deploy it live on a Telegram bot.\ntip\nWe\u0026rsquo;ve walked through how to:\nSet up a Telegram bot with a Python wrapper library. Use the Gradio API to access the GPT-J model prediction. Host the Telegram bot on Hugging Face Spaces. Link to my Telegram bot here - Try it out.\nThe end result üëâ a 24/7 working Telegram bot that has access to the GPT-J-6B model ü•≥\nFor FREE üöÄ\nThat\u0026rsquo;s about a wrap! Congratulations on making it! So, what\u0026rsquo;s next?\ntip\nHere are some of my suggestions to level-up your bot:\nMake your bot multifunctional by creating commands that query other GPT models - try using a GPT-Neo model here. Check out other SOTA language models or hybrid models like DALL-E and deploy them on your bot. Create a Discord bot and deploy a model of your choice. I\u0026rsquo;d love to see what you create. Tag me in your Twitter/LinkedIn post! üòç\nüôè Comments \u0026amp; Feedback I hope you\u0026rsquo;ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter post or drop me a message. Deploying GPT-like language models on a chatbot is tricky.\nYou might wonder\n‚Ä¢ How to access the model?\n‚Ä¢ Where to host the bot?\nIn this üßµI walk you through how easily I deployed a GPT-J-6B model by #EleutherAI on a #Telegram bot with @huggingface and @Gradio.\nFor FREE üöÄ pic.twitter.com/z0uvnxksWt\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) May 20, 2022 ","description":"Leverage SOTA models on Hugging Face Hub - for free!","permalink":"http://localhost:1313/portfolio/deploy_gpt_hf_models_on_telegram/","title":"Deploying GPT-J Models on a Telegram Bot with Hugging Face Hub - For Free","type":"Portfolio"},{"content":"üîé Motivation tip\nBy the end of this post you will learn how to:\nInstall the Weights and Biases client and log the YOLOX training metrics. Compare training metrics on Weights and Biases dashboard. Picking the best model with mAP and FPS values. \u0026ldquo;So many models, so little time!\u0026rdquo;\nAs a machine learning engineer, I often hear this phrase thrown around in many variations.\nIn object detection alone, there are already several hundreds of models out there. Within the YOLO series alone there are YOLOv1 to YOLOv5, YOLOR, YOLOP, YOLOS, PPYOLO, YOLOX, the list can go on forever.\nWith each passing day, better models are added as new discoveries are made. Which one do we pick? How do we know if it\u0026rsquo;s best for the application? If you\u0026rsquo;re new, this can easily get overwhelming.\nIn this post I will show you how I use a free tool known as Weights and Biases (Wandb) to quickly log your experiments and compare them side-by-side.\nIn the interest of time, we will limit our scope to the YOLOX series in this post. We will answer the following questions by the end of the post.\nHow to keep track and log the performance of each model? How do the models compare to one another? How to pick the best model for your application? In this blog post I will show you how I accomplish all of them by using Wandb.\nPS: No Excel sheets involved ü§´\nüïπ Wandb - Google Drive for Machine Learning Life is short they say.\nSo why waste it on monitoring your deep learning models when you can automate them? This is what Wandb is trying to solve. It\u0026rsquo;s like Google Drive for machine learning.\nWandb helps individuals and teams build models faster. With just a few lines of code, you can compare models, log important metrics, and collaborate with teammates. It\u0026rsquo;s free to get started. Click here to create an account.\nThis post is a sequel to my previous post where I showed how to deploy YOLOX models on CPU at 50 FPS. This time around I will show you how I get the most from the YOLOX models by logging the performance metrics and comparing them on Wandb.\nLet\u0026rsquo;s first install the Wandb client for Python:\npip install wandb Next, run\nwandb login from your terminal to authenticate your machine. The API key is stored in ~/.netrc.\nIf the installation and authentication are completed successfully, you can now use Wandb to monitor and log all the YOLOX training metrics.\nüëÄ Monitoring Training Metrics Within the YOLOX series, there are at least 6 different variations of the model sorted from largest to smallest in size:\nYOLOX-X (largest) YOLOX-L YOLOX-M YOLOX-S YOLOX-Tiny YOLOX-Nano (smallest) Let\u0026rsquo;s train all the YOLOX models and log the training metrics to Wandb. For that, you need to install the YOLOX library following the instructions here.\nYou\u0026rsquo;d also need to prepare a custom Exp file to specify the model and training hyperparameters. I will use an existing Exp file from my last blog post.\nAll you need to do is run the train.py script from the YOLOX repository and specify wandb in the --logger argument.\npython tools/train.py -f exps/example/custom/yolox_s.py \\ -d 1 -b 64 --fp16 \\ -o -c /path/to/yolox_s.pth \\ --logger wandb \\ wandb-project \u0026lt;your-project-name\u0026gt; \\ wandb-id \u0026lt;your-run-id\u0026gt; note\n-f specifies the location of the custom Exp file.\n-d specifies the number of GPUs available on your machine.\n-b specifies the batch size.\n-c specifies the path to save your checkpoint.\n--fp16 tells the model to train in mixed precision mode.\n-o specifies the option to occupy GPU memory first for training.\n--logger specifies the type of logger we want to use. Specify wandb here.\nwandb-project specifies the name of the project on Wandb. (Optional)\nwandb-id specifies the id of the run. (Optional)\nIf the optional arguments are not specified, a random project name and id will be generated. You can always change them on the Wandb dashboard later.\nI\u0026rsquo;d recommend to specify self.save_history_ckpt = False in your Exp file. If set to True this saves the model checkpoint at every epoch and uploads them to wandb. This makes the logging process A LOT slower because every checkpoint is uploaded to wandb as an artifact.\nOnce everything is set in place, let\u0026rsquo;s run the training script and head to the project dashboard on Wandb to monitor the logged metrics.\nAfter running the training for all the YOLOX models, the project dashboard should look like the following.\nLogged metrics during training for all YOLOX models. Logged hyperparameters for all YOLOX models. As shown above, all the training metrics and hyperparameters are logged for each YOLOX model in an organized table. You can share this dashboard with your teammates so they can view the metrics in real-time as you train. You can also conveniently export the table and graphs into other forms such as .csv should you require.\nI\u0026rsquo;m not a fan of Excel sheets, so I\u0026rsquo;ll keep them on Wandb üòé. Access my dashboard for this post here.\nTo gauge the quality of the model, we can look at the COCOAP50_95 plot or also known as the COCO Metric or mean average precision (mAP) plot. The mAP plot indicates how well the model performs on the validation set (higher values are better) as we train the model and is shown below. From the mAP plot, looks like YOLOX-X scored the highest mAP followed by YOLOX-L, YOLOX-M, YOLOX-S, YOLOX-Tiny and YOLOX-Nano. It\u0026rsquo;s a little hard to tell from the figure above. I encourage you to check out the dashboard where you can zoom in and resize the plots.\n‚ö°Ô∏è Inference with Quantized Model Comparing the mAP value on Wandb only gives us an idea of how well the model performs on the validation set. It does not indicate how fast the model will run in deployment and how well the model will perform in the real world.\nIn object detection, it is always good to verify the performance by visual inspection of the running model. For that, let\u0026rsquo;s run inference on a video for each model.\nRunning inference on a GPU is boring ü§∑‚Äç‚ôÇÔ∏è, we know YOLOX models can run very fast on GPUs. To make it more interesting, let\u0026rsquo;s run the models on a CPU instead.\nTraditionally, object detection models run slowly on a CPU. To overcome that, let\u0026rsquo;s convert the YOLOX models into a form that can run efficiently on CPUs.\nFor that, we use Intel\u0026rsquo;s Post-training Optimization Toolkit (POT) which runs an INT8 quantization algorithm on the YOLOX models. Quantization optimizes the model to use integer tensors instead of floating-point tensors. This results in a 2-4x faster and smaller models. Plus, we can now run the models on a CPU for real-time inference!\nIf you\u0026rsquo;re new to my posts, I wrote on how to run the quantization here. Let\u0026rsquo;s check out how the models perform running on a Core i9 CPU üëá\nYOLOX-X (mAP: 0.8869, FPS: 7+) YOLOX-X is the largest model that scores the highest mAP. The PyTorch model is 792MB and the quantized model is about 100MB in size. The quantized YOLOX-X model runs only at about 7 FPS on a CPU. YOLOX-L (mAP: 0.8729, FPS: 15+) The PyTorch model is 434MB and the quantized model is about 56MB in size. The quantized YOLOX-L model runs at about 15 FPS on a CPU. YOLOX-M (mAP: 0.8688, FPS: 25+) The PyTorch model is 203MB and the quantized model is about 27MB. The quantized YOLOX-M model runs at about 25 FPS on a CPU.\nYOLOX-S (mAP: 0.8560, FPS: 50+) The PyTorch model is 72MB and the quantized model is about 10MB in size. The quantized YOLOX-S model runs at about 50 FPS on a CPU. YOLOX-Tiny (mAP: 0.8422, FPS: 70+) The PyTorch model is 41MB and the quantized model is about 6MB in size. The quantized YOLOX-Tiny model runs at about 70 FPS on a CPU. YOLOX-Nano (mAP: 0.7905, FPS: 100+) YOLOX-Nano scored the lowest on the mAP compared to others. The PyTorch model is 7.6MB and the quantized model is about 2MB in size. However, it is the fastest running model with over 100 FPS on CPU. tip\nLarger models score higher mAP and lower FPS. Smaller models score lower mAP and higher FPS. To answer the question of which model is best will ultimately depend on your application. If you need a fast model and only have a lightweight CPU on the edge, give YOLOX-Nano a try. If you prioritize accuracy over anything else and have a reasonably good CPU - YOLOX-X seems to fit.\nEverything else lies in between.\nThis is a classic trade-off of accuracy vs latency in machine learning. Understanding your application well goes a long way to help you pick the best model.\n‚õ≥Ô∏è Wrapping up tip\nIn this post we\u0026rsquo;ve covered\nHow to install wandb client and use it with the YOLOX model. How to compare training metrics on the Wandb dashboard. Picking the best model using mAP values and inference speed on a CPU. So, what\u0026rsquo;s next? In this short post, we have not explored all features available on Wandb. As the next steps, I encourage you to check the Wandb documentation page to see what\u0026rsquo;s possible.\nHere are my 3 suggestions:\nLearn how to tune hyperparameters with Wandb Sweeps. Learn how to visualize and inspect your data on the Wandb dashboard. Learn how to version your model and data. üôè Comments \u0026amp; Feedback I hope you\u0026rsquo;ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/Linked post or drop me a message. Picking an object detection model is a PAIN.\nWithin the YOLO family there\u0026#39;s YOLOv1 to YOLOv5, YOLOR, YOLOX, PPYOLO.. it\u0026#39;s never ending üòµ\nHow do you pick the right one for your application?\nIn this thread, I will show you how to squeze the best of YOLOX using @weights_biases pic.twitter.com/RjnmOz1mNh\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) May 11, 2022 ","description":"Monitor your models with Wandb and pick the best!","permalink":"http://localhost:1313/portfolio/comparing_yolox_models_weights_and_biases/","title":"Squeezing the Best Performance Out of YOLOX with Weights and Biases","type":"Portfolio"},{"content":"üö¶ Motivation tip\nBy the end of this post, you will learn how to:\nTrain state-of-the-art YOLOX model with your own data. Convert the YOLOX PyTorch model into ONNX and OpenVINO IR format. Run quantization algorithm to 10x your model\u0026rsquo;s inference speed. P/S: The final model runs faster on the CPU than the GPU! üò±\nDeep learning (DL), seems to be the magic word that makes anything mundane cool again. We find them everywhere - in news reports, blog posts, articles, research papers, advertisements, and even baby books.\nExcept in production ü§∑‚Äç‚ôÇÔ∏è.\nAs much as we were made to believe DL is the answer to our problems, more than 85% of models don\u0026rsquo;t make it into production - according to a recent survey by Gartner.\nThe barrier? Deployment.\nFor some applications such as self-driving car, real-time deployment is critical and has huge implications.\nAs data scientists, even though we can easily train our models on GPUs, it is uncommon and sometimes impractical to deploy them on GPUs in production. On the other hand, CPUs are far more common in production environment, and a lot cheaper.\nBut can we feasibly deploy real-time DL models on a CPU? Running DL models on a CPU is orders of magnitude slower compared to GPU, right?\nWrong.\nIn this post, I will walk you through how we go from this üêåüêåüêå\nto this üöÄüöÄüöÄ Yes, that\u0026rsquo;s right, we can run DL models on a CPU at 50+ FPS üò± and I\u0026rsquo;m going to show you how in this post. If that looks interesting, let\u0026rsquo;s dive in.\n‚õ∑ Modeling with YOLOX We will use a state-of-the-art YOLOX model to detect the license plate of vehicles around the neighborhood. YOLOX is one of the most recent YOLO series models that is both lightweight and accurate.\nIt claims better performance than YOLOv4, YOLOv5, and EfficientDet models. Additionally, YOLOX is an anchor-free, one-stage detector which makes it faster than its counterparts.\nBefore we start training, let\u0026rsquo;s collect images of the license plates and annotate them. I collected about 40 images in total. 30 of the images will be used as the training set and 10 as the validation set.\ntip\nThis is an incredibly small sample size for any DL model, but I found that it works reasonably well for our task at hand. We likely need more images to make this model more robust. However, this is still a good starting point. Sample images of vehicle license plates. To label the images, let\u0026rsquo;s use the open-source CVAT labeling tool by Intel. There are a ton of other labeling tools out there feel free to use them if you are comfortable.\nIf you\u0026rsquo;d like to try CVAT, head to https://cvat.org/ - create an account and log in. No installation is needed.\nA top menu bar should be visible as shown below. Click on Task, fill in the name of the task, add related labels, and upload the images. Since we are only interested in labeling the license plate, I\u0026rsquo;ve entered only one label - LP (license plate). Once the uploading completes, you will see a summary page as below. Click on Job #368378 at ‚ë¢ and it should bring you to the labeling page. To start labeling, click on the square icon at ‚ë† and click Shape at ‚ë° in the figure below. You can then start drawing bounding boxes around the license plate. Do this for all 40 images. Once done, we are ready to export the annotations on the Tasks page. Make sure the format is COCO 1.0 and click on OK. If you\u0026rsquo;d like to download the images check the Save images box. Since I have those images already, I don\u0026rsquo;t have to download them. Now, that we have our dataset ready, let\u0026rsquo;s begin training. If you haven\u0026rsquo;t already, install the YOLOX package by following the instructions here.\nPlace your images and annotations in the datasets folder following the structure outlined here.\nNext, we must prepare a custom Exp class. The Exp class is a .py file where we configure everything about the model - dataset location, data augmentation, model architecture, and other training hyperparameters. More info on the Exp class here.\nFor simplicity let\u0026rsquo;s use one of the smaller models - YOLOX-s.\nThere are other YOLOX models you can try like YOLOX-m, YOLOX-l, YOLOX-x, YOLOX-Nano, YOLOX-Tiny, etc. Feel free to experiment with them. More details are on the README of the YOLOX repo.\nMy custom Exp class for the YOLOX-s model looks like the following\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import os from yolox.exp import Exp as MyExp class Exp(MyExp): def __init__(self): super(Exp, self).__init__() self.depth = 0.33 self.width = 0.50 self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\u0026#34;.\u0026#34;)[0] # Define yourself dataset path self.data_dir = \u0026#34;data\u0026#34; self.train_ann = \u0026#34;train.json\u0026#34; self.val_ann = \u0026#34;val.json\u0026#34; self.num_classes = 1 self.data_num_workers = 4 self.eval_interval = 1 # --------------- transform config ----------------- # self.degrees = 10.0 self.translate = 0.1 self.scale = (0.1, 2) self.mosaic_scale = (0.8, 1.6) self.shear = 2.0 self.perspective = 0.0 self.enable_mixup = True # -------------- training config --------------------- # self.warmup_epochs = 5 self.max_epoch = 300 self.warmup_lr = 0 self.basic_lr_per_img = 0.01 / 64.0 self.scheduler = \u0026#34;yoloxwarmcos\u0026#34; self.no_aug_epochs = 15 self.min_lr_ratio = 0.05 self.ema = True self.weight_decay = 5e-4 self.momentum = 0.9 We are now ready to start training. The training script is provided in the tools folder. To start training, run\npython tools/train.py -f exps/example/custom/yolox_s.py -d 1 -b 64 --fp16 -o -c /path/to/yolox_s.pth note\n-f specifies the location of the custom Exp file.\n-d specifies the number of GPUs available on your machine.\n-b specifies the batch size.\n-c specifies the path to save your checkpoint.\n--fp16 tells the model to train in mixed precision mode.\n-o specifies the option to occupy GPU memory first for training.\nAfter the training completes, you can run inference on the model by utilizing the demo.py script in the same folder. Run\npython tools/demo.py video -f exps/example/custom/yolox_s.py -c /path/to/your/yolox_s.pth --path /path/to/your/video --conf 0.25 --nms 0.45 --tsize 640 --device gpu note\n-f specifies the path to the custom Exp file.\n-c specifies the path to your saved checkpoint.\n--path specifies the path to the video you want to infer on.\n--conf specifies the confidence threshold of the detection.\n--nms specifies the non-maximum suppression threshold.\n--tsize specifies the test image size.\n--device specifies the device to run the model on - cpu or gpu.\nI\u0026rsquo;m running this on a computer with an RTX3090 GPU. The output looks like the following. Out of the box, the model averaged 40+ FPS on an RTX3090 GPU. But, on a Core i9-11900 CPU (a relatively powerful CPU to date) it averaged at 5+ FPS - not good for a real-time detection task.\nLet\u0026rsquo;s improve that by optimizing the model.\nü§ñ ONNX Runtime ONNX is an open format built to represent machine learning models. The goal of ONNX is to ensure interoperability among machine learning models via commonly accepted standards. This allows developers to flexibly move between frameworks such as PyTorch or Tensorflow with less compatibility issues.\nONNX supports a cross-platform model accelerator known as the ONNX Runtime. This improves the inference performance of a wide variety of models capable of running on various operating systems.\nLet\u0026rsquo;s convert our trained YOLOX-s model into the ONNX format. For that, you must install the onnxruntime package via pip.\npip install onnxruntime To convert our model run\npython tools/export_onnx.py --output-name your_yolox.onnx -f exps/your_dir/your_yolox.py -c your_yolox.pth Let\u0026rsquo;s load the ONNX model and run the inference using the ONNX Runtime.\nAs shown, the FPS slightly improved from 5+ FPS to about 10+ FPS with the ONNX model and Runtime on CPU - still not ideal for real-time inference. Just by converting the model to ONNX, we already 2x the inference performance.\nLet\u0026rsquo;s see if we can improve that further.\nüîó OpenVINO Intermediate Representation OpenVINO is a toolkit to optimize DL models. It enables a model to be optimized once and deployed on any supported Intel hardware including CPU, GPU, VPU, and FPGAs.\nTo optimize a model, we will use a tool known as Model Optimizer (MO) by Intel. MO converts all floating-point weights of the original model to FP16 data type.\nThe resulting form is known as the OpenVINO Intermediate Representation (IR) or called the compressed FP16 model. The compressed FP16 model takes half the disk space compared to the original model and may have some negligible accuracy drop.\nLet\u0026rsquo;s convert our model into the IR form. For that, we need to install the openvino-dev package.\npip install openvino-dev[onnx]==2022.1.0 Once installed, we can invoke the mo command to convert the model. mo is the abbreviation of\nmo --input_model models/ONNX/yolox_s_lp.onnx --input_shape [1,3,640,640] --data_type FP16 --output_dir models/IR/ note\nThe mo accepts a few parameters:\n--input_model specifies the path to the previously converted ONNX model.\n--input_shape specifies the shape of the input image.\n--data_type specifies the data type.\n--output_dir specifies the directory to save the IR.\nThis results in a set of IR files that consists of an .xml and .bin file.\n.xml - Describes the model architecture\n.bin - Contains the weights and biases of the model.\nTogether, you can deploy them on any of the supported Intel hardware.\nNow, let\u0026rsquo;s run an inference using the IR files on the same video and observe its performance. As you can see the FPS bumped up to 16+ FPS. It\u0026rsquo;s now beginning to look more feasible for real-time detection. Let\u0026rsquo;s call it a day and celebrate the success of our model! ü•≥\nOr, is there more to it? Enter üëáüëáüëá\nüõ† Post-Training Quantization Apart from the Model Optimizer, OpenVINO also comes with a Post-training Optimization Toolkit (POT) designed to supercharge the inference of DL models without retraining or finetuning.\nTo achieve that, POT runs 8-bit quantization algorithms and optimizes the model to use integer tensors instead of floating-point tensors on some operations. This results in a 2-4x faster and smaller model.\nThis is where the real magic happens.\nFrom the OpenVINO documentation page, the POT supports two types of quantization:\nDefaultQuantization is a default method that provides fast and in most cases accurate results for 8-bit quantization.\nAccuracyAwareQuantization enables remaining at a predefined range of accuracy drop after quantization at the cost of performance improvement. It may require more time for quantization.\nThe quantization algorithm requires a representative subset of the dataset to estimate the model accuracy during the quantization process. Let\u0026rsquo;s prepare a few images and put them in a separate directory.\nWe are going to use the DefaultQuantization in this post. For that, let\u0026rsquo;s run\npot -q default -m models/IR/yolox_s_lp.xml -w models/IR/yolox_s_lp.bin --engine simplified --data-source data/pot_images --output-dir models/INT8 note\nThe pot command accepts a few parameters:\n-m specifies the directory to the .xml file. -w specifies the directory to the .bin file. --engine specifies the type of quantization algorithm. --data-source specifies the directory of the images used to quantize the model. --output-dir specifies the directory to save the quantized model. This results in another set of IR files saved in --output-dir.\nüöÄ Real-time Inference @50+ FPS Now, the moment of truth. Let\u0026rsquo;s load the quantized model and run the same inference again.\nBoom!\nThe first time I saw the numbers, I could hardly believe my eyes. 50+ FPS on a CPU! That\u0026rsquo;s about 10x faster üöÄ compared to our initial model! Plus, this is also faster than the RTX3090 GPU!\nMind = blown ü§Ø\nüèÅ Conclusion tip\nIn this post you\u0026rsquo;ve learned how to:\nTrain a custom YOLOX model with your own dataset. Convert the trained model into ONNX and IR forms for inference. 10x the inference speed of the model with 8-bit quantization. So, what\u0026rsquo;s next? To squeeze even more out of the model I recommend:\nExperiment with smaller YOLOX models like YOLOX-Nano or YOLOX-Tiny. Try using a smaller input resolution such as 416x416. We\u0026rsquo;ve used 640x640 in this post. Try using the AccuracyAwareQuantization which runs quantization on the model with lesser accuracy loss. There\u0026rsquo;s also a best practice guide to quantize your model with OpenVINO here.\nIf you enjoyed this post, you might also like the following post where I show how to accelerate your PyTorch Image Models (TIMM) 8x faster with ONNX Runtime and TensorRT.\nSeptember 30, 2024 Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime \u0026amp; Optimizations üôè Comments \u0026amp; Feedback I hope you\u0026rsquo;ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or drop me a message. Deploying object detection models on a CPU is a PAIN.\nIn this thread I will show you how I optimized and 10x my YOLOX model from 5 FPS to 50 FPS on a CPU. Yes!! CPU!\nAnd yes for FREE.\nThe optimized model runs FASTER on a CPU than GPU ü§Ø https://t.co/nlzjO8wmVV\nA thread üëá\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) May 3, 2022 ","description":"Learn how to 10x your YOLOX model and run it faster than GPU in few simple steps!","permalink":"http://localhost:1313/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/","title":"Faster than GPU: How to 10x your Object Detection Model and Deploy on CPU at 50+ FPS","type":"Portfolio"},{"content":"üöë Deployment: Where ML models go to die In this post, I will outline the basic steps to deploy ML models onto lightweight mobile devices easily, quickly and for free.\ntip\nBy the end of this post, you will learn about:\nLeveraging Hugging Face infrastructure to host models. Deploying on any edge device using REST API. Displaying the results on a Flutter Android app. According to Gartner, more than 85% of machine learning (ML) models never made it into production. This trend is expected to continue further this year in 2022.\nIn other words, despite all the promises and hype around ML, most models fail to deliver in a production environment. According to Barr Moses, CEO, Monte Carlo, deployment is one of the critical points where many models fail.\nSo what exactly is the deployment of ML model? Simply put, deployment is making a model\u0026rsquo;s capability or insight available to other users or systems - Luigi Patruno.\nüèπ Begin with deployment in mind Many ML researchers take pride in training bleeding-edge models with state-of-the-art (SOTA) results on datasets. As a researcher, I understand how deeply satisfying it feels to accomplish that.\nUnfortunately, many of these so-called \u0026ldquo;SOTA models\u0026rdquo; will end up on preprints, Jupyter notebooks, or in some obscure repository, nobody cares about after the initial hype.\nEventually, they are forgotten and lost in the ocean of newer \u0026ldquo;SOTA models\u0026rdquo;. To make things worse, the obsession with chasing after \u0026ldquo;SOTA models\u0026rdquo; often causes researchers to lose track of the end goal of building the model - deployment.\nSource: ClearML on Reddit. Hence, as ML engineers, it is very helpful if we build models with deployment in mind, as the end result.\nBecause only when a model is deployed can it add value to businesses or organizations. This is the beginning of getting a model into production.\nDeployment is unfortunately a messy and complicated topic in MLOps - too deep for us to cover here. Luckily, that is not the purpose of this post.\ntip\nMy objective in this post is to show you how you can deploy an ML model easily on a mobile device without getting your hands dirty with servers, backends or Kubernetes. The following figure shows the deployment architecture that allows us to accomplish that. Deployment architecture. ü§ó Hosting a model on Hugging Face The first piece of the puzzle is to host our model on some cloud infrastructure. In this post, let\u0026rsquo;s use a free service known as Hugging Face Spaces.\nSpaces is a platform where anyone can upload their model and share it with the world. If you head to https://huggingface.co/spaces, you will find thousands of models that researchers made freely available online. These models are hosted on Spaces for demo and sharing purposes. But they can be scaled up into full-fledge production with the Inference API.\nLet\u0026rsquo;s set up a Space to host our model. If you\u0026rsquo;re unsure how to do that, I wrote a recent guide on how to set your own Space with the Gradio app here.\nIn this post, I will use an IceVision object detection model trained to detect microalgae cells from an image. I trained this model in under a minute with only 17 labeled images. Here is how I did it.\nOnce the Space is set, we will have a Gradio interface like the following This Space is now ready to be shared with anyone with an internet connection and a browser. Try the live demo below üëáüëáüëá\nBut what if we want to make the app work on a mobile device without using a browser? Enter üëá\nüìû Calling the HTTP Endpoint One neat feature of the Gradio app is it exposes the model through a RESTful API. This makes the model prediction accessible via HTTP request which we can conveniently use on any mobile device!\nNow, any computationally lightweight device can make use of the model\u0026rsquo;s prediction just by running a simple HTTP call. All the heavy lifting is taken care of by the Hugging Face infrastructure.\ntip\nThis can be a game-changer if the model is complex and the edge device is not powerful enough to run the model - which is a common scenario. Additionally, this also reduces deployment hardware costs, because now any lightweight, portable mobile device with an internet connection can leverage the model\u0026rsquo;s cell counting capability.\nThe figure below shows the endpoint for us to call the model.\nAs shown, the input to the model is an image, and the output, an image (with bounding boxes) and also a value of the microalgae count. You can check out the API here.\nIf you\u0026rsquo;d like to test the HTTP endpoint live, head to the API page as the following figure. Alternatively, you can also try them out on your computer with curl:\ncurl -X POST https://hf.space/embed/dnth/webdemo-microalgae-counting/+/api/predict/ -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;data\u0026#34;: [\u0026#34;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHB...\u0026#34;]}\u0026#39; Let\u0026rsquo;s copy the URL endpoint and use in the next section\nüì≤ Displaying results in Flutter We will be using Flutter to make a simple Android app that sends an image and receive the bounding box prediction via HTTP calls.\nFlutter uses the Dart programming language that makes it incredibly easy to construct a graphical user interface (GUI). I omit the codes to construct the GUI in this post for simplicity. Let me know if you\u0026rsquo;d like to access it. There are also tons of tutorials on how to construct the GUI so, I will not cover them here too.\nThe snippet of code that allows us to perform the HTTP call to the Hugging Face server is as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import \u0026#39;dart:convert\u0026#39;; import \u0026#39;package:http/http.dart\u0026#39; as http; Future\u0026lt;Map\u0026gt; detectImage(String imageBase64) async { final response = await http.post( Uri.parse( \u0026#39;https://hf.space/gradioiframe/dnth/webdemo-microalgae-counting/+/api/predict/\u0026#39;), headers: \u0026lt;String, String\u0026gt;{ \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json; charset=UTF-8\u0026#39;, }, body: jsonEncode(\u0026lt;String, List\u0026lt;dynamic\u0026gt;\u0026gt;{ \u0026#39;data\u0026#39;: [imageBase64] }), ); if (response.statusCode == 200) { final detectionResult = jsonDecode(response.body)[\u0026#34;data\u0026#34;]; final imageData = detectionResult[0].replaceAll(\u0026#39;data:image/png;base64,\u0026#39;, \u0026#39;\u0026#39;); return {\u0026#34;count\u0026#34;: detectionResult[1], \u0026#34;image\u0026#34;: imageData}; // If the server did return a 200 CREATED response, // then decode the image and return it. } else { // If the server did not return a 200 OKAY response, // then throw an exception. throw Exception(\u0026#39;Failed to get results.\u0026#39;); } } The detectImage function in line 4 takes in a single parameter String base64 format image and returns a Map which consists of the image with bounding box and the microalgae count in line 22.\nThe URL endpoint that we copied from the previous section is on line 7.\nThe screen recording below illustrates the Flutter app sending a sample image to the Hugging Face inference server and getting a response on the number of detected microalgae cells and the image with all the bounding boxes.\nI published the app on Google Playstore. If you like, try them out here.\nI\u0026rsquo;ve also published another similar app that deploys a deep learning classifier model (trained with Fastai) that categorizes paddy leaf diseases here using the same approach outlined in this post.\nüí° Up Next That\u0026rsquo;s about it! In this post hopefully, it\u0026rsquo;s clear now that deploying deep learning models on mobile devices doesn\u0026rsquo;t need to be complicated - at least in the beginning when it\u0026rsquo;s critical to gain users\u0026rsquo; feedback before deciding if it\u0026rsquo;s right to scale up.\nCaveat: I do acknowledge that the approach in this post might not be optimal in some circumstances, especially if you have thousands of users on your app.\nFor that, I would recommend scaling up to use the Hugging Face Inference API - a fully hosted production-ready solution üëá. It is also possible now to deploy Hugging Face models on AWS Sagemaker for serverless inference. Check them out here.\nFinally, you could also use the same Flutter codebase and export it into an iOS, Windows, or even a Web app. This is the beauty of using Flutter for front-end development. Code once, and export to multiple platforms.\nüôè Comments \u0026amp; Feedback If you have any questions, comments, or feedback, please leave them on the following Twitter post or drop me a message. Deploying deep learning models on mobile devices can be a pain. I\u0026#39;ll walk you through simple steps I took to deploy an object detection model trained with IceVision on Android using @huggingface Spaces and @FlutterDev - for free.\nA thread üëáüëáhttps://t.co/2XRWHlTZ7p\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) April 21, 2022 ","description":"Leverage giant models in the cloud on Android with Hugging Face and Flutter.","permalink":"http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/","title":"How to Deploy Object Detection Models on Android with Flutter","type":"Portfolio"},{"content":"üï∂Ô∏è Motivation Many biology and medical procedures involve counting cells from images taken with a microscope. Counting cells reveals the concentration of bacteria and viruses and gives vital information on the progress of a disease.\nTo accomplish the counting, researchers painstakingly count the cells by hand with the assistance of a device called hemocytometer. This process is repetitive, tedious, and prone to errors.\nWhat if we could automate the counting by using an intelligent deep learning algorithm instead?\nIn this blog post, I will walk you through how to use the IceVision library and train a state-of-the-art deep learning model with Fastai to count microalgae cells.\ntip\nBy the end of this post, you will learn how to:\nHow to install the IceVision and and labelImg package. Prepare and label any dataset for object detection. Train a high-performance VFNet model with IceVision \u0026amp; Fastai. Use the model for inference on new images. P/S: The end result - A high performance object detector in only 17 lines of code! üöÄ\nDid I mention that all the tools used in this project are completely open-source and free of charge? Yes! If you\u0026rsquo;re ready let\u0026rsquo;s begin.\n‚öôÔ∏è Installation Throughout this post, we will make use a library known as IceVision - a computer vision-focused library built to work with Fastai. Let\u0026rsquo;s install them first.\nThere are many ways to accomplish the installation. For your convenience, I\u0026rsquo;ve prepared an installation script that simplifies the process into just a few lines of code.\nTo get started, let\u0026rsquo;s clone the Git repository by typing the following in your terminal:\ngit clone https://github.com/dnth/microalgae-cell-counter-blogpost Next, navigate into the directory:\ncd microalgae-cell-counter-blogpost/ Install IceVision and all other libraries used for this post:\nbash icevision_install.sh cuda11 0.12.0 Depending on your system CUDA version, you may want to change cuda11 to cuda10, especially on older systems. The number following the CUDA version is the version of IceVision. The version I\u0026rsquo;m using for this blog post is 0.12.0. You can alternatively replace the version number with master to install the bleeding edge version of IceVision from the master branch on Github.\nIf you would like to install the CPU version of the library it can be done with:\nbash icevision_install.sh cpu 0.12.0 info\nTraining an object detection model on a CPU can be many times slower compared to a GPU. If you do not have an available GPU, use Google Colab. The installation may take a few minutes depending on your internet connection speed. Let the installation complete before proceeding.\nüîñ Labeling the data All deep learning models require data to work. To construct a model for microalgae cell counting, we require images of microalgae cells to work with. For the purpose of this post, I\u0026rsquo;ve acquired image samples from a lab.\nThe following shows a sample image of the microalgae cells as seen through a microscope. The cells are colored green. Can you count how many cells are present in this image? There are a bunch of other images in the data/not_labeled/ folder. There is only one issue now, and that is the images are not labeled. Let\u0026rsquo;s label the images with bounding boxes using an open-source image labeling tool labelImg.\nThe labelImg app enables us to label images with class names and bounding boxes surrounding the object of interest. The following figure shows a demo of the app. The labelImg app is already installed in the installation step. To launch the app, type in your terminal:\nlabelImg A window like the following should appear. Let\u0026rsquo;s load the data/not_labeled/ images folder into labelImg and start labeling them! To do that, click on the Open Dir icon and navigate to the folder.\nAn image should now show up in labelImg. To label, click on the Create RectBox icon to start drawing bounding boxes around the microalgae cells. Next, you will be prompted to enter a label name. Key in microalgae as the label name. Once done, a rectangular bounding box should appear on-screen.\nNow comes the repetitive part, we will need to draw a bounding box for each microalgae cell for all images in the folder. To accelerate the process I highly recommend the use of hotkeys keys with labelImg. The hotkeys are shown below. Once done, remember to save the annotations. The annotations are saved in an XML file with a file name matching to the image file name as shown below. It took me a few hours to meticulously label the images. If you don\u0026rsquo;t feel like spending time labeling all the images (although I recommend doing them at least once), you can find the labeled ones in the data/labeled/ folder.\nüåÄ Modeling Once the labeling is done, we are now ready to start modeling in a jupyter notebook environment.\nTo launch the jupyter notebook run the following in your terminal\njupyter lab A browser window should pop up. On the left pane, double click the train.ipynb to open the notebook. All the codes in this section are inside the notebook. Here, I will attempt to walk you through just enough details of the code to get you started with modeling on your own data. If you require further clarifications, the IceVision documentation is a good starting point. Or drop me a message.\nThe first cell in the notebook is the imports. With IceVision all the necessary components are imported with one line of code:\nfrom icevision.all import * If something wasn\u0026rsquo;t properly installed, the imports will raise an error message. In that event, you must go back to the installation step before proceeding. If there are no errors, we are ready to dive in further.\nüéØ Preparing datasets After the imports, we must now load the labeled images and bounding boxes into jupyter. This is also known as data parsing and is accomplished with the following:\nparser = parsers.VOCBBoxParser(annotations_dir=\u0026#34;data/labeled\u0026#34;, images_dir=\u0026#34;data/labeled\u0026#34;) The parameter annotations_dir and images_dir are the directories to the images and annotations respectively. Since both the images and annotations are located in the same directory, they are the same as such in the code.\nNext, we will randomly pick and divide the images and bounding boxes into two groups of data namely train_records and valid_records. By default, the split will be 80:20 to train:valid proportion. You can change the ratio by altering the values in RandomSplitter.\ntrain_records, valid_records = parser.parse(data_splitter=RandomSplitter([0.8, 0.2]) The following code shows the class names from the parsed data:\nparser.class_map It should output:\n\u0026lt;ClassMap: {\u0026#39;background\u0026#39;: 0, \u0026#39;Microalgae\u0026#39;: 1}\u0026gt; which shows a ClassMap that contains the class name as the key and class index as the value in a Python dictionary. The background class is automatically added. In the data labeling step, we do not need to label the background.\nNext, we will apply basic data augmentation which is a technique used to diversify the training images by applying the random transformation. Learn more here.\nThe following code specifies the kinds of transformations we would like to perform on our images. Behind the scenes, these transformations are performed with the Albumentations library.\nimage_size = 640 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=image_size+128), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()]) We must specify the dimensions of the image in image_size = 640. This value will then be used in tfms.A.aug_tfms that ensures that all images are resized to a 640x640 resolution and normalized in tfms.A.Normalize().\nSome models like EfficientDet only work with image size divisible by 128. Other common values you can try are 384, 512, 768, etc. But beware using a large image size may consume more memory and in some cases halts training. Starting with a small value like 384 is probably a good idea. I found 640 works best for this dataset.\nUse tfms.A.aug_tfms performs transformations to the image such as varying the lighting, rotation, shifting, flipping, blurring, padding, etc. The full list of transforms and the parameters can be found in the aug_tfms documentation.\nIn this code snippet, we created two distinct transforms namely train_tfms and valid_tfms that will be used during the training and validation steps respectively.\nNext, we will apply the train_tfms to our train_records and valid_tfms to valid_records with the following snippet.\ntrain_ds = Dataset(train_records, train_tfms) valid_ds = Dataset(valid_records, valid_tfms) This results in the creation of a Dataset object which is a collection of transformed images and bounding boxes.\nTo visualize the train_ds we can run:\nsamples = [train_ds[0] for _ in range(4)] show_samples(samples, ncols=4) This will show us 4 samples from the train_ds. Note the variations in lighting, translation, and rotation compared to the original images. The transformations are applied on the fly. So each run on the snippet produces slightly different results.\nüóùÔ∏è Choosing a library, model, and backbone IceVision supports hundreds of high-quality pre-trained models from Torchvision, Open MMLab\u0026rsquo;s MMDetection, Ultralytic\u0026rsquo;s YOLOv5 and Ross Wightman\u0026rsquo;s EfficientDet.\nDepending on your preference, you may choose the model and backbone from these libraries. In this post I will choose the VarifocalNet (VFNet) model from MMDetection which can be accomplished with:\nmodel_type = models.mmdet.vfnet backbone = model_type.backbones.resnet50_fpn_mstrain_2x model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) There are various ResNet backbones that you can select from such as resnet50_fpn_1x, resnet50_fpn_mstrain_2x, resnet50_fpn_mdconv_c3_c5_mstrain_2x, resnet101_fpn_1x, resnet101_fpn_mstrain_2x, resnet101_fpn_mdconv_c3_c5_mstrain_2x, resnext101_32x4d_fpn_mdconv_c3_c5_mstrain_2x, and resnext101_64x4d_fpn_mdconv_c3_c5_mstrain_2x.\nAdditionally, IceVision also recently supports state-of-the-art Swin Transformer backbone for the VFNet model swin_t_p4_w7_fpn_1x_coco, swin_s_p4_w7_fpn_1x_coco, and swin_b_p4_w7_fpn_1x_coco.\nWhich combination of model_type and backbone that performs best is something you need to experiment with. Feel free to experiment and swap out the backbone and note the performance of the model. There are other model types with their respective backbones which you can find here.\nüèÉ Metrics and Training To start the training, the model needs to take in the images and bounding boxes from the train_ds and valid_ds we created.\nFor that, we will need to use a dataloader which will help us iterate over the elements in the dataset we created and load them into the model. We will construct two separate dataloaders for train_ds and valid_ds respectively.\ntrain_dl = model_type.train_dl(train_ds, batch_size=2, num_workers=4, shuffle=True) valid_dl = model_type.valid_dl(valid_ds, batch_size=2, num_workers=4, shuffle=False) Here, we can specify the batch_size parameter which is the number of images and bounding boxes given to the model in a single forward pass. The shuffle parameter specifies if you would like to randomly shuffle the order of the data. The num_workers parameter specifies how many sub-processes to use to load the data. Let\u0026rsquo;s keep it at 4 for now.\nNext, we need to specify a measure of how well our model performs during training. This measure is specified using a metric - which involves using specific math equations to output a score that tells us if the model is improving or not during training. Some commonly used metrics include accuracy, error rate, F1 Score, etc. For object detection tasks the COCOMetric is commonly used. If you are interested this blog explains the math behind the metrics used for object detection.\nOnce the metric is defined, we can then load all three components - dataloaders, model, and metric into a Fastai Learner for training.\nmetrics = [COCOMetric(metric_type=COCOMetricType.bbox)] learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics) With deep learning models, there are many hyperparameters that we can configure before we run the training. One of the most important hyperparameters to get right is the learning rate. Since IceVision is built to work with Fastai, we have access to a handy tool known as the learning rate finder first proposed by Leslie Smith and popularized by the Fastai community for its effectiveness. This is an incredibly simple yet powerful tool to find a range of optimal learning rate values that gives us the best training performance.\nAll we need to do is run:\nlearn.lr_find() which outputs: The most optimal learning rate value lies in the region where the loss descends most rapidly. From the figure above, this is somewhere in between 1e-4 to 1e-2. The orange dot on the plot shows the point where the slope is the steepest and is generally a good value to use as the learning rate.\nNow, let\u0026rsquo;s load this learning rate value of 1e-3 into the fine_tune function and start training.\nlearn.fine_tune(10, 1e-3, freeze_epochs=1) The first parameter in fine_tune is the number of epochs to train for. One epoch is defined as a complete iteration over the entire dataset. In this post, I will only train for 10 epochs. Training for longer will likely improve the model, so I will leave that to you to experiment with. The second parameter is the learning rate value we wish to use to train the model. Let\u0026rsquo;s put the value 1e-3 from the learning rate finder.\nThe above code snippet trains the model for 10 epochs. By default, this will start the training in two phases.\nIn the first phase ‚ûÄ, only the last layer of the model is trained. The rest of the model is frozen. In the second phase ‚ûÅ, the entire model is trained end-to-end. The figure below shows the training output.\nThe freeze_epochs parameter specifies the number of epochs to train in ‚ûÄ.\nDuring the training, the train_loss, valid_loss, and COCOMetric are printed at every epoch. Ideally, the losses should decrease, and COCOMetric should increase the longer we train. As shown above, each epoch only took 2 seconds to complete on a GPU - which is incredibly fast.\nOnce the training completes, we can view the performance of the model by showing the inference results on valid_ds. The following figure shows the output at a detection threshold of 0.5. You can increase the detection_threshold value to only show the bounding boxes with a higher confidence value.\nmodel_type.show_results(model, valid_ds, detection_threshold=.5) For completeness, here are the codes for the Modeling section which include steps to load the data, instantiate the model, train the model, and show the results. That\u0026rsquo;s only 17 lines of code!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from icevision.all import * parser = parsers.VOCBBoxParser(annotations_dir=\u0026#34;data/labeled\u0026#34;, images_dir=\u0026#34;data/labeled\u0026#34;) train_records, valid_records = parser.parse() image_size = 640 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=image_size+128), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()]) train_ds = Dataset(train_records, train_tfms) valid_ds = Dataset(valid_records, valid_tfms) model_type = models.mmdet.vfnet backbone = model_type.backbones.resnet50_fpn_mstrain_2x model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) train_dl = model_type.train_dl(train_ds, batch_size=2, num_workers=4, shuffle=True) valid_dl = model_type.valid_dl(valid_ds, batch_size=2, num_workers=4, shuffle=False) metrics = [COCOMetric(metric_type=COCOMetricType.bbox)] learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics) learn.fine_tune(10, 1e-3, freeze_epochs=1) model_type.show_results(model, valid_ds, detection_threshold=.5) üì® Exporting model Once you are satisfied with the performance and quality of the model, we can export all the model configurations (hyperparameters) and weights (parameters) for future use.\nThe following code packages the model into a checkpoint and exports it into a local directory.\nfrom icevision.models.checkpoint import * save_icevision_checkpoint(model, model_name=\u0026#39;mmdet.vfnet\u0026#39;, backbone_name=\u0026#39;resnet50_fpn_mstrain_2x\u0026#39;, img_size=640, classes=parser.class_map.get_classes(), filename=\u0026#39;./models/model_checkpoint.pth\u0026#39;, meta={\u0026#39;icevision_version\u0026#39;: \u0026#39;0.12.0\u0026#39;}) The parameters model_name, backbone_name, and img_size have to match what we used during training.\nfilename specifies the directory and name of the checkpoint file.\nmeta is an optional parameter you can use to save all other information about the model.\nOnce completed the checkpoint should be saved in the models/ folder. We can now use this checkpoint independently outside of the training notebook.\nüß≠ Inferencing on a new image To demonstrate that the model checkpoint file can be loaded independently, I created another notebook with the name inference.ipynb. In this notebook, we are going to load the checkpoint and use it for inference on a brand new image.\nLet\u0026rsquo;s import all the necessary packages:\nfrom icevision.all import * from icevision.models.checkpoint import * from PIL import Image And specify the checkpoint path.\ncheckpoint_path = \u0026#34;./models/model_checkpoint.pth\u0026#34; We can load the checkpoint with the function model_from_checkpoint. From the checkpoint, we can retrieve all other configurations such as the model type, class map, image size, and the transforms.\ncheckpoint_and_model = model_from_checkpoint(checkpoint_path) model = checkpoint_and_model[\u0026#34;model\u0026#34;] model_type = checkpoint_and_model[\u0026#34;model_type\u0026#34;] class_map = checkpoint_and_model[\u0026#34;class_map\u0026#34;] img_size = checkpoint_and_model[\u0026#34;img_size\u0026#34;] valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(img_size), tfms.A.Normalize()]) The model is now ready for inference. Let\u0026rsquo;s try to load an image with:\nimg = Image.open(\u0026#39;data/not_labeled/IMG_20191203_164256.jpg\u0026#39;) We can pass the image into the end2end_detect function to run the inference.\npred_dict = model_type.end2end_detect(img, valid_tfms, model, class_map=class_map, detection_threshold=0.5, display_label=True, display_bbox=True, return_img=True, font_size=50, label_color=\u0026#34;#FF59D6\u0026#34;) The output pred_dict is a Python dictionary. To view the output image with the bounding boxes:\npred_dict[\u0026#34;img\u0026#34;] which outputs\nTo count the number of microalgae cells on the image, we can count the number of bounding boxes on the image by with:\nlen(pred_dict[\u0026#39;detection\u0026#39;][\u0026#39;bboxes\u0026#39;]) which outputs 29 on my computer.\nTo save the image with the bounding boxes, you can run:\npred_dict[\u0026#34;img\u0026#34;].save(\u0026#34;inference.png\u0026#34;) As you can see, there are some missed detections of the microalgae cells. But, considering this is our first try, and we only trained for 10 epochs (which took less than 30 seconds to complete), this is an astonishing feat! Additionally, I\u0026rsquo;ve only used 17 labeled images to train the model.\nIn this post, I\u0026rsquo;ve demonstrated that we can train a sophisticated object detection model with only a few images in a very short time. This outstanding feat is possible thanks to the Fastai library which incorporated all the best practices in training deep learning models.\nAt this point, we have not even tuned any hyperparameters (other than learning rate) to optimize performance. Most hyperparameters are default values in Fastai that worked extremely well out-of-the-box with this dataset and model.\nTo improve performance, you may want to experiment by labeling more data and adjusting a few other hyperparameters such as image size, batch size, training epochs, the ratio of training/validation split, different model types, and backbones.\nüìñ Wrapping Up Congratulations on making it through this post! It wasn\u0026rsquo;t that hard right? Hopefully, this post also boosted your confidence that object detection is not as hard as it used to be. With many high-level open-source packages like IceVision and Fastai, anyone with a computer and a little patience can break into object detection.\nIn this post, I\u0026rsquo;ve shown you how you can construct a model that detects microalgae cells.\ntip\nYou\u0026rsquo;ve learned:\nHow to install the IceVision and and labelImg package. Prepare a dataset of images and bounding boxes with 17 images. Train a high-performance VFNet model with IceVision in only 17 lines of code. Use the model for inference to detect microalgae cells. In reality, the same steps can be used to detect any other cells or any other objects for that matter. Realizing this is an extremely powerful paradigm shift for me.\nThink about all the problems we can solve by accurately detecting specific objects. Detecting intruders, detecting dangerous objects such as a gun, detecting defects on a production line, detecting smoke/fire, detecting skin cancer, detecting plant disease, and so much more.\nYour creativity and imagination are the limits. The world is your oyster. Now go out there and use this newly found superpower to make a difference.\nnote\nAll the codes and data are available on this Github repository. üôè Comments \u0026amp; Feedback If you find this useful, or if you have any questions, comments, or feedback, I would be grateful if you can leave them on the following Twitter post or drop me a message. Sharing a blog post on how I trained a deep learning model to count microalgae cells in 17 lines of code with 17 labeled images using IceVision + @fastdotai #machinelearning #Microbiology #DataScientist #pythonprogramming #innovation #CellBiologyhttps://t.co/AcEtmLS0C9\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) April 11, 2022 So what\u0026rsquo;s next? If you are interested to learn how I deploy this model on Android checkout this post.\n","description":"Leverage hundreds state-of-the-art models on IceVision trained with best practices of Fastai","permalink":"http://localhost:1313/portfolio/training_dl_model_for_cell_counting/","title":"Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images","type":"Portfolio"},{"content":"‚òÄÔ∏è Introduction I was recently given the recognition by the folks at IceVision to be part of the core-developer team! Farid Hassainia, a co-creator of the IceVision Tweeted about it on March 17, 2022. We are super excited to introduce you to our üéâNew Core-Dev: Dickson Neoh @dicksonneoh7\nDickson is actively contributing to IceVision. He ardently helps IceVision users in our Discord Community.\nWe are very happy to have him on our team! pic.twitter.com/SO1qLHkarm\n\u0026mdash; Farid (@ai_fast_track) March 17, 2022 üèÖ Lessons learned Reflecting on how far I\u0026rsquo;ve come, I am astonished and humbled. Just 3 months ago, I learned how to open my first Pull Request on GitHub.\nDespite having coded for over 10 years, I never bothered to learn about something so commonly used in the software world - Git, a version control system. I took the plunge, and the first step was painful. But every subsequent step got better and better.\nI never looked back since.\nEven though it\u0026rsquo;s not late, I wish I had started earlier. If you are new and want to contribute to open-source, start now. Here are some of the lessons I learned along the way that changed the way I learn, and code.\nüßë‚Äçüîß I became a better coder I\u0026rsquo;ve spent years coding and dabbling in many projects since I was a student. With so much time spent on coding, one would be expected to have a good grasp of it. But in reality, I didn\u0026rsquo;t know how good or bad my codes were. Or whether the codes are of best practices or optimal in any way given that a significant portion of the codes was taken off snippets from various sources like Stackoverflow, blog posts, and even GitHub.\nMost of the time, when the code works, I leave it as it is, and when it doesn\u0026rsquo;t, I try to troubleshoot to the best of my ability. I can recall on numerous occasions when my codes didn\u0026rsquo;t work out, and I didn\u0026rsquo;t know why. And sometimes the codes worked, and I still have no idea why ü§¶‚Äç‚ôÇÔ∏è.\nBecause the codes were not publicly shared, there was very limited feedback or insights that anyone can offer to improve them even when I asked for help. As a result, bugs in the codes remained as bugs, and they never got fixed. This severely limited what I could have learned and gained had I gotten better feedback by sharing my codes publicly. This is the major difference between learning in private and in public.\nüî• Contributing to Open Source is Super Power üí™\nHere is what our new Core-Contributor @dicksonneoh7 has to say about that:\n\u0026quot;Contributing to IceVision has been a single turning point that supercharged my learning. I have learned more in this 3 months than ever before! \u0026quot; pic.twitter.com/lynwKDr6Ih\n\u0026mdash; Farid (@ai_fast_track) March 18, 2022 Learning in public is many times more powerful compared to learning in private. Nowadays, it\u0026rsquo;s so easy to start learning in public by contributing to open-source software. All you have to do is pick the one you are most interested in and start engaging the community around the project.\nFor me, the project that got me interested was IceVision. To those unfamiliar, IceVision is a computer vision library built on top of Fastai to make applying deep learning to images very easy for new users. Without much coding experience, you can get started and train your own object detection model in a few lines of code within minutes. Checkout their tutorial here.\nAt first, all I did was try to use the library for my own side projects. Gradually, I realized there are limitations to the existing features of the library. I started asking for help from the community on how can I overcome the limitations by proposing a new feature. This was how I discovered that there are ways I can contribute to the library by adding the features I needed.\nWith IceVision, I have always wanted to use Transformer model for object detection tasks. However, that was not included in the library at that point. I then started to research and ask for help on how to include that model in the library. That was the beginning of how I started contributing to the library. I ended up adding more than 10 models to the library in 14 pull requests. A dozen commits and thousands of lines of codes later, I became an active contributor to the library. The rest is history.\nLittle that I know, I was ranked #26 on Top GitHub Users By Country out of 1000+ contributors from Malaysia. The ranking is based on the number of public contributions on GitHub. All these happened within a short span of 3 months! Ranked #26 after contributing to open-source for 3 months. üë®‚Äçüè´ I got mentorship from coding experts Getting a mentor is no doubt one of the best investments for rapid progress in any area of personal growth. Ask any successful figures for advice, and getting mentors will surely top the list. In my journey of contributing to open-source, one of the most rewarding experiences is the opportunity to learn from the maintainers of the library as they help and guide me to contribute to the library. More often than not, these maintainers (especially for reputable libraries) are highly experienced coders.\nOn one occasion, I got the opportunity to get on a video call with Farid to discuss how to fix an issue with the newly added features of IceVision. Even though our interactions were purely online, I got a glimpse into his thought process as he explained how things are done in IceVision. Needless to say, I learned a lot, not only on the technical aspect but also on the elusive non-technical side such as the mindset of a developer. Farid was also kind enough to offer me practical advice on how to stand out as a developer and encouraged me to blog about what I learned. This blog exists thanks to his advice and encouragement.\nAs I familiarize myself more with the library, I also have had many opportunities to help others who are struggling with the same problems I had. In the process of helping others solve their problems, I often find myself learning new things and exploring areas I never bother to look into. As I help others, I gained and learned a lot myself which allowed me to contribute more to the library. This became an empowering cycle to continue contributing to the library. One day, I hope to be a mentor of some sort to others and help inspire people in the ways I was inspired.\n‚ÄúIf I have seen further it is by standing on the shoulders of giants.‚Äù ‚Äî Isaac Newton üåé I started a website from scratch Encouragement from Farid led me to start blogging. In the process, I explored and learned how to use several blogging platforms including Wordpress, Ghost, and Hugo.\nI finally settled with Hugo as it gives the best flexibility for my use. In the process, I also learned about self-hosting a webpage on GitHub pages and Digital Ocean, purchasing a domain, and GitHub workflows.\nIn building this website, I found myself contributing to the Hugo theme that I used for this site. I learned how Hugo themes work and a little about Go language used in the theme. By this time, I became very comfortable with Pull Requests even though it was only weeks after I stumbled upon Hugo.\nThe learning curve was steep, but it\u0026rsquo;s well worth it.\nAdditionally, in the process of blogging, I also realized that writing about something really helped me solidify and clarify my understanding of the things I write about. More often than not, I find myself in the illusion of understanding something when I really don\u0026rsquo;t. Strangely, this is only evident when I start writing about it.\nI guess this is because writing is an act of synthesizing from a knowledge pool which can only be accomplished when one has a good understanding of the concept. This is why one of the greatest physicists of all time Richard Feynman once said \u0026ldquo;If you want to master something, teach it\u0026rdquo;.\nLearning by writing is also a form of active learning as opposed to passively absorbing content. This has significantly improved my understanding of many concepts I thought I already knew.\nApart from that, I also shared my blog posts on Twitter and LinkedIn which resulted in the online presence and visibility of my blog. Once again the IceVision community is very kind to share my posts and provided me some valuable feedback.\nStarting a website and learning its intricacies are something that I would never venture into had I not contributed to open-source. I never expected this, and I find this a huge bonus learning just because I started contributing to IceVision.\nüôå I learned about the kindness of strangers Through my experience in contributing to open-source, I learned that there are lots of kind people out there who are more than willing to help. You will just have to find them.\nIn the IceVision community, there are many such people, and I am indebted to them for helping me in becoming a better coder and a better person. I got to know people from around the globe from various backgrounds and cultures. This also helped me build my self-esteem as I interact with them. I also learned to appreciate that we all are gifted in different ways and everyone has unique combinations of talents and skills - just like members of the Avengers.\nIn a short period of time, I made a few friends and expanded my social circle. Through them, I also managed to score a few job application interviews, as I was looking for new employment opportunities.\nP/S: I am still looking for an employment opportunity, I would be forever grateful if you can connect me to anyone who\u0026rsquo;s hiring for computer vision or machine learning positions.\nüéÅ Wrapping up It\u0026rsquo;s a wrap! Contributing to open-source has undoubtedly changed my perspective quite a bit in merely 3 months.\nI have learned valuable lessons in becoming a better coder, a writer, and a better person. I wish to also thank the folks at IceVision for making such an awesome deep learning computer vision library available.\nSpecial shoutout to Farid and Francesco for helping me out along the way. If you\u0026rsquo;re interested, do join in the Discord channel or get in touch with me on Twitter or LinkedIn.\nPhoto from our most recent meetup - IceVision core developers from around the globe. ‚õè Comments \u0026amp; Feedback If you have any questions, comments, or feedback, I would be grateful if you can leave them on the following Twitter post or drop me a message. I wrote a blog post on how I went from \u0026quot;Not knowing how to open a pull request\u0026quot; to \u0026quot;becoming core developer of IceVision\u0026quot; in less than 3 months. This documents my learnings along the way. Special shoutout to @ai_fast_track and @Fra_Pochetti in the post.https://t.co/uoTVd8toDc\n\u0026mdash; Dickson Neoh üöÄ (@dicksonneoh7) April 5, 2022 ","description":"First Pull Request to a core developer of IceVision in 3 months.","permalink":"http://localhost:1313/blog/contributing_to_open_source_lessons_learned/","title":"Contributing to open-source: Lessons learned","type":"Blog"},{"content":"Introduction So, you‚Äôve trained a deep learning model that can detect objects from images. Next, how can you share the awesomeness of your model with the rest of the world? You might be a PhD student trying to get some ideas from your peers or supervisors, or a startup founder who wishes to share a minimum viable product to your clients for feedback. But, at the same time you don\u0026rsquo;t wish to go through the hassle of dealing with MLOps. This blog post is for you. In this post I will walk you through how to deploy your model and share them to the world for free!\nTraining a Model with IceVision We will be using the awesome IceVision object detection package as an example for this post. IceVision is an agnostic computer vision library pluggable to multiple deep learning frameworks such as Fastai and PyTorch Lightning. What makes IceVision awesome is you can train state-of-the-art object detection models with only few lines of codes. It\u0026rsquo;s very easy to get started, check out the tutorial here.\nIn the getting started notebook, we used a dataset from Icedata repository known as the Fridge Objects dataset. This dataset consists 134 images of 4 classes: can, carton, milk bottle, water bottle. Let\u0026rsquo;s now continue to train our model. Let\u0026rsquo;s train a VFNet model with a ResNet backbone implemented in mmdet. In the notebook, you can easily specify this model using two lines of codes as follows.\n1 2 model_type = models.mmdet.vfnet backbone = model_type.backbones.resnet50_fpn_mstrain_2x After you\u0026rsquo;re satisfied with the performance of your model, let\u0026rsquo;s save the model into a checkpoint to be used for inferencing later. With IceVision this can be done easily. Just add the following snippet to your notebook and run.\n1 2 3 4 5 6 7 8 from icevision.models.checkpoint import * save_icevision_checkpoint(model, model_name=\u0026#39;mmdet.vfnet\u0026#39;, backbone_name=\u0026#39;resnet50_fpn_mstrain_2x\u0026#39;, img_size=image_size, classes=parser.class_map.get_classes(), filename=\u0026#39;./models/model_checkpoint.pth\u0026#39;, meta={\u0026#39;icevision_version\u0026#39;: \u0026#39;0.12.0\u0026#39;}) Feel free to modify the model_name, backbone_name according to the model you used during training. The img_size argument is image size that the model is trained on. The classes argument is a list of classes from the dataset. The filename argument specifies the directory and name of the checkpoint file. The meta argument stores other metadata that you would like to keep track of for future reference.\nThe notebook that I used for this section can be found here.\nUser Interface with Gradio At this point, in order to run inference on the model, one will need to write inference codes as shown here. This is non-trivial and can be time-consuming especially to those who are not familiar. Gradio simplifies this by providing a simple graphical user interface wrapping the inference code so that anyone can run inference on the model without having to code.\nThe following figure shows a screenshot of the Gradio app that runs in the browser. The left pane shows the input image, and the right pane shows the inference results. Users can upload an image or select from a list of example images and click on Submit to run it through the model for inference.\nSo how do we load our model into the Gradio app? First, we must first install the Gradio package by running pip install gradio. Next, create a file with the name app.py and paste the following codes into the file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from gradio.outputs import Label from icevision.all import * from icevision.models.checkpoint import * import PIL import gradio as gr import os # Load model checkpoint_path = \u0026#34;model_checkpoint.pth\u0026#34; checkpoint_and_model = model_from_checkpoint(checkpoint_path) model = checkpoint_and_model[\u0026#34;model\u0026#34;] model_type = checkpoint_and_model[\u0026#34;model_type\u0026#34;] class_map = checkpoint_and_model[\u0026#34;class_map\u0026#34;] # Transforms img_size = checkpoint_and_model[\u0026#34;img_size\u0026#34;] valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(img_size), tfms.A.Normalize()]) # Populate examples in Gradio interface examples = [ [\u0026#39;1.jpg\u0026#39;], [\u0026#39;2.jpg\u0026#39;], [\u0026#39;3.jpg\u0026#39;] ] def show_preds(input_image): img = PIL.Image.fromarray(input_image, \u0026#34;RGB\u0026#34;) pred_dict = model_type.end2end_detect(img, valid_tfms, model, class_map=class_map, detection_threshold=0.5, display_label=False, display_bbox=True, return_img=True, font_size=16, label_color=\u0026#34;#FF59D6\u0026#34;) return pred_dict[\u0026#34;img\u0026#34;] gr_interface = gr.Interface( fn=show_preds, inputs=[\u0026#34;image\u0026#34;], outputs=[gr.outputs.Image(type=\u0026#34;pil\u0026#34;, label=\u0026#34;RetinaNet Inference\u0026#34;)], title=\u0026#34;Fridge Object Detector\u0026#34;, description=\u0026#34;A VFNet model that detects common objects found in fridge. Upload an image or click an example image below to use.\u0026#34;, examples=examples, ) gr_interface.launch(inline=False, share=False, debug=True) Make sure that the path to your model checkpoint and example images are correct. Running the app.py loads our model into the Gradio app. Run the script by typing python app.py in the terminal. If there are no errors, the terminal will show local URL to access the Gradio app. You can copy the address and open it with a browser. The URL address on my machine is http://127.0.0.1:7860/, it may vary on yours.\nHugging Face Spaces The Gradio app URL link from the previous section can only be accessed locally. But what if you would like to share the link to someone across the internet for free? In this section, we will discover how to make your Gradio app accessible to anyone by deploying the app on a free platform known as HuggingFace Spaces. Spaces is the new \u0026lsquo;marketplace\u0026rsquo; for various bleeding edge machine learning models. Many researchers have uploaded interesting and state-of-the-art models on Space to showcase them as a demo. You can discover and try them out here.\nCreating a Space To host a model on Spaces, you must sign up for an account at https://huggingface.co/. After that, head over to https://huggingface.co/spaces and click on Create New Space button as shown below.\nNext fill in the Space name and select a License. Make sure to select Gradio as the Space SDK and keep the repository Public. Click on Create space button when you\u0026rsquo;re done.\nOnce done, your Space is now ready. The Space you\u0026rsquo;ve created behaves like a git repository. You can perform various git related operations such as git clone, git push and git pull to update the repository. Alternatively, you can also add files into the Space directly in the browser.\nIn this blog post, I am going to show you how add files into your Space using the browser.\nInstallation files Let\u0026rsquo;s start with the installation files. These are the files that determines the packages that will be installed on your Space to run your app. The packages are specified in two files ie. requirements.txt, and packages.txt.\nThe requirements.txt lists all the Python packages that will be pip-installed on the Space. The packages.txt is a file created to specify the OpenCV version to be installed on your Space. This package will be read and installed with apt-get install. For some reason putting the opencv-python package in the requirements.txt file doesn\u0026rsquo;t work.\nLet\u0026rsquo;s begin adding these files. Click on the Files and versions tab. Next, click on Add file and Create a new file. Name your file as requirements.txt and paste the following snippets as the content. Click on Commit new file button at the bottom of the page.\n1 2 3 4 5 --find-links https://download.openmmlab.com/mmcv/dist/cpu/torch1.10.0/index.html mmcv-full==1.3.17 mmdet==2.17.0 gradio==2.7.5 icevision[all]==0.12.0 Now, do the same for packages.txt which only has the OpenCV package as the file content.\n1 python3-opencv We are now done adding all installation files into our Space.\nGradio application file Next let\u0026rsquo;s add the Gradio app, model checkpoint and some sample images. Let\u0026rsquo;s add the app.py we had from the previous section using the same method we did for the installation files. The app.py hosts the logic of your application and this is where the code for the Gradio app resides. Space will automatically run app.py upon startup.\nModel checkpoint and samples Next let\u0026rsquo;s add our checkpoint file model_checkpoint.pth by clicking on Upload File. Drag and drop the model checkpoint file and click on Commit changes.\nYou will also see a Building status indicating that it is setting up by installing the packages and running it upon completion. Every time there is a change in any of the files, the Space will be rebuilt.\nUsing the same method let\u0026rsquo;s upload the sample images as well. Make sure the filename of your sample image matches the filename specified in line 20-24 of app.py.\n20examples = [ 21 [\u0026#39;1.jpg\u0026#39;], 22 [\u0026#39;2.jpg\u0026#39;], 23 [\u0026#39;3.jpg\u0026#39;] 24] At this point the Space repository should look like the following Once the building completes, the status changes to Running and the Space should look like the following and is now ready to be used.\nThe completed app looks like the following You can now share the URL to your Space to anyone across the internet for free.\nConclusion This blog post outlined a step-by-step guide on how you can deploy IceVision models on HuggingFace Space in just a few steps. Hope this helps you share the awesomeness of your model to anyone across the world. The Space used in this blog post can be found here.\nYou can also embed the running app on any webpage in your HTML code like the following:\n","description":"Share your models to anyone across the internet for free in record time.","permalink":"http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/","title":"Deploying Object Detection Models on Hugging Face Spaces","type":"Portfolio"},{"content":"A collection of recorded lecture classes for the Digital Logic Design course I taught at National Energy University, Malaysia.\nView the playlist here\n","description":"Recorded lectures.","permalink":"http://localhost:1313/blog/course_digital_logic_design/","title":"Digital Logic Design - Course","type":"Blog"}]} 