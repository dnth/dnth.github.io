<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ONNX on Dickson Neoh - Personal Portfolio</title>
    <link>https://dicksonneoh.com/tags/onnx/</link>
    <description>Recent content in ONNX on Dickson Neoh - Personal Portfolio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Apr 2022 15:00:15 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/tags/onnx/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Faster than GPU: How to 10x your Object Detection Model and Deploy on CPU at 50&#43; FPS</title>
      <link>https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</link>
      <pubDate>Sat, 30 Apr 2022 15:00:15 +0800</pubDate>
      
      <guid>https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</guid>
      <description>ðŸš¦ Motivation .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:#fff;background:#6ab0de}.notice.warning .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:0.125em;position:relative} tip
By the end of this post, you will learn how to:
 Train state-of-the-art YOLOX model with your own data. Convert the YOLOX PyTorch model into ONNX and OpenVINO IR format. Run quantization algorithm to 10x your model&amp;rsquo;s inference speed.  P/S: The final model runs faster on the CPU than the GPU!</description>
    </item>
    
  </channel>
</rss>
