<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hugging Face on Dickson Neoh - Personal Portfolio</title>
    <link>http://localhost:1313/tags/hugging-face/</link>
    <description>Recent content in Hugging Face on Dickson Neoh - Personal Portfolio</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Sep 2024 09:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/hugging-face/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime &amp; Optimizations</title>
      <link>http://localhost:1313/portfolio/supercharge_your_pytorch_image_models/</link>
      <pubDate>Mon, 30 Sep 2024 09:00:00 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/supercharge_your_pytorch_image_models/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;ðŸš€ Motivation&lt;/h3&gt;&#xA;&lt;p&gt;Having real-time inference is crucial for computer vision applications.&#xA;In some domains, a 1-second delay in inference could mean life or death.&lt;/p&gt;&#xA;&lt;p&gt;Imagine sitting in a self-driving car and the car takes &lt;strong&gt;one full second&lt;/strong&gt; to detect an oncoming speeding truck.&lt;/p&gt;&#xA;&lt;p&gt;Just one second too late, and you could end up in the clouds ðŸ‘¼ðŸ‘¼ðŸ‘¼&lt;/p&gt;&#xA;&lt;p&gt;Or if you&amp;rsquo;re lucky, you get a very up-close view of the pavement.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android &amp; iOS</title>
      <link>http://localhost:1313/portfolio/bringing_high_quality_image_models_to_mobile/</link>
      <pubDate>Thu, 16 Mar 2023 11:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/bringing_high_quality_image_models_to_mobile/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;ðŸŒŸ Motivation&lt;/h3&gt;&#xA;&lt;p&gt;For many data scientist (including myself), we pride ourselves in training a model, seeing the loss graph go down, and claim victory when the test set accuracy reaches 99.99235%.&lt;/p&gt;&#xA;&lt;p&gt;Why not?&lt;/p&gt;&#xA;&lt;p&gt;This is the after all the juiciest part of the job. &amp;ldquo;Solving&amp;rdquo; one dataset after another, it may seem like anything around you can be &lt;em&gt;conquered&lt;/em&gt; with a simple &lt;code&gt;model.fit&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;That was me two years ago.&lt;/p&gt;&#xA;&lt;p&gt;The naive version of me thought that was all about it with machine learning (ML).&#xA;As long as we have a dataset, ML is the way to go.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying GPT-J Models on a Telegram Bot with Hugging Face Hub - For Free</title>
      <link>http://localhost:1313/portfolio/deploy_gpt_hf_models_on_telegram/</link>
      <pubDate>Thu, 19 May 2022 11:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/deploy_gpt_hf_models_on_telegram/</guid>
      <description>&lt;h3 id=&#34;-motivation&#34;&gt;ðŸ’¥ Motivation&lt;/h3&gt;&#xA;&lt;style type=&#34;text/css&#34;&gt;&#xA;    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice&#xA;    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0&#xA;    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning&#xA;    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info&#xA;    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note&#xA;    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip&#xA;    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice&#xA;    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline&#xA;    svg{top:0.125em;position:relative}&lt;/style&gt;&#xA;    &lt;div&gt;&lt;svg width=&#34;0&#34; height=&#34;0&#34; display=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&#xA;            &lt;symbol id=&#34;tip-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;note-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;warning-notice&#34; viewBox=&#34;0 0 576 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;info-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;        &lt;/svg&gt;&lt;/div&gt;&lt;div class=&#34;notice tip&#34; &gt;&#xA;        &lt;p class=&#34;first notice-title&#34;&gt;&lt;span class=&#34;icon-notice baseline&#34;&gt;&lt;svg&gt;&#xA;                    &lt;use href=&#34;#tip-notice&#34;&gt;&lt;/use&gt;&#xA;                &lt;/svg&gt;&lt;/span&gt;tip&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Deploy Object Detection Models on Android with Flutter</title>
      <link>http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/</link>
      <pubDate>Sun, 17 Apr 2022 15:00:15 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/</guid>
      <description>&lt;h3 id=&#34;-deployment-where-ml-models-go-to-die&#34;&gt;ðŸš‘ Deployment: Where ML models go to die&lt;/h3&gt;&#xA;&lt;p&gt;In this post, I will outline the basic steps to deploy ML models onto lightweight mobile devices &lt;strong&gt;easily, quickly and for free&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;style type=&#34;text/css&#34;&gt;&#xA;    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice&#xA;    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0&#xA;    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning&#xA;    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info&#xA;    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note&#xA;    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip&#xA;    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice&#xA;    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline&#xA;    svg{top:0.125em;position:relative}&lt;/style&gt;&#xA;    &lt;div&gt;&lt;svg width=&#34;0&#34; height=&#34;0&#34; display=&#34;none&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&#xA;            &lt;symbol id=&#34;tip-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;note-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;warning-notice&#34; viewBox=&#34;0 0 576 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;            &lt;symbol id=&#34;info-notice&#34; viewBox=&#34;0 0 512 512&#34; preserveAspectRatio=&#34;xMidYMid meet&#34;&gt;&#xA;                &lt;path&#xA;                    d=&#34;M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z&#34; /&gt;&#xA;            &lt;/symbol&gt;&#xA;        &lt;/svg&gt;&lt;/div&gt;&lt;div class=&#34;notice tip&#34; &gt;&#xA;        &lt;p class=&#34;first notice-title&#34;&gt;&lt;span class=&#34;icon-notice baseline&#34;&gt;&lt;svg&gt;&#xA;                    &lt;use href=&#34;#tip-notice&#34;&gt;&lt;/use&gt;&#xA;                &lt;/svg&gt;&lt;/span&gt;tip&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying Object Detection Models on Hugging Face Spaces</title>
      <link>http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/</link>
      <pubDate>Thu, 17 Feb 2022 13:42:56 +0800</pubDate>
      <guid>http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;So, youâ€™ve trained a deep learning model that can detect objects from images.&#xA;Next, how can you share the awesomeness of your model with the rest of the world?&#xA;You might be a PhD student trying to get some ideas from your peers or supervisors, or a startup founder who wishes to share a minimum viable product to your clients for feedback.&#xA;But, at the same time you don&amp;rsquo;t wish to go through the hassle of dealing with MLOps.&#xA;This blog post is for you. In this post I will walk you through how to deploy your model and share them to the world for free!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
