<!DOCTYPE html>
<html>
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</title>
  <meta name="description" content="Leverage hundreds state-of-the-art models on IceVision trained with best practices of Fastai" />

  <meta property="og:url" content="http://localhost:1313/portfolio/training_dl_model_for_cell_counting/">
  <meta property="og:site_name" content="Dickson Neoh - Personal Portfolio">
  <meta property="og:title" content="Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images">
  <meta property="og:description" content="Leverage hundreds state-of-the-art models on IceVision trained with best practices of Fastai">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="portfolio">
    <meta property="article:published_time" content="2022-04-11T15:07:15+08:00">
    <meta property="article:modified_time" content="2022-04-11T15:07:15+08:00">
    <meta property="article:tag" content="IceVision">
    <meta property="article:tag" content="Fast.ai">
    <meta property="article:tag" content="LabelImg">
    <meta property="article:tag" content="Cell-Counting">
    <meta property="article:tag" content="Microbiology">
    <meta property="og:image" content="http://localhost:1313/images/portfolio/training_dl_model_for_cell_counting/post_image.png">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/portfolio/training_dl_model_for_cell_counting/post_image.png">
  <meta name="twitter:title" content="Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images">
  <meta name="twitter:description" content="Leverage hundreds state-of-the-art models on IceVision trained with best practices of Fastai">


  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/slick/slick.css" />
  <link rel="stylesheet" href="http://localhost:1313/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/font-awesome/css/font-awesome.min.css" />

  <!-- Magnific Popup -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="http://localhost:1313/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="http://localhost:1313/images/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="http://localhost:1313/images/favicon.gif" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-54500366-2');
  </script>
  
</head>

<body>
  <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
    <a href="http://localhost:1313/" class="navbar-brand">
      <img src="http://localhost:1313/images/site-navigation/logo_dn_resize.png" alt="site-logo">
    </a>
    <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
      <ul class="nav navbar-nav main-navigation my-0 mx-auto">
        
        
        <li class="nav-item">
          <a href="http://localhost:1313/#home"
            class="nav-link text-dark text-sm-center p-2 ">Home</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#about"
            class="nav-link text-dark text-sm-center p-2 ">About</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#service"
            class="nav-link text-dark text-sm-center p-2 ">Services</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#portfolio"
            class="nav-link text-dark text-sm-center p-2 ">Projects</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#resume"
            class="nav-link text-dark text-sm-center p-2 ">Resume</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#skills"
            class="nav-link text-dark text-sm-center p-2 ">Skills</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#blog"
            class="nav-link text-dark text-sm-center p-2 ">Blogs</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#contact"
            class="nav-link text-dark text-sm-center p-2 ">Contact</a>
        </li>
        
      </ul>
      <div class="navbar-nav">
        <a href="http://localhost:1313/contact" class="btn btn-primary btn-zoom hire_button">Hire Me Now</a>
      </div>
       

    </div>
  </div>
</nav>
  <div id="content">
    

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          
        </nav>
      </div>
    </div>

    <div class="row p-3">
      <div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center">
        <i class="fa fa-calendar"></i> &ensp;
        April 11, 2022 &ensp; &ensp;
        <i class="fa fa-clock-o"></i> &ensp;
        16 mins read
      </div>
      
        <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
          <i class="fa fa-tag"></i> &ensp;
          
            <a href="http://localhost:1313/tags/icevision/">IceVision</a> 
            
              <span class="separator">‚Ä¢</span>
            
          
            <a href="http://localhost:1313/tags/fast.ai/">Fast.ai</a> 
            
              <span class="separator">‚Ä¢</span>
            
          
            <a href="http://localhost:1313/tags/labelimg/">LabelImg</a> 
            
              <span class="separator">‚Ä¢</span>
            
          
            <a href="http://localhost:1313/tags/cell-counting/">Cell-Counting</a> 
            
              <span class="separator">‚Ä¢</span>
            
          
            <a href="http://localhost:1313/tags/microbiology/">Microbiology</a> 
            
          
        </div>
      
      
        <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
          <i class="fa fa-folder"></i> &ensp;
          
            <a href="http://localhost:1313/categories/modeling/">Modeling</a>
            
              <span class="separator">‚Ä¢</span>
            
          
            <a href="http://localhost:1313/categories/object-detection/">Object-Detection</a>
            
              <span class="separator">‚Ä¢</span>
            
          
            <a href="http://localhost:1313/categories/tutorial/">Tutorial</a>
            
          
        </div>
      
    </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
    <img src=http://localhost:1313/images/hero/figure-svg.svg alt="">
  </div>
  <div class="animate-shape">
    <img src=http://localhost:1313/images/skill/skill-background-shape.svg alt="">
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600">
      <defs>
        <linearGradient id="d" x1="0.929" y1="0.111" x2="0.263" y2="0.935" gradientUnits="objectBoundingBox">
          <stop offset="0" stop-color="#f1f6f9" />
          <stop offset="1" stop-color="#f1f6f9" stop-opacity="0" />
        </linearGradient>
      </defs>
      <g data-name="blob-shape (3)">
        <path class="blob" fill="url(#d)"
          d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z" />
      </g>
    </svg>
  </div>
  <div class="animate-pattern">
    <img src=http://localhost:1313/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <div class="singleBlog__feature">
          <img src=http://localhost:1313/images/portfolio/training_dl_model_for_cell_counting/post_image.png alt="feature-image">
        </div>
      </div>
    </div>
    <div class="row mt-5">
      <div class="col-lg-12">
        <div class="singleBlog__content">
          <hr>
          
            <h3>Table of Contents</h3>
            <nav id="TableOfContents">
  <ul>
        <li><a href="#-motivation">üï∂Ô∏è Motivation</a></li>
        <li><a href="#-installation">‚öôÔ∏è Installation</a></li>
        <li><a href="#-labeling-the-data">üîñ Labeling the data</a></li>
        <li><a href="#-modeling">üåÄ Modeling</a>
          <ul>
            <li><a href="#-preparing-datasets">üéØ Preparing datasets</a></li>
            <li><a href="#-choosing-a-library-model-and-backbone">üóùÔ∏è Choosing a library, model, and backbone</a></li>
            <li><a href="#-metrics-and-training">üèÉ Metrics and Training</a></li>
            <li><a href="#-exporting-model">üì® Exporting model</a></li>
          </ul>
        </li>
        <li><a href="#-inferencing-on-a-new-image">üß≠ Inferencing on a new image</a></li>
        <li><a href="#-wrapping-up">üìñ Wrapping Up</a></li>
        <li><a href="#-comments--feedback">üôè Comments &amp; Feedback</a></li>
      </ul>
</nav>
          
          <hr>
          <h3 id="-motivation">üï∂Ô∏è Motivation</h3>
<p>Many biology and medical procedures involve counting cells from images taken with a microscope.
Counting cells reveals the concentration of bacteria and viruses and gives vital information on the progress of a disease.</p>
<p>To accomplish the counting, researchers painstakingly count the cells by hand with the assistance of a device called <a href="https://www.youtube.com/watch?v=WWS9sZbGj6A&amp;ab_channel=ThermoFisherScientific" target="_blank" rel="nofollow noopener noreferrer">hemocytometer</a>.
This process is repetitive, tedious, and prone to errors.</p>
<p><em>What if we could automate the counting by using an intelligent deep learning algorithm instead?</em></p>
<p>In this blog post, I will walk you through how to use the <a href="https://airctic.com/dev/getting_started_object_detection/" target="_blank" rel="nofollow noopener noreferrer">IceVision</a> library and train a state-of-the-art deep learning model with <a href="https://github.com/fastai/fastai" target="_blank" rel="nofollow noopener noreferrer">Fastai</a> to count microalgae cells.</p>
<style type="text/css">
    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
    svg{top:0.125em;position:relative}</style>
    <div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
            <symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
            </symbol>
            <symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
            </symbol>
            <symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
            </symbol>
            <symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
            </symbol>
        </svg></div><div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>By the end of this post, you will learn how to:</p>
<ul>
<li>How to install the IceVision and and labelImg package.</li>
<li>Prepare and label any dataset for object detection.</li>
<li>Train a high-performance <code>VFNet</code> model with IceVision &amp; Fastai.</li>
<li>Use the model for inference on new images.</li>
</ul>
<p><strong>P/S</strong>: The end result - A high performance object detector in only 17 lines of code! üöÄ</p></div>





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/inference.png" srcset="/portfolio/training_dl_model_for_cell_counting/inference_hu3527343430129712590.png 360w, /portfolio/training_dl_model_for_cell_counting/inference_hu2051645480678992338.png 720w, /portfolio/training_dl_model_for_cell_counting/inference_hu5851132926929726688.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>


<p>Did I mention that all the tools used in this project are completely open-source and free of charge? Yes!
If you&rsquo;re ready let&rsquo;s begin.</p>
<!-- 




<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/quote.png" srcset="/portfolio/training_dl_model_for_cell_counting/quote_hu3752763430904016176.png 360w, /portfolio/training_dl_model_for_cell_counting/quote_hu15973446780296611971.png 720w, /portfolio/training_dl_model_for_cell_counting/quote_hu5870353358440137951.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

 -->
<h3 id="-installation">‚öôÔ∏è Installation</h3>
<p>Throughout this post, we will make use a library known as <a href="https://airctic.com/0.12.0/" target="_blank" rel="nofollow noopener noreferrer">IceVision</a> - a computer vision-focused library built to work with <a href="https://github.com/fastai/fastai" target="_blank" rel="nofollow noopener noreferrer">Fastai</a>. Let&rsquo;s install them first.</p>
<p>There are many ways to accomplish the installation.
For your convenience, I&rsquo;ve prepared an installation script that simplifies the process into just a few lines of code.</p>
<p>To get started, let&rsquo;s clone the Git repository by typing the following in your terminal:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/dnth/microalgae-cell-counter-blogpost
</span></span></code></pre></div><p>Next, navigate into the directory:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd microalgae-cell-counter-blogpost/
</span></span></code></pre></div><p>Install IceVision and all other libraries used for this post:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>bash icevision_install.sh cuda11 0.12.0
</span></span></code></pre></div><p>Depending on your system <code>CUDA</code> version, you may want to change <code>cuda11</code> to <code>cuda10</code>, especially on older systems.
The number following the <code>CUDA</code> version is the version of IceVision.
The version I&rsquo;m using for this blog post is <code>0.12.0</code>.
You can alternatively replace the version number with <code>master</code> to install the bleeding edge version of IceVision from the master branch on Github.</p>
<p>If you would like to install the CPU version of the library it can be done with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>bash icevision_install.sh cpu 0.12.0
</span></span></code></pre></div><div class="notice info" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#info-notice"></use>
                </svg></span>info</p><p>Training an object detection model on a CPU can be many times slower compared to a GPU.
If you do not have an available GPU, use <a href="https://colab.research.google.com/" target="_blank" rel="nofollow noopener noreferrer">Google Colab</a>. </p></div>
<p>The installation may take a few minutes depending on your internet connection speed.
Let the installation complete before proceeding.</p>
<h3 id="-labeling-the-data">üîñ Labeling the data</h3>
<p>All deep learning models require data to work.
To construct a model for microalgae cell counting, we require images of microalgae cells to work with.
For the purpose of this post, I&rsquo;ve acquired image samples from a lab.</p>
<p>The following shows a sample image of the microalgae cells as seen through a microscope.
The cells are colored green.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/hemocytometer.jpg" srcset="/portfolio/training_dl_model_for_cell_counting/hemocytometer_hu4135327651567365190.jpg 360w, /portfolio/training_dl_model_for_cell_counting/hemocytometer_hu768015082218673367.jpg 720w, /portfolio/training_dl_model_for_cell_counting/hemocytometer_hu7548118093443355443.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" alt="Can you count how many cells are present in this image?" />
    
    
    <figcaption>
        <p style="text-align: center;"><small>
        Can you count how many cells are present in this image?
        
            
        
        </small></p> 
    </figcaption>
    
</figure>

</p>
<p>There are a bunch of other images in the <code>data/not_labeled/</code> folder.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/dataset_sample.png" srcset="/portfolio/training_dl_model_for_cell_counting/dataset_sample_hu12767056833071613357.png 360w, /portfolio/training_dl_model_for_cell_counting/dataset_sample_hu8549963374764502329.png 720w, /portfolio/training_dl_model_for_cell_counting/dataset_sample_hu6378381327670950199.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

</p>
<p>There is only one issue now, and that is the images are not labeled.
Let&rsquo;s label the images with bounding boxes using an open-source image labeling tool <a href="https://github.com/tzutalin/labelImg" target="_blank" rel="nofollow noopener noreferrer">labelImg</a>.</p>
<p>The <code>labelImg</code> app enables us to label images with class names and bounding boxes surrounding the object of interest.
The following figure shows a demo of the app.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot.jpg" srcset="/portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot_hu13807641137641906544.jpg 360w, /portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot_hu385633525159828125.jpg 720w, /portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot_hu8224050367298907079.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

</p>
<p>The <code>labelImg</code> app is already installed in the installation step.
To launch the app, type in your terminal:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>labelImg
</span></span></code></pre></div><p>A window like the following should appear.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/labelimg_start.png" srcset="/portfolio/training_dl_model_for_cell_counting/labelimg_start_hu15759557383422184328.png 360w, /portfolio/training_dl_model_for_cell_counting/labelimg_start_hu14990276704710602133.png 720w, /portfolio/training_dl_model_for_cell_counting/labelimg_start_hu17628428663927974566.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

</p>
<p>Let&rsquo;s load the <code>data/not_labeled/</code> images folder into <code>labelImg</code> and start labeling them!
To do that, click on the <strong>Open Dir</strong> icon and navigate to the folder.</p>
<p>An image should now show up in <code>labelImg</code>.
To label, click on the <strong>Create RectBox</strong> icon to start drawing bounding boxes around the microalgae cells.
Next, you will be prompted to enter a label name.
Key in <code>microalgae</code> as the label name.
Once done, a rectangular bounding box should appear on-screen.</p>
<!-- 




<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/labelimg_loaded.png" srcset="/portfolio/training_dl_model_for_cell_counting/labelimg_loaded_hu593883362565650690.png 360w, /portfolio/training_dl_model_for_cell_counting/labelimg_loaded_hu8133796620100110092.png 720w, /portfolio/training_dl_model_for_cell_counting/labelimg_loaded_hu14291502145683485641.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

 -->
<figure><img src="/portfolio/training_dl_model_for_cell_counting/label.gif" width="700">
</figure>

<p>Now comes the repetitive part, we will need to draw a bounding box for each microalgae cell for all images in the folder.
To accelerate the process I highly recommend the use of hotkeys keys with <code>labelImg</code>.
The hotkeys are shown below.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/hotkeys.png" srcset="/portfolio/training_dl_model_for_cell_counting/hotkeys_hu16800903957444330344.png 360w, /portfolio/training_dl_model_for_cell_counting/hotkeys_hu17065029086626337031.png 720w, /portfolio/training_dl_model_for_cell_counting/hotkeys_hu6332320019217241535.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" width="400" />
    
    
</figure>

</p>
<p>Once done, remember to save the annotations.
The annotations are saved in an <code>XML</code> file with a file name matching to the image file name as shown below.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/xml_files.png" srcset="/portfolio/training_dl_model_for_cell_counting/xml_files_hu11216654128083210576.png 360w, /portfolio/training_dl_model_for_cell_counting/xml_files_hu16172132550861217442.png 720w, /portfolio/training_dl_model_for_cell_counting/xml_files_hu7137647071455947929.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

</p>
<p>It took me a few hours to meticulously label the images.
If you don&rsquo;t feel like spending time labeling all the images (although I recommend doing them at least once), you can find the labeled ones in the <code>data/labeled/</code> folder.</p>
<h3 id="-modeling">üåÄ Modeling</h3>
<p>Once the labeling is done, we are now ready to start modeling in a <code>jupyter</code> notebook environment.</p>
<p>To launch the <code>jupyter</code> notebook run the following in your terminal</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>jupyter lab
</span></span></code></pre></div><p>A browser window should pop up.
On the left pane, double click the <code>train.ipynb</code> to open the notebook.
All the codes in this section are inside the notebook.
Here, I will attempt to walk you through just enough details of the code to get you started with modeling on your own data.
If you require further clarifications, the IceVision <a href="https://airctic.com/0.12.0/" target="_blank" rel="nofollow noopener noreferrer">documentation</a> is a good starting point.
Or drop me a <a href="https://dicksonneoh.com/contact/" target="_blank" rel="nofollow noopener noreferrer">message</a>.</p>
<p>The first cell in the notebook is the imports.
With IceVision all the necessary components are imported with one line of code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> icevision.all <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span></code></pre></div><p>If something wasn&rsquo;t properly installed, the imports will raise an error message.
In that event, you must go back to the installation step before proceeding.
If there are no errors, we are ready to dive in further.</p>
<h4 id="-preparing-datasets">üéØ Preparing datasets</h4>
<p>After the imports, we must now load the labeled images and bounding boxes into <code>jupyter</code>.
This is also known as <em>data parsing</em> and is accomplished with the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>parser <span style="color:#f92672">=</span> parsers<span style="color:#f92672">.</span>VOCBBoxParser(annotations_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/labeled&#34;</span>, images_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/labeled&#34;</span>)
</span></span></code></pre></div><p>The parameter <code>annotations_dir</code> and <code>images_dir</code> are the directories to the images and annotations respectively.
Since both the images and annotations are located in the same directory, they are the same as such in the code.</p>
<p>Next, we will randomly pick and divide the images and bounding boxes into two groups of data namely <code>train_records</code> and <code>valid_records</code>.
By default, the split will be <code>80:20</code> to <code>train:valid</code> proportion.
You can change the ratio by altering the values in <code>RandomSplitter</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train_records, valid_records <span style="color:#f92672">=</span> parser<span style="color:#f92672">.</span>parse(data_splitter<span style="color:#f92672">=</span>RandomSplitter([<span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">0.2</span>])
</span></span></code></pre></div><p>The following code shows the class names from the parsed data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>parser<span style="color:#f92672">.</span>class_map
</span></span></code></pre></div><p>It should output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&lt;</span>ClassMap: {<span style="color:#e6db74">&#39;background&#39;</span>: <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;Microalgae&#39;</span>: <span style="color:#ae81ff">1</span>}<span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><p>which shows a <code>ClassMap</code> that contains the class name as the key and class index as the value in a Python <a href="https://www.w3schools.com/python/python_dictionaries.asp" target="_blank" rel="nofollow noopener noreferrer">dictionary</a>.
The <code>background</code> class is automatically added.
In the data labeling step, we do not need to label the background.</p>
<p>Next, we will apply basic data augmentation which is a technique used to diversify the training images by applying the random transformation.
Learn more <a href="https://medium.com/analytics-vidhya/image-augmentation-9b7be3972e27" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p>
<p>The following code specifies the kinds of transformations we would like to perform on our images.
Behind the scenes, these transformations are performed with the <a href="https://albumentations.ai/" target="_blank" rel="nofollow noopener noreferrer">Albumentations</a> library.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">640</span>
</span></span><span style="display:flex;"><span>train_tfms <span style="color:#f92672">=</span> tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Adapter([<span style="color:#f92672">*</span>tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>aug_tfms(size<span style="color:#f92672">=</span>image_size, presize<span style="color:#f92672">=</span>image_size<span style="color:#f92672">+</span><span style="color:#ae81ff">128</span>), tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Normalize()])
</span></span><span style="display:flex;"><span>valid_tfms <span style="color:#f92672">=</span> tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Adapter([<span style="color:#f92672">*</span>tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>resize_and_pad(image_size), tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Normalize()])
</span></span></code></pre></div><p>We must specify the dimensions of the image in <code>image_size = 640</code>.
This value will then be used in <code>tfms.A.aug_tfms</code> that ensures that all images are resized to a <code>640x640</code> resolution and normalized in <code>tfms.A.Normalize()</code>.</p>
<p>Some models like <code>EfficientDet</code> only work with image size divisible by <code>128</code>.
Other common values you can try are <code>384</code>, <code>512</code>, <code>768</code>, etc.
But beware using a large image size may consume more memory and in some cases halts training.
Starting with a small value like <code>384</code> is probably a good idea.
I found <code>640</code> works best for this dataset.</p>
<p>Use <code>tfms.A.aug_tfms</code> performs transformations to the image such as varying the lighting, rotation, shifting, flipping, blurring, padding, etc.
The full list of transforms and the parameters can be found in the <code>aug_tfms</code> <a href="https://airctic.com/0.12.0/albumentations_tfms/" target="_blank" rel="nofollow noopener noreferrer">documentation</a>.</p>
<p>In this code snippet, we created two distinct transforms namely <code>train_tfms</code> and <code>valid_tfms</code> that will be used during the training and validation steps respectively.</p>
<p>Next, we will apply the <code>train_tfms</code> to our <code>train_records</code> and <code>valid_tfms</code> to <code>valid_records</code> with the following snippet.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train_ds <span style="color:#f92672">=</span> Dataset(train_records, train_tfms)
</span></span><span style="display:flex;"><span>valid_ds <span style="color:#f92672">=</span> Dataset(valid_records, valid_tfms)
</span></span></code></pre></div><p>This results in the creation of a <code>Dataset</code> object which is a collection of transformed images and bounding boxes.</p>
<p>To visualize the <code>train_ds</code> we can run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>samples <span style="color:#f92672">=</span> [train_ds[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>)]
</span></span><span style="display:flex;"><span>show_samples(samples, ncols<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)
</span></span></code></pre></div><p>This will show us 4 samples from the <code>train_ds</code>.
Note the variations in lighting, translation, and rotation compared to the original images.





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/show_ds.png" srcset="/portfolio/training_dl_model_for_cell_counting/show_ds_hu13304492532064535652.png 360w, /portfolio/training_dl_model_for_cell_counting/show_ds_hu6357921854194327456.png 720w, /portfolio/training_dl_model_for_cell_counting/show_ds_hu16445726036934519474.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

</p>
<p>The transformations are applied on the fly.
So each run on the snippet produces slightly different results.</p>
<h4 id="-choosing-a-library-model-and-backbone">üóùÔ∏è Choosing a library, model, and backbone</h4>
<p>IceVision supports hundreds of high-quality pre-trained models from <a href="https://github.com/pytorch/vision" target="_blank" rel="nofollow noopener noreferrer">Torchvision</a>, Open MMLab&rsquo;s <a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="nofollow noopener noreferrer">MMDetection</a>, Ultralytic&rsquo;s <a href="https://github.com/ultralytics/yolov5" target="_blank" rel="nofollow noopener noreferrer">YOLOv5</a> and Ross Wightman&rsquo;s <a href="https://github.com/rwightman/efficientdet-pytorch" target="_blank" rel="nofollow noopener noreferrer">EfficientDet</a>.</p>
<p>Depending on your preference, you may choose the model and backbone from these libraries.
In this post I will choose the <a href="https://arxiv.org/abs/2008.13367" target="_blank" rel="nofollow noopener noreferrer">VarifocalNet</a> (<code>VFNet</code>) model from MMDetection which can be accomplished with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_type <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>mmdet<span style="color:#f92672">.</span>vfnet
</span></span><span style="display:flex;"><span>backbone <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>backbones<span style="color:#f92672">.</span>resnet50_fpn_mstrain_2x
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>model(backbone<span style="color:#f92672">=</span>backbone(pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), num_classes<span style="color:#f92672">=</span>len(parser<span style="color:#f92672">.</span>class_map)) 
</span></span></code></pre></div><p>There are various ResNet backbones that you can select from such as
<code>resnet50_fpn_1x</code>,
<code>resnet50_fpn_mstrain_2x</code>,
<code>resnet50_fpn_mdconv_c3_c5_mstrain_2x</code>,
<code>resnet101_fpn_1x</code>,
<code>resnet101_fpn_mstrain_2x</code>,
<code>resnet101_fpn_mdconv_c3_c5_mstrain_2x</code>,
<code>resnext101_32x4d_fpn_mdconv_c3_c5_mstrain_2x</code>, and
<code>resnext101_64x4d_fpn_mdconv_c3_c5_mstrain_2x</code>.</p>
<p>Additionally, IceVision also recently supports state-of-the-art Swin Transformer backbone for the VFNet model
<code>swin_t_p4_w7_fpn_1x_coco</code>,
<code>swin_s_p4_w7_fpn_1x_coco</code>, and
<code>swin_b_p4_w7_fpn_1x_coco</code>.</p>
<p>Which combination of <code>model_type</code> and <code>backbone</code> that performs best is something you need to experiment with.
Feel free to experiment and swap out the backbone and note the performance of the model.
There are other model types with their respective backbones which you can find <a href="https://github.com/airctic/icevision/blob/master/notebooks/getting_started_object_detection.ipynb" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p>
<h4 id="-metrics-and-training">üèÉ Metrics and Training</h4>
<p>To start the training, the model needs to take in the images and bounding boxes from the <code>train_ds</code> and <code>valid_ds</code> we created.</p>
<p>For that, we will need to use a dataloader which will help us iterate over the elements in the dataset we created and load them into the model.
We will construct two separate dataloaders for <code>train_ds</code> and <code>valid_ds</code> respectively.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train_dl <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>train_dl(train_ds, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>valid_dl <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>valid_dl(valid_ds, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p>Here, we can specify the <code>batch_size</code> parameter which is the number of images and bounding boxes given to the model in a single forward pass.
The <code>shuffle</code> parameter specifies if you would like to randomly shuffle the order of the data.
The <code>num_workers</code> parameter specifies how many sub-processes to use to load the data.
Let&rsquo;s keep it at <code>4</code> for now.</p>
<p>Next, we need to specify a measure of how well our model performs during training.
This measure is specified using a metric - which involves using specific math equations to output a score that tells us if the model is improving or not during training.
Some commonly used metrics include accuracy, error rate, F1 Score, etc.
For object detection tasks the <code>COCOMetric</code> is commonly used.
If you are interested <a href="https://blog.zenggyu.com/en/post/2018-12-16/an-introduction-to-evaluation-metrics-for-object-detection/" target="_blank" rel="nofollow noopener noreferrer">this blog</a> explains the math behind the metrics used for object detection.</p>
<p>Once the metric is defined, we can then load all three components - dataloaders, model, and metric into a Fastai <code>Learner</code> for training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>metrics <span style="color:#f92672">=</span> [COCOMetric(metric_type<span style="color:#f92672">=</span>COCOMetricType<span style="color:#f92672">.</span>bbox)]
</span></span><span style="display:flex;"><span>learn <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>fastai<span style="color:#f92672">.</span>learner(dls<span style="color:#f92672">=</span>[train_dl, valid_dl], model<span style="color:#f92672">=</span>model, metrics<span style="color:#f92672">=</span>metrics)
</span></span></code></pre></div><p>With deep learning models, there are many hyperparameters that we can configure before we run the training.
One of the most important hyperparameters to get right is the learning rate.
Since IceVision is built to work with Fastai, we have access to a handy tool known as the learning rate finder first proposed by <a href="https://arxiv.org/abs/1506.01186" target="_blank" rel="nofollow noopener noreferrer">Leslie Smith</a> and popularized by the Fastai community for its effectiveness.
This is an incredibly simple yet powerful tool to find a range of optimal learning rate values that gives us the best training performance.</p>
<p>All we need to do is run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>learn<span style="color:#f92672">.</span>lr_find()
</span></span></code></pre></div><p>which outputs:





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/lr_find.png" srcset="/portfolio/training_dl_model_for_cell_counting/lr_find_hu14508982698442946917.png 360w, /portfolio/training_dl_model_for_cell_counting/lr_find_hu6647490568008742557.png 720w, /portfolio/training_dl_model_for_cell_counting/lr_find_hu14937520330375202717.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>

</p>
<p>The most optimal learning rate value lies in the region where the loss descends most rapidly.
From the figure above, this is somewhere in between <code>1e-4</code> to <code>1e-2</code>.
The orange dot on the plot shows the point where the slope is the steepest and is generally a good value to use as the learning rate.</p>
<p>Now, let&rsquo;s load this learning rate value of 1e-3 into the <code>fine_tune</code> function and start training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>learn<span style="color:#f92672">.</span>fine_tune(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">1e-3</span>, freeze_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>The first parameter in <code>fine_tune</code> is the number of epochs to train for.
One epoch is defined as a complete iteration over the entire dataset.
In this post, I will only train for 10 epochs.
Training for longer will likely improve the model, so I will leave that to you to experiment with.
The second parameter is the learning rate value we wish to use to train the model.
Let&rsquo;s put the value <code>1e-3</code> from the learning rate finder.</p>
<p>The above code snippet trains the model for 10 epochs.
By default, this will start the training in two phases.</p>
<p>In the first phase ‚ûÄ, only the last layer of the model is trained.
The rest of the model is frozen.
In the second phase ‚ûÅ, the entire model is trained end-to-end.
The figure below shows the training output.</p>
<p>The <code>freeze_epochs</code> parameter specifies the number of <code>epochs</code> to train in ‚ûÄ.</p>





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/train.png" srcset="/portfolio/training_dl_model_for_cell_counting/train_hu9784085991413002447.png 360w, /portfolio/training_dl_model_for_cell_counting/train_hu17332203943224860492.png 720w, /portfolio/training_dl_model_for_cell_counting/train_hu17060715769827579056.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>


<p>During the training, the <code>train_loss</code>, <code>valid_loss</code>, and <code>COCOMetric</code> are printed at every epoch.
Ideally, the losses should decrease, and <code>COCOMetric</code> should increase the longer we train.
As shown above, each epoch only took 2 seconds to complete on a GPU - which is incredibly fast.</p>
<p>Once the training completes, we can view the performance of the model by showing the inference results on <code>valid_ds</code>.
The following figure shows the output at a detection threshold of <code>0.5</code>.
You can increase the <code>detection_threshold</code> value to only show the bounding boxes with a higher confidence value.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_type<span style="color:#f92672">.</span>show_results(model, valid_ds, detection_threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">.5</span>)
</span></span></code></pre></div>




<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/show_results.png" srcset="/portfolio/training_dl_model_for_cell_counting/show_results_hu9228230060414688445.png 360w, /portfolio/training_dl_model_for_cell_counting/show_results_hu13178588556848777821.png 720w, /portfolio/training_dl_model_for_cell_counting/show_results_hu6207685631246181575.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>


<p>For completeness, here are the codes for the <em>Modeling</em> section which include steps to load the data, instantiate the model, train the model, and show the results.
That&rsquo;s only 17 lines of code!</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> icevision.all <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>parser <span style="color:#f92672">=</span> parsers<span style="color:#f92672">.</span>VOCBBoxParser(annotations_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/labeled&#34;</span>, images_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/labeled&#34;</span>)
</span></span><span style="display:flex;"><span>train_records, valid_records <span style="color:#f92672">=</span> parser<span style="color:#f92672">.</span>parse()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">640</span>
</span></span><span style="display:flex;"><span>train_tfms <span style="color:#f92672">=</span> tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Adapter([<span style="color:#f92672">*</span>tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>aug_tfms(size<span style="color:#f92672">=</span>image_size, presize<span style="color:#f92672">=</span>image_size<span style="color:#f92672">+</span><span style="color:#ae81ff">128</span>), tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Normalize()])
</span></span><span style="display:flex;"><span>valid_tfms <span style="color:#f92672">=</span> tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Adapter([<span style="color:#f92672">*</span>tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>resize_and_pad(image_size), tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Normalize()])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ds <span style="color:#f92672">=</span> Dataset(train_records, train_tfms)
</span></span><span style="display:flex;"><span>valid_ds <span style="color:#f92672">=</span> Dataset(valid_records, valid_tfms)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_type <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>mmdet<span style="color:#f92672">.</span>vfnet
</span></span><span style="display:flex;"><span>backbone <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>backbones<span style="color:#f92672">.</span>resnet50_fpn_mstrain_2x
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>model(backbone<span style="color:#f92672">=</span>backbone(pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), num_classes<span style="color:#f92672">=</span>len(parser<span style="color:#f92672">.</span>class_map)) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_dl <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>train_dl(train_ds, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>valid_dl <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>valid_dl(valid_ds, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>metrics <span style="color:#f92672">=</span> [COCOMetric(metric_type<span style="color:#f92672">=</span>COCOMetricType<span style="color:#f92672">.</span>bbox)]
</span></span><span style="display:flex;"><span>learn <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>fastai<span style="color:#f92672">.</span>learner(dls<span style="color:#f92672">=</span>[train_dl, valid_dl], model<span style="color:#f92672">=</span>model, metrics<span style="color:#f92672">=</span>metrics)
</span></span><span style="display:flex;"><span>learn<span style="color:#f92672">.</span>fine_tune(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">1e-3</span>, freeze_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_type<span style="color:#f92672">.</span>show_results(model, valid_ds, detection_threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">.5</span>)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="-exporting-model">üì® Exporting model</h4>
<p>Once you are satisfied with the performance and quality of the model, we can export all the model configurations (hyperparameters) and weights (parameters) for future use.</p>
<p>The following code packages the model into a checkpoint and exports it into a local directory.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> icevision.models.checkpoint <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>save_icevision_checkpoint(model,
</span></span><span style="display:flex;"><span>                        model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mmdet.vfnet&#39;</span>, 
</span></span><span style="display:flex;"><span>                        backbone_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;resnet50_fpn_mstrain_2x&#39;</span>,
</span></span><span style="display:flex;"><span>                        img_size<span style="color:#f92672">=</span><span style="color:#ae81ff">640</span>,
</span></span><span style="display:flex;"><span>                        classes<span style="color:#f92672">=</span>parser<span style="color:#f92672">.</span>class_map<span style="color:#f92672">.</span>get_classes(),
</span></span><span style="display:flex;"><span>                        filename<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./models/model_checkpoint.pth&#39;</span>,
</span></span><span style="display:flex;"><span>                        meta<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;icevision_version&#39;</span>: <span style="color:#e6db74">&#39;0.12.0&#39;</span>})
</span></span></code></pre></div><p>The parameters <code>model_name</code>, <code>backbone_name</code>, and <code>img_size</code> have to match what we used during training.</p>
<p><code>filename</code> specifies the directory and name of the checkpoint file.</p>
<p><code>meta</code> is an optional parameter you can use to save all other information about the model.</p>
<p>Once completed the checkpoint should be saved in the <code>models/</code> folder. We can now use this checkpoint independently outside of the training notebook.</p>
<h3 id="-inferencing-on-a-new-image">üß≠ Inferencing on a new image</h3>
<p>To demonstrate that the model checkpoint file can be loaded independently, I created another notebook with the name <code>inference.ipynb</code>.
In this notebook, we are going to load the checkpoint and use it for inference on a brand new image.</p>
<p>Let&rsquo;s import all the necessary packages:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> icevision.all <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> icevision.models.checkpoint <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</span></span></code></pre></div><p>And specify the checkpoint path.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>checkpoint_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./models/model_checkpoint.pth&#34;</span>
</span></span></code></pre></div><p>We can load the checkpoint with the function <code>model_from_checkpoint</code>.
From the checkpoint, we can retrieve all other configurations such as the model type, class map, image size, and the transforms.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>checkpoint_and_model <span style="color:#f92672">=</span> model_from_checkpoint(checkpoint_path)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> checkpoint_and_model[<span style="color:#e6db74">&#34;model&#34;</span>]
</span></span><span style="display:flex;"><span>model_type <span style="color:#f92672">=</span> checkpoint_and_model[<span style="color:#e6db74">&#34;model_type&#34;</span>]
</span></span><span style="display:flex;"><span>class_map <span style="color:#f92672">=</span> checkpoint_and_model[<span style="color:#e6db74">&#34;class_map&#34;</span>]
</span></span><span style="display:flex;"><span>img_size <span style="color:#f92672">=</span> checkpoint_and_model[<span style="color:#e6db74">&#34;img_size&#34;</span>]
</span></span><span style="display:flex;"><span>valid_tfms <span style="color:#f92672">=</span> tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Adapter([<span style="color:#f92672">*</span>tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>resize_and_pad(img_size), tfms<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>Normalize()])
</span></span></code></pre></div><p>The model is now ready for inference.
Let&rsquo;s try to load an image with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>img <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;data/not_labeled/IMG_20191203_164256.jpg&#39;</span>)
</span></span></code></pre></div><p>We can pass the image into the <code>end2end_detect</code> function to run the inference.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pred_dict <span style="color:#f92672">=</span> model_type<span style="color:#f92672">.</span>end2end_detect(img, valid_tfms, model, 
</span></span><span style="display:flex;"><span>                                      class_map<span style="color:#f92672">=</span>class_map, 
</span></span><span style="display:flex;"><span>                                      detection_threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>                                      display_label<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, 
</span></span><span style="display:flex;"><span>                                      display_bbox<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, 
</span></span><span style="display:flex;"><span>                                      return_img<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, 
</span></span><span style="display:flex;"><span>                                      font_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, 
</span></span><span style="display:flex;"><span>                                      label_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;#FF59D6&#34;</span>)
</span></span></code></pre></div><p>The output <code>pred_dict</code> is a Python dictionary.
To view the output image with the bounding boxes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pred_dict[<span style="color:#e6db74">&#34;img&#34;</span>]
</span></span></code></pre></div><p>which outputs</p>





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/inference.png" srcset="/portfolio/training_dl_model_for_cell_counting/inference_hu3527343430129712590.png 360w, /portfolio/training_dl_model_for_cell_counting/inference_hu2051645480678992338.png 720w, /portfolio/training_dl_model_for_cell_counting/inference_hu5851132926929726688.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>


<p>To count the number of microalgae cells on the image, we can count the number of bounding boxes on the image by with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>len(pred_dict[<span style="color:#e6db74">&#39;detection&#39;</span>][<span style="color:#e6db74">&#39;bboxes&#39;</span>])
</span></span></code></pre></div><p>which outputs <code>29</code> on my computer.</p>
<p>To save the image with the bounding boxes, you can run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pred_dict[<span style="color:#e6db74">&#34;img&#34;</span>]<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;inference.png&#34;</span>)
</span></span></code></pre></div><p>As you can see, there are some missed detections of the microalgae cells.
But, considering this is our first try, and we only trained for 10 epochs (which took less than 30 seconds to complete), this is an astonishing feat!
Additionally, I&rsquo;ve only used 17 labeled images to train the model.</p>
<p>In this post, I&rsquo;ve demonstrated that we can train a sophisticated object detection model with only a few images in a very short time.
This outstanding feat is possible thanks to the Fastai library which incorporated all the best practices in training deep learning models.</p>
<p>At this point, we have not even tuned any hyperparameters (other than learning rate) to optimize performance.
Most hyperparameters are default values in Fastai that worked extremely well out-of-the-box with this dataset and model.</p>
<p>To improve performance, you may want to experiment by labeling more data and adjusting a few other hyperparameters such as image size, batch size, training epochs, the ratio of training/validation split, different model types, and backbones.</p>
<h3 id="-wrapping-up">üìñ Wrapping Up</h3>
<p>Congratulations on making it through this post! It wasn&rsquo;t that hard right?
Hopefully, this post also boosted your confidence that object detection is not as hard as it used to be.
With many high-level open-source packages like IceVision and Fastai, anyone with a computer and a little patience can break into object detection.</p>
<p>In this post, I&rsquo;ve shown you how you can construct a model that detects microalgae cells.</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>You&rsquo;ve learned:</p>
<ul>
<li>How to install the IceVision and and labelImg package.</li>
<li>Prepare a dataset of images and bounding boxes with 17 images.</li>
<li>Train a high-performance <code>VFNet</code> model with IceVision in only 17 lines of code.</li>
<li>Use the model for inference to detect microalgae cells.</li>
</ul></div>
<p>In reality, the same steps can be used to detect any other cells or any other objects for that matter.
Realizing this is an extremely powerful paradigm shift for me.</p>
<p>Think about all the problems we can solve by accurately detecting specific objects. Detecting intruders, detecting dangerous objects such as a gun, detecting defects on a production line, detecting smoke/fire, detecting skin cancer, detecting plant disease, and so much more.</p>
<p>Your creativity and imagination are the limits.
The world is your oyster. Now go out there and use this newly found superpower to make a difference.</p>





<figure>
	
    
        <img src="/portfolio/training_dl_model_for_cell_counting/quote_robert_greene.jpg" srcset="/portfolio/training_dl_model_for_cell_counting/quote_robert_greene_hu4133693763009670907.jpg 360w, /portfolio/training_dl_model_for_cell_counting/quote_robert_greene_hu14345699487701132788.jpg 720w, /portfolio/training_dl_model_for_cell_counting/quote_robert_greene_hu11732222204575676497.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" />
    
    
</figure>


<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>All the codes and data are available on this Github <a href="https://github.com/dnth/microalgae-cell-counter-blogpost" target="_blank" rel="nofollow noopener noreferrer">repository</a>. </p></div>
<h3 id="-comments--feedback">üôè Comments &amp; Feedback</h3>
<p>If you find this useful, or if you have any questions, comments, or feedback, I would be grateful if you can leave them on the following Twitter post or <a href="https://dicksonneoh.com/contact/" target="_blank" rel="nofollow noopener noreferrer">drop me a message</a>.
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Sharing a blog post on how I trained a deep learning model to count microalgae cells in 17 lines of code with 17 labeled images using IceVision + <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a> <a href="https://twitter.com/hashtag/machinelearning?src=hash&amp;ref_src=twsrc%5Etfw">#machinelearning</a> <a href="https://twitter.com/hashtag/Microbiology?src=hash&amp;ref_src=twsrc%5Etfw">#Microbiology</a> <a href="https://twitter.com/hashtag/DataScientist?src=hash&amp;ref_src=twsrc%5Etfw">#DataScientist</a> <a href="https://twitter.com/hashtag/pythonprogramming?src=hash&amp;ref_src=twsrc%5Etfw">#pythonprogramming</a> <a href="https://twitter.com/hashtag/innovation?src=hash&amp;ref_src=twsrc%5Etfw">#innovation</a> <a href="https://twitter.com/hashtag/CellBiology?src=hash&amp;ref_src=twsrc%5Etfw">#CellBiology</a><a href="https://t.co/AcEtmLS0C9">https://t.co/AcEtmLS0C9</a></p>&mdash; Dickson Neoh üöÄ (@dicksonneoh7) <a href="https://twitter.com/dicksonneoh7/status/1513478343726809090?ref_src=twsrc%5Etfw">April 11, 2022</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</p>
<p>So what&rsquo;s next? If you are interested to learn how I deploy this model on Android checkout this <a href="https://dicksonneoh.com/portfolio/how_to_deploy_od_models_on_android_with_flutter/" target="_blank" rel="nofollow noopener noreferrer">post</a>.</p>

          





<section class="social-share">

    <ul class="share-icons">
        <hr>

        <h5>ü§ü Follow me</h5>

        <p>
            Don't want to miss any of my future content? Follow me on Twitter and LinkedIn where I share these tips in
            bite-size posts.
        </p>

        
        <li>
            <a href="https://twitter.com/dicksonneoh7" target="_blank" rel="noopener" aria-label="Follow on Twitter"
                class="share-btn x">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path fill="#ffffff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
</svg>&nbsp;
                Twitter
            </a>
        </li>
        &nbsp;

        <li>
            <a href="https://www.linkedin.com/in/dickson-neoh/" target="_blank" rel="noopener"
                aria-label="Follow on LinkedIn" class="share-btn linkedin">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="linkedin_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.2264764"
     inkscape:cy="40.603642"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-129.66672,-101.87176)">
    <path
       d="m 129.66672,102.59335 v 4.90682 c 0,0.39507 0.32652,0.72159 0.72159,0.72159 h 4.90682 c 0.39507,0 0.72159,-0.32652 0.72159,-0.72159 v -4.90682 c 0,-0.39507 -0.32652,-0.72159 -0.72159,-0.72159 h -4.90682 c -0.39507,0 -0.72159,0.32652 -0.72159,0.72159 z m 5.62841,-0.14432 c 0.083,0 0.14432,0.0613 0.14432,0.14432 v 4.90682 c 0,0.083 -0.0613,0.14431 -0.14432,0.14431 h -4.90682 c -0.083,0 -0.14432,-0.0613 -0.14432,-0.14431 v -4.90682 c 0,-0.083 0.0613,-0.14432 0.14432,-0.14432 z m -4.55504,0.99219 c 0,0.2742 0.22189,0.49609 0.49609,0.49609 0.27421,0 0.4961,-0.22189 0.4961,-0.49609 0,-0.27421 -0.22189,-0.4961 -0.4961,-0.4961 -0.2742,0 -0.49609,0.22189 -0.49609,0.4961 z m 2.30007,1.26278 h -0.018 v -0.37883 h -0.81179 v 2.74204 h 0.84787 v -1.35298 c 0,-0.35719 0.0703,-0.70355 0.51413,-0.70355 0.43657,0 0.44198,0.40409 0.44198,0.72159 v 1.33494 h 0.84787 v -1.50632 c 0,-0.73783 -0.15695,-1.29886 -1.01925,-1.29886 -0.41492,0 -0.68912,0.2273 -0.80277,0.44197 z m -2.21889,2.36321 h 0.85689 v -2.74204 h -0.85689 z"
       id="path1321"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                LinkedIn
            </a>
        </li>
        &nbsp;

        <li>
            <a href="https://github.com/dnth/" target="_blank" rel="noopener" aria-label="Follow on GitHub"
                class="share-btn github">
                <svg width="24" height="24" viewBox="0 0 256 250" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid">
    <g>
        <path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#FFFFFF"></path>
    </g>
</svg>&nbsp;
                GitHub
            </a>
        </li>
        &nbsp;
        <hr>
        <h5>üîÑ Share this post</h5>
        
        
        <li>
            <a href="https://twitter.com/intent/tweet?&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f&amp;text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images @dicksonneoh7"
                target="_blank" rel="noopener" aria-label="Share on Twitter" class="share-btn x">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path fill="#ffffff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
</svg>&nbsp;
                Twitter
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f&amp;source=http%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f&amp;title=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images&amp;summary=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images"
                target="_blank" rel="noopener" aria-label="Share on LinkedIn" class="share-btn linkedin">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="linkedin_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.2264764"
     inkscape:cy="40.603642"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-129.66672,-101.87176)">
    <path
       d="m 129.66672,102.59335 v 4.90682 c 0,0.39507 0.32652,0.72159 0.72159,0.72159 h 4.90682 c 0.39507,0 0.72159,-0.32652 0.72159,-0.72159 v -4.90682 c 0,-0.39507 -0.32652,-0.72159 -0.72159,-0.72159 h -4.90682 c -0.39507,0 -0.72159,0.32652 -0.72159,0.72159 z m 5.62841,-0.14432 c 0.083,0 0.14432,0.0613 0.14432,0.14432 v 4.90682 c 0,0.083 -0.0613,0.14431 -0.14432,0.14431 h -4.90682 c -0.083,0 -0.14432,-0.0613 -0.14432,-0.14431 v -4.90682 c 0,-0.083 0.0613,-0.14432 0.14432,-0.14432 z m -4.55504,0.99219 c 0,0.2742 0.22189,0.49609 0.49609,0.49609 0.27421,0 0.4961,-0.22189 0.4961,-0.49609 0,-0.27421 -0.22189,-0.4961 -0.4961,-0.4961 -0.2742,0 -0.49609,0.22189 -0.49609,0.4961 z m 2.30007,1.26278 h -0.018 v -0.37883 h -0.81179 v 2.74204 h 0.84787 v -1.35298 c 0,-0.35719 0.0703,-0.70355 0.51413,-0.70355 0.43657,0 0.44198,0.40409 0.44198,0.72159 v 1.33494 h 0.84787 v -1.50632 c 0,-0.73783 -0.15695,-1.29886 -1.01925,-1.29886 -0.41492,0 -0.68912,0.2273 -0.80277,0.44197 z m -2.21889,2.36321 h 0.85689 v -2.74204 h -0.85689 z"
       id="path1321"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                LinkedIn
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f" target="_blank" rel="noopener"
                aria-label="Share on Facebook" class="share-btn facebook">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="facebook_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-7.8050197"
     inkscape:cy="32.710925"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-130.36281,-104.14567)">
    <path
       d="m 130.36281,104.72294 v 5.19545 c 0,0.3157 0.26158,0.57728 0.57727,0.57728 h 5.19546 c 0.3157,0 0.57727,-0.26158 0.57727,-0.57728 v -5.19545 c 0,-0.3157 -0.26157,-0.57727 -0.57727,-0.57727 h -5.19546 c -0.31569,0 -0.57727,0.26157 -0.57727,0.57727 z m 5.77273,0 v 5.19545 h -1.4973 v -1.94829 h 0.74865 l 0.10824,-0.86591 h -0.85689 v -0.55923 c 0,-0.25256 0.0631,-0.42394 0.42393,-0.42394 h 0.46904 v -0.78473 c -0.0794,-0.0108 -0.35719,-0.0271 -0.67649,-0.0271 -0.66567,0 -1.11847,0.40048 -1.11847,1.14553 v 0.64943 h -0.75767 v 0.86591 h 0.75767 v 1.94829 h -2.79617 v -5.19545 z"
       id="path1085"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                Facebook
            </a>
        </li>
         &nbsp;

        <br>

        
        
        <li>
            <a href="https://telegram.me/share/url?text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f" target="_blank"
                rel="noopener" aria-label="Share on Telegram" class="share-btn telegram">
                <svg width="7.3503098mm"
   height="7.1592798mm" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473;" xml:space="preserve">
<g>
	<path d="M152.531,179.476c-1.48,0-2.95-0.438-4.211-1.293l-47.641-32.316l-25.552,18.386c-2.004,1.441-4.587,1.804-6.914,0.972
		c-2.324-0.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821,93.928c-2.886-1.104-4.8-3.865-4.821-6.955
		c-0.021-3.09,1.855-5.877,4.727-7.02l174.312-69.36c0.791-0.336,1.628-0.53,2.472-0.582c0.302-0.018,0.605-0.018,0.906-0.001
		c1.748,0.104,3.465,0.816,4.805,2.13c0.139,0.136,0.271,0.275,0.396,0.42c1.11,1.268,1.72,2.814,1.835,4.389
		c0.028,0.396,0.026,0.797-0.009,1.198c-0.024,0.286-0.065,0.571-0.123,0.854L159.898,173.38c-0.473,2.48-2.161,4.556-4.493,5.523
		C154.48,179.287,153.503,179.476,152.531,179.476z M104.862,130.579l42.437,28.785L170.193,39.24l-82.687,79.566l17.156,11.638
		C104.731,130.487,104.797,130.533,104.862,130.579z M69.535,124.178l5.682,21.53l12.242-8.809l-16.03-10.874
		C70.684,125.521,70.046,124.893,69.535,124.178z M28.136,86.782l31.478,12.035c2.255,0.862,3.957,2.758,4.573,5.092l3.992,15.129
		c0.183-1.745,0.974-3.387,2.259-4.624L149.227,38.6L28.136,86.782z"
		id="path1039"
       style="fill:#ffffff;stroke-width:0.0165365"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>
&nbsp;
                Telegram
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="whatsapp://send?text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLeverage%20hundreds%20state-of-the-art%20models%20on%20IceVision%20trained%20with%20best%20practices%20of%20Fastai%0a%0ahttp%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f%0a" target="_blank" aria-label="Share on WhatsApp"
                class="share-btn whatsapp">
                <svg
   width="6.0324998mm"
   height="6.05896mm"
   viewBox="0 0 6.0324997 6.05896"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="whatsapp_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="4.9987205"
     inkscape:cy="35.692618"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-126.67735,-103.17712)">
    <path
       d="m 131.83672,104.07671 c -0.58208,-0.58209 -1.34937,-0.89959 -2.14312,-0.89959 -1.64042,0 -2.98979,1.34938 -2.98979,3.01625 0,0.52917 0.13229,1.03188 0.39687,1.48167 l -0.42333,1.56104 1.5875,-0.42333 c 0.42333,0.23812 0.92604,0.34396 1.42875,0.34396 1.66687,0 3.01625,-1.34938 3.01625,-3.01625 -0.0265,-0.74084 -0.3175,-1.50813 -0.87313,-2.06375 z m -2.14312,4.6302 c -0.44979,0 -0.87313,-0.13229 -1.27,-0.34395 l -0.10583,-0.0529 -0.92605,0.26458 0.26459,-0.89958 -0.0794,-0.13229 c -0.26458,-0.39688 -0.37041,-0.84667 -0.37041,-1.34938 0,-1.37583 1.11125,-2.48708 2.48708,-2.48708 0.66146,0 1.29646,0.26458 1.77271,0.74083 0.47625,0.47625 0.74083,1.11125 0.74083,1.77271 -0.0265,1.37583 -1.13771,2.48708 -2.51354,2.48708 z m 1.34937,-1.87854 c -0.0794,-0.0265 -0.44979,-0.23812 -0.5027,-0.26458 -0.0794,-0.0265 -0.1323,-0.0265 -0.18521,0.0265 -0.0529,0.0794 -0.21167,0.26458 -0.23813,0.29104 -0.0529,0.0529 -0.0794,0.0529 -0.15875,0.0265 -0.0794,-0.0265 -0.3175,-0.13229 -0.60854,-0.37042 -0.23812,-0.21167 -0.37042,-0.44979 -0.42333,-0.52917 -0.0529,-0.0794 0,-0.13229 0.0265,-0.15875 0.0265,-0.0264 0.0794,-0.0794 0.10583,-0.13229 0.0529,-0.0265 0.0794,-0.0794 0.10583,-0.13229 0.0265,-0.0529 0,-0.10583 0,-0.13229 0,-0.0265 -0.18521,-0.39688 -0.26458,-0.55563 -0.0265,-0.10583 -0.10583,-0.0794 -0.13229,-0.0794 h -0.15875 c 0,0 -0.10584,0.0265 -0.18521,0.0794 -0.0794,0.0794 -0.26458,0.26459 -0.26458,0.635 0,0.37042 0.26458,0.74084 0.29104,0.79375 0.0265,0.0529 0.52916,0.82021 1.29646,1.13771 0.1852,0.0794 0.3175,0.13229 0.42333,0.15875 0.18521,0.0529 0.34396,0.0529 0.47625,0.0265 0.15875,-0.0265 0.44979,-0.18521 0.50271,-0.34396 0.0529,-0.18521 0.0529,-0.3175 0.0529,-0.34396 -0.0264,-0.0794 -0.0794,-0.10583 -0.15875,-0.13229 z"
       id="path1793"
       style="stroke-width:0.264583;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                WhatsApp
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images.&amp;body=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLeverage%20hundreds%20state-of-the-art%20models%20on%20IceVision%20trained%20with%20best%20practices%20of%20Fastai%0a%0ahttp%3a%2f%2flocalhost%3a1313%2fportfolio%2ftraining_dl_model_for_cell_counting%2f%0a" target="_blank"
                class="share-btn email" aria-label="Share via Email">
                <svg
   width="6.3499999mm"
   height="4.3961601mm"
   viewBox="0 0 6.3499999 4.3961601"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="email_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.7526575"
     inkscape:cy="33.4125"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-130.10375,-103.97942)">
    <path
       d="m 130.10375,104.22365 v 3.9077 0.24423 h 0.24423 5.86154 0.24423 v -0.24423 -3.9077 -0.24423 h -0.24423 -5.86154 -0.24423 z m 5.29675,0.24423 -2.12175,1.41196 -2.12175,-1.41196 z m -2.25913,1.91569 0.13738,0.0839 0.13738,-0.0839 2.54916,-1.70198 v 3.20553 h -5.37308 v -3.20553 z"
       id="path824"
       style="stroke-width:0.0152644;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                Email
            </a>
        </li>
        <hr>

        
        <section>
            <h5>‚ù§Ô∏è Show some love</h5>
            <p>
                Creating free ML contents doesn't pay my bills. Support me in creating more free contents like these. 
                Consider buying me a coffee. Your support means a lot to me.
            </p>
            <div style="text-align:center">
                <a href="https://www.buymeacoffee.com/dicksonneoh" target="_blank"><img
                        src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" alt="Buy Me A Coffee"
                        style="height: 60px !important;width: 217px !important;"></a>
            </div>
        </section>
        <hr>
    </ul>
</section>

        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-10 offset-lg-1">
        <nav class="case-details-nav d-flex justify-content-between align-items-start">
          
            <div class="previous">
              <div class="d-flex align-items-center mb-3">
                <div class="icon mr-3">
                  <svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285">
                    <g data-name="Group 1243" fill="#2d2d2d">
                      <path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z" />
                      <path data-name="Path 1455" d="M13.137 1.41L3.39 15.558l-.975-1.415L12.166 0z" />
                    </g>
                  </svg>
                </div>
                <span class="small">Prev blog</span>
              </div>
              <div class="blog-nav-item">
                <div class="blog-nav-thumb">
                  <a href="http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/">
                    <img src="http://localhost:1313/images/portfolio/deploy_iceVision_models_on_huggingface_spaces/feature_image_.gif" alt="post-image">
                  </a>
                </div>
                <h5 class="title"><a class="text-dark" href="http://localhost:1313/portfolio/deploy_icevision_models_on_huggingface_spaces/">Deploying Object Detection Models on Hugging Face Spaces</a></h5>
              </div>
            </div>
          
          
            <div class="next">
              <div class="d-flex align-items-center justify-content-end mb-3">
                <span class="small">Next blog</span>
                <div class="icon ml-3">
                  <svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285">
                    <g data-name="Group 1244" fill="#2d2d2d">
                      <path data-name="Path 1456" d="M12.162 12.725L2.416 26.87l.978 1.41 9.746-14.138z" />
                      <path data-name="Path 1455" d="M2.416 1.415l9.743 14.141.975-1.414L3.39 0z" />
                    </g>
                  </svg>
                </div>
              </div>
              <div class="blog-nav-item">
                <div class="blog-nav-thumb">
                  <a href="http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/">
                    <img src="http://localhost:1313/images/portfolio/how_to_deploy_od_models_on_android_with_flutter/thumbnail.gif" alt="post-image">
                  </a>
                </div>
                <h5 class="title"><a class="text-dark" href="http://localhost:1313/portfolio/how_to_deploy_od_models_on_android_with_flutter/">How to Deploy Object Detection Models on Android with Flutter</a></h5>
              </div>
            </div>
          
        </nav>
      </div>
    </div>
  </div>
</section>


  </div>
  <section class="footer" id="contact">
	<div class="footer__background_shape">
		<svg viewBox="0 0 1920 79">
			<path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
		</svg>
	</div>
	<div class="container">
		<div class="row">
			<div class="col-lg-12">
				<div class="footer__cta">
					<div class="shape-1">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="shape-2">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="text-light footer__cta_content">
						<span>Contact me</span>
						<h2 class="mb-0 mb-3">Let‚Äôs Start a Project</h2>
					</div>
					<div class="footer__cta_action">
						
					</div>
					<a href="https://api.whatsapp.com/send?phone=60133250827" rel="noopener" target="_blank" class="btn btn-light btn-zoom mr-3 mb-3 fa fa-whatsapp my-float">&nbsp&nbspChat on WhatsApp</a>
					<a href="https://t.me/dicksonneoh" rel="noopener" target="_blank" class="btn btn-light btn-zoom mr-3 mb-3 fa fa-telegram my-float">&nbsp&nbspChat on Telegram</a>
					<a class="btn btn-light btn-zoom mb-3 fa fa-envelope" href="http://localhost:1313/contact">&nbsp&nbspSend me a message</a>
				</div>
			</div>
		</div>
		<div class="row footer__widget">
			<div class="col-lg-4">
				<div class="footer__widget_logo mb-5">
					<img src="http://localhost:1313/images/site-navigation/logo_dn_resize.png" alt="widget-logo">
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_sitemap mb-5">
					<h4 class="base-font">Sitemap</h4>
					<ul class="unstyle-list small">
						
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/#about">About me</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/">Frequently Ask Question</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/">Privacy &amp; Policy</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/#portfolio">Latest Article</a></li>
						
					</ul>
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_address mb-5">
					<h4 class="base-font">Address</h4>
					
					<ul class="fa-ul small">
						<li class="mb-2"><a class="text-light" href="tel:&#43;%2860%29%203%208921%202020"><span class="fa-li"><i
										class="fa fa-phone"></i></span>&#43;(60) 3 8921 2020</a></li>
						<li class="mb-2"><a class="text-light" href="mailto:dickson.neoh@gmail.com"><span class="fa-li"><i
										class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li>
						<li class="mb-2">
							<span class="fa-li"><i class="fa fa-map-marker"></i></span>Kuala Lumpur, Malaysia.</a>
						</li>
					</ul>
				</div>
			</div>
		</div>
		<div class="row footer__footer">
			<div class="col-lg-6">
				<div class="footer__footer_copy text-light">
					<p>All right reserved copyright ¬© Dickson Neoh 2024</p>
				</div>
			</div>
			<div class="col-lg-6">
				<div class="footer__footer_social">
					<ul class="unstyle-list">
						
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://www.linkedin.com/in/dickson-neoh/"><i
									class="fa fa-linkedin-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://twitter.com/dicksonneoh7"><i
									class="fa fa-twitter-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://github.com/dnth"><i
									class="fa fa-github-square"></i></a>
						</li>
						
					</ul>
				</div>
			</div>
		</div>
	</div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script>
<script src="http://localhost:1313/plugins/jQuery/jquery.min.js"></script>
<script src="http://localhost:1313/plugins/bootstrap/bootstrap.min.js"></script>
<script src="http://localhost:1313/plugins/slick/slick.min.js"></script>
<script src="http://localhost:1313/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="http://localhost:1313/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="http://localhost:1313/plugins/tweenmax/TweenMax.min.js"></script>
<script src="http://localhost:1313/plugins/imagesloaded/imagesloaded.min.js"></script>
<script src="http://localhost:1313/plugins/masonry/masonry.min.js"></script>

<script src="http://localhost:1313/js/form-handler.min.js"></script>

<script src="http://localhost:1313/js/script.min.js"></script>

</body>

</html>