<!DOCTYPE html>
<html>
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <title>Supercharge Your PyTorch Image Models: Bag of Tricks to 123x Faster Inference with ONNX Runtime</title>
  <meta name="description" content="Learn how to accelerate TIMM model inference up to 84x faster using ONNX Runtime and TensorRT optimization techniques!" />

  <meta property="og:url" content="http://localhost:1313/portfolio/supercharge_timm_tensorrt/">
  <meta property="og:site_name" content="Dickson Neoh - Personal Portfolio">
  <meta property="og:title" content="Supercharge Your PyTorch Image Models: Bag of Tricks to 123x Faster Inference with ONNX Runtime">
  <meta property="og:description" content="Learn how to accelerate TIMM model inference up to 84x faster using ONNX Runtime and TensorRT optimization techniques!">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="portfolio">
    <meta property="article:published_time" content="2024-09-09T09:00:00+08:00">
    <meta property="article:modified_time" content="2024-09-09T09:00:00+08:00">
    <meta property="article:tag" content="TIMM">
    <meta property="article:tag" content="ONNX">
    <meta property="article:tag" content="TensorRT">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Inference">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="og:image" content="http://localhost:1313/images/portfolio/supercharge_your_pytorch_image_models/post_image.jpg">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/portfolio/supercharge_your_pytorch_image_models/post_image.jpg">
  <meta name="twitter:title" content="Supercharge Your PyTorch Image Models: Bag of Tricks to 123x Faster Inference with ONNX Runtime">
  <meta name="twitter:description" content="Learn how to accelerate TIMM model inference up to 84x faster using ONNX Runtime and TensorRT optimization techniques!">


  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/slick/slick.css" />
  <link rel="stylesheet" href="http://localhost:1313/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/font-awesome/css/font-awesome.min.css" />

  <!-- Magnific Popup -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="http://localhost:1313/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="http://localhost:1313/images/favicon-purple.ico" type="image/x-icon" />
  <link rel="icon" href="http://localhost:1313/images/favicon.gif" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-54500366-2');
  </script>
  
</head>

<body>
  <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
    <a href="http://localhost:1313/" class="navbar-brand">
      <img src="http://localhost:1313/images/site-navigation/logo_resized.png" alt="site-logo">
    </a>
    <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
      <ul class="nav navbar-nav main-navigation my-0 mx-auto">
        
        
        <li class="nav-item">
          <a href="http://localhost:1313/#home"
            class="nav-link text-dark text-sm-center p-2 ">Home</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#about"
            class="nav-link text-dark text-sm-center p-2 ">About</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#service"
            class="nav-link text-dark text-sm-center p-2 ">Services</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#portfolio"
            class="nav-link text-dark text-sm-center p-2 ">Projects</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#resume"
            class="nav-link text-dark text-sm-center p-2 ">Resume</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#skills"
            class="nav-link text-dark text-sm-center p-2 ">Skills</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#blog"
            class="nav-link text-dark text-sm-center p-2 ">Blogs</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#contact"
            class="nav-link text-dark text-sm-center p-2 ">Contact</a>
        </li>
        
      </ul>
      <div class="navbar-nav">
        <a href="http://localhost:1313/contact" class="btn btn-primary btn-zoom hire_button">Hire Me Now</a>
      </div>
       

    </div>
  </div>
</nav>
  <div id="content">
    

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Supercharge Your PyTorch Image Models: Bag of Tricks to 123x Faster Inference with ONNX Runtime</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          
        </nav>
      </div>
    </div>

    <div class="row p-3">
      <div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center">
        <i class="fa fa-calendar"></i> &ensp;
        September 9, 2024 &ensp; &ensp;
        <i class="fa fa-clock-o"></i> &ensp;
        12 mins read
      </div>
      
        <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
          <i class="fa fa-tag"></i> &ensp;
          
            <a href="http://localhost:1313/tags/timm/">TIMM</a> 
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/tags/onnx/">ONNX</a> 
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/tags/tensorrt/">TensorRT</a> 
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/tags/optimization/">Optimization</a> 
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/tags/inference/">Inference</a> 
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/tags/deep-learning/">Deep-Learning</a> 
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/tags/computer-vision/">Computer-Vision</a> 
            
          
        </div>
      
      
        <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
          <i class="fa fa-folder"></i> &ensp;
          
            <a href="http://localhost:1313/categories/machine-learning/">Machine-Learning</a>
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/categories/deployment/">Deployment</a>
            
              <span class="separator">•</span>
            
          
            <a href="http://localhost:1313/categories/performance/">Performance</a>
            
          
        </div>
      
    </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
    <img src=http://localhost:1313/images/hero/figure-svg.svg alt="">
  </div>
  <div class="animate-shape">
    <img src=http://localhost:1313/images/skill/skill-background-shape.svg alt="">
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600">
      <defs>
        <linearGradient id="d" x1="0.929" y1="0.111" x2="0.263" y2="0.935" gradientUnits="objectBoundingBox">
          <stop offset="0" stop-color="#f1f6f9" />
          <stop offset="1" stop-color="#f1f6f9" stop-opacity="0" />
        </linearGradient>
      </defs>
      <g data-name="blob-shape (3)">
        <path class="blob" fill="url(#d)"
          d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z" />
      </g>
    </svg>
  </div>
  <div class="animate-pattern">
    <img src=http://localhost:1313/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <div class="singleBlog__feature">
          <img src=http://localhost:1313/images/portfolio/supercharge_your_pytorch_image_models/post_image.jpg alt="feature-image">
        </div>
      </div>
    </div>
    <div class="row mt-5">
      <div class="col-lg-12">
        <div class="singleBlog__content">
          
            <h3>Table of Contents</h3>
            <nav id="TableOfContents">
  <ul>
        <li><a class="toc-link" href="#-motivation">🚀 Motivation</a></li>
        <li><a class="toc-link" href="#-installation">💻 Installation</a></li>
        <li><a class="toc-link" href="#-load-and-infer">🔧 Load and Infer</a></li>
        <li><a class="toc-link" href="#-pytorch-latency-benchmark">⏱️ PyTorch Latency Benchmark</a></li>
        <li><a class="toc-link" href="#-convert-to-onnx">🔄 Convert to ONNX</a></li>
        <li><a class="toc-link" href="#-onnx-runtime---cpu">🖥️ ONNX Runtime - CPU</a></li>
        <li><a class="toc-link" href="#-onnx-runtime---gpu">🖼️ ONNX Runtime - GPU</a></li>
        <li><a class="toc-link" href="#-onnx-runtime---tensorrt">📊 ONNX Runtime - TensorRT</a></li>
        <li><a class="toc-link" href="#-turbocharge-by-baking-pre-processing-into-onnx">🧠 Turbocharge by Baking Pre-processing into ONNX</a></li>
        <li><a class="toc-link" href="#-conclusion">🔥 Conclusion</a></li>
      </ul>
</nav>
            <div class="floating-toc">
              <h3>Table of Contents</h3>
              <nav id="TableOfContents">
  <ul>
        <li><a class="toc-link" href="#-motivation">🚀 Motivation</a></li>
        <li><a class="toc-link" href="#-installation">💻 Installation</a></li>
        <li><a class="toc-link" href="#-load-and-infer">🔧 Load and Infer</a></li>
        <li><a class="toc-link" href="#-pytorch-latency-benchmark">⏱️ PyTorch Latency Benchmark</a></li>
        <li><a class="toc-link" href="#-convert-to-onnx">🔄 Convert to ONNX</a></li>
        <li><a class="toc-link" href="#-onnx-runtime---cpu">🖥️ ONNX Runtime - CPU</a></li>
        <li><a class="toc-link" href="#-onnx-runtime---gpu">🖼️ ONNX Runtime - GPU</a></li>
        <li><a class="toc-link" href="#-onnx-runtime---tensorrt">📊 ONNX Runtime - TensorRT</a></li>
        <li><a class="toc-link" href="#-turbocharge-by-baking-pre-processing-into-onnx">🧠 Turbocharge by Baking Pre-processing into ONNX</a></li>
        <li><a class="toc-link" href="#-conclusion">🔥 Conclusion</a></li>
      </ul>
</nav>
            </div>
          
          <hr>
          <h3 id="-motivation">🚀 Motivation</h3>
<p>Real time inference speed is crucial for many applications in production. Some could mean life or death. 💀</p>
<p>Imagine you&rsquo;re behind the wheels of a self-driving car and the car takes 1 second to detect an oncoming truck.</p>
<p>Just one second too late, and you could end up talking to celestial beings&hellip; 👼👼👼</p>
<p>Or if you&rsquo;re lucky, on the ground.</p>
<figure><img src="/portfolio/supercharge_timm_tensorrt/banana_peel_robot.gif" width="480">
</figure>

<p>I hope that shows you how crucial this problem is.</p>
<p>Today (2024), ML models are being deployed in all kinds of high-stakes industries like healthcare, finance, and self-driving cars.</p>
<blockquote class="blockquote">
  <p class="mb-0">It&rsquo;s not just about being right - it&rsquo;s about being right, right now.</p>
</blockquote>
<p>This post shows how you can bring any models from <a href="https://huggingface.co/docs/timm/index" target="_blank" rel="nofollow noopener noreferrer">TIMM</a> and supercharge its inference speed with optimized <a href="https://onnxruntime.ai/" target="_blank" rel="nofollow noopener noreferrer">ONNX Runtime</a> and <a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="nofollow noopener noreferrer">TensorRT</a>.</p>
<style type="text/css">
    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
    svg{top:0.125em;position:relative}</style>
    <div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
            <symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
            </symbol>
            <symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
            </symbol>
            <symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
            </symbol>
            <symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
            </symbol>
        </svg></div><div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>By the end of this post you&rsquo;ll learn how to:</p>
<ul>
<li>📥 Load any pre-trained model from <a href="https://huggingface.co/docs/timm/index" target="_blank" rel="nofollow noopener noreferrer">TIMM</a></li>
<li>🔄 Convert the model to ONNX format</li>
<li>🖥️ Run inference with ONNX Runtime (CPU &amp; GPU)</li>
<li>🎮 Run inference with TensorRT (GPU)</li>
<li>🛠️ Tweak the TensorRT parameters for better performance</li>
<li>🧠 Bake the pre-processing into the ONNX model</li>
</ul>
<p>You can find the code for this post on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div>
<p>If that sounds exciting let&rsquo;s dive in! 🏊‍♂️</p>
<h3 id="-installation">💻 Installation</h3>
<p>I will be using the conda environment to install the packages. Feel free to use any other environment of your choice.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>conda create -n supercharge_timm_tensorrt python<span style="color:#f92672">=</span>3.11
</span></span><span style="display:flex;"><span>conda activate supercharge_timm_tensorrt
</span></span></code></pre></div><p>In this post I will be using the <a href="https://github.com/huggingface/pytorch-image-models" target="_blank" rel="nofollow noopener noreferrer"><code>timm</code></a> library to load a pre-trained model and run inference. So let&rsquo;s install <code>timm</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install timm
</span></span></code></pre></div><p>If you&rsquo;re not familiar, <code>timm</code> is a library that provides thousands of pre-trained models that&rsquo;s being used in research and production.
If you&rsquo;ve used a PyTorch vision model, chances are it&rsquo;s using a model from <code>timm</code>.</p>
<h3 id="-load-and-infer">🔧 Load and Infer</h3>
<p>Let&rsquo;s load one of the top performing models from the timm <a href="https://huggingface.co/spaces/timm/leaderboard" target="_blank" rel="nofollow noopener noreferrer">leaderboard</a> - the <code>eva02_large_patch14_448.mim_m38m_ft_in22k_in1k</code> model.</p>
<p>This model boasts impressive ImageNet accuracy scores of <strong>90.05%</strong> for top-1 and <strong>99.06%</strong> for top-5 classifications.</p>
<!-- <iframe
	src="https://timm-leaderboard.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe> -->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> timm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k&#39;</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> timm<span style="color:#f92672">.</span>create_model(model_name, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>eval()
</span></span></code></pre></div><p>Next, we need to get the data config and transformations for the model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data_config <span style="color:#f92672">=</span> timm<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>resolve_model_data_config(model)
</span></span><span style="display:flex;"><span>transforms <span style="color:#f92672">=</span> timm<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>create_transform(<span style="color:#f92672">**</span>data_config, is_training<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p>With the model and transformations ready, let&rsquo;s run inference to get the top 5 predictions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> urllib.request <span style="color:#f92672">import</span> urlopen
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>img <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(urlopen(<span style="color:#e6db74">&#39;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>inference_mode():
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> model(transforms(img)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>top5_probabilities, top5_class_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(output<span style="color:#f92672">.</span>softmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span>, k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><p>Get the top 5 predictions and print them.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> imagenet_classes <span style="color:#f92672">import</span> IMAGENET2012_CLASSES
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>im_classes <span style="color:#f92672">=</span> list(IMAGENET2012_CLASSES<span style="color:#f92672">.</span>values())
</span></span><span style="display:flex;"><span>class_names <span style="color:#f92672">=</span> [im_classes[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> top5_class_indices[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, prob <span style="color:#f92672">in</span> zip(class_names, top5_probabilities[<span style="color:#ae81ff">0</span>]):
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>prob<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#34;</span>)
</span></span></code></pre></div>




<figure style="text-align: center;">
    
    <a href="/portfolio/supercharge_timm_tensorrt/beignets-task-guide.png" class="image-popup">
        <img src="/portfolio/supercharge_timm_tensorrt/beignets-task-guide.png" 
             srcset="/portfolio/supercharge_timm_tensorrt/beignets-task-guide_hu8348793543510315561.png 360w, /portfolio/supercharge_timm_tensorrt/beignets-task-guide_hu10579896065997683559.png 720w, /portfolio/supercharge_timm_tensorrt/beignets-task-guide_hu153020910606401730.png 1920w" 
             sizes="(max-width: 400) 100vw, 400"
             
             width="400" 
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


<p>Top 5 predictions:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; espresso: 26.78%
&gt;&gt;&gt; eggnog: 2.88%
&gt;&gt;&gt; cup: 2.60%
&gt;&gt;&gt; chocolate sauce, chocolate syrup: 2.39%
&gt;&gt;&gt; bakery, bakeshop, bakehouse: 1.48%
</code></pre><p>The predictions looks good! Now let&rsquo;s benchmark the model inference latency.</p>
<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>You can find the code for this section on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensorrt/blob/main/00_benchmark_timm.py" target="_blank" rel="nofollow noopener noreferrer">here</a>. </p></div>
<h3 id="-pytorch-latency-benchmark">⏱️ PyTorch Latency Benchmark</h3>
<p>We will run the inference 10 times and record the average time on both CPU and GPU.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_benchmark</span>(model, device, num_images<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>inference_mode():
</span></span><span style="display:flex;"><span>        start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_images):
</span></span><span style="display:flex;"><span>            input_tensor <span style="color:#f92672">=</span> transforms(img)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            model(input_tensor)
</span></span><span style="display:flex;"><span>        end <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    ms_per_image <span style="color:#f92672">=</span> (end <span style="color:#f92672">-</span> start) <span style="color:#f92672">/</span> num_images <span style="color:#f92672">*</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>    fps <span style="color:#f92672">=</span> num_images <span style="color:#f92672">/</span> (end <span style="color:#f92672">-</span> start)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;PyTorch model on </span><span style="color:#e6db74">{</span>device<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>ms_per_image<span style="color:#e6db74">:</span><span style="color:#e6db74">.3f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms per image, FPS: </span><span style="color:#e6db74">{</span>fps<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># CPU Benchmark</span>
</span></span><span style="display:flex;"><span>run_benchmark(model, torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cpu&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPU Benchmark </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available():
</span></span><span style="display:flex;"><span>    run_benchmark(model, torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span>))
</span></span></code></pre></div><p>Alright the benchmarks are in</p>
<pre tabindex="0"><code>&gt;&gt;&gt; PyTorch model on cpu: 1584.379 ms per image, FPS: 0.63
&gt;&gt;&gt; PyTorch model on cuda: 77.226 ms per image, FPS: 12.95
</code></pre><div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>I&rsquo;m using the following hardware for the benchmarks:</p>
<ul>
<li>GPU: NVIDIA RTX 3090</li>
<li>CPU: 11th Gen Intel® Core™ i9-11900 @ 2.50GHz × 16</li>
</ul>
<p>You can find the code for the PyTorch benchmarks on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort/blob/main/01_pytorch_latency_benchmark.py" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div>
<p>Although the performance on the GPU is not bad, 12 FPS is still not fast enough for real-time inference.
Let&rsquo;s forget about using the model on a CPU for inference. Remember a one second too late could mean a lot!</p>
<p>But we can do better.</p>
<h3 id="-convert-to-onnx">🔄 Convert to ONNX</h3>
<p>ONNX is an open and interoperable format for deep learning models. It lets us deploy models across different frameworks and devices.</p>
<p>As a bonus, ONNX Runtime can optimize the model for faster inference. Before we can use ONNX Runtime to run inference, we need to convert the model to ONNX format.</p>
<p>So let&rsquo;s first install <code>onnx</code> and run the conversion.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install onnx
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> timm
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> timm<span style="color:#f92672">.</span>create_model(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k&#34;</span>, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>onnx_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(
</span></span><span style="display:flex;"><span>    model,
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">448</span>, <span style="color:#ae81ff">448</span>),
</span></span><span style="display:flex;"><span>    onnx_filename,
</span></span><span style="display:flex;"><span>    export_params<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,
</span></span><span style="display:flex;"><span>    do_constant_folding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    input_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;input&#34;</span>],
</span></span><span style="display:flex;"><span>    output_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;output&#34;</span>],
</span></span><span style="display:flex;"><span>    dynamic_axes<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;input&#34;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#34;batch_size&#34;</span>}, 
</span></span><span style="display:flex;"><span>                  <span style="color:#e6db74">&#34;output&#34;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#34;batch_size&#34;</span>}},
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>You will end up with a file called <code>eva02_large_patch14_448.onnx</code> in your working directory.</p>
<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>Parameters:</p>
<ul>
<li><code>model</code>: The pre-trained model to be exported.</li>
<li><code>torch.randn(1, 3, 448, 448)</code>: A dummy input tensor with the appropriate shape.</li>
<li><code>&quot;eva02_large_patch14_448.onnx&quot;</code>: The name of the output ONNX file.</li>
<li><code>export_params=True</code>: Whether to export the model parameters.</li>
<li><code>opset_version=18</code>: The ONNX operator set version to use.</li>
<li><code>do_constant_folding=True</code>: Whether to do constant folding for optimization.</li>
<li><code>input_names=['input']</code>: The name of the input tensor.</li>
<li><code>output_names=['output']</code>: The name of the output tensor.</li>
<li><code>dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}</code>: Dynamic axes for the input and output tensors.</li>
</ul>
<p>You can find the code for the ONNX conversion on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort/blob/main/03_convert_to_onnx.py" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div>
<h3 id="-onnx-runtime---cpu">🖥️ ONNX Runtime - CPU</h3>
<p>Now that we have the ONNX model, let&rsquo;s run inference with ONNX Runtime on the CPU.</p>
<p>If you haven&rsquo;t installed <code>onnxruntime</code>, do so now.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install onnxruntime onnxruntime-gpu
</span></span></code></pre></div><p>Now that we have the ONNX model and the ONNX Runtime installed, let&rsquo;s run inference with ONNX Runtime on the CPU.</p>
<p>First, let&rsquo;s replicate the transforms from the PyTorch model using numpy.
If you print the transforms used in the PyTorch model, you can see that it&rsquo;s a sequence of transformations that converts the image to the appropriate shape and normalization for the model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(transforms)
</span></span></code></pre></div><pre tabindex="0"><code>&gt;&gt;&gt; Compose(
&gt;&gt;&gt;     Resize(size=(448, 448), interpolation=bicubic, max_size=None, antialias=True)
&gt;&gt;&gt;     CenterCrop(size=(448, 448))
&gt;&gt;&gt;     MaybeToTensor()
&gt;&gt;&gt;     Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
&gt;&gt;&gt; )
</code></pre><p>The equivalent transforms in numpy is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transforms_numpy</span>(image: Image<span style="color:#f92672">.</span>Image):
</span></span><span style="display:flex;"><span>    image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>convert(<span style="color:#e6db74">&#39;RGB&#39;</span>)
</span></span><span style="display:flex;"><span>    image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>resize((<span style="color:#ae81ff">448</span>, <span style="color:#ae81ff">448</span>), Image<span style="color:#f92672">.</span>BICUBIC)
</span></span><span style="display:flex;"><span>    img_numpy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(image)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>    img_numpy <span style="color:#f92672">=</span> img_numpy<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.485</span>, <span style="color:#ae81ff">0.456</span>, <span style="color:#ae81ff">0.406</span>])<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    std <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.229</span>, <span style="color:#ae81ff">0.224</span>, <span style="color:#ae81ff">0.225</span>])<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    img_numpy <span style="color:#f92672">=</span> (img_numpy <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> std
</span></span><span style="display:flex;"><span>    img_numpy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>expand_dims(img_numpy, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    img_numpy <span style="color:#f92672">=</span> img_numpy<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> img_numpy
</span></span></code></pre></div><p>Now let&rsquo;s run inference with ONNX Runtime.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> onnxruntime <span style="color:#66d9ef">as</span> ort
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create ONNX Runtime session with CPU provider</span>
</span></span><span style="display:flex;"><span>onnx_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style="display:flex;"><span>session <span style="color:#f92672">=</span> ort<span style="color:#f92672">.</span>InferenceSession(
</span></span><span style="display:flex;"><span>    onnx_filename, 
</span></span><span style="display:flex;"><span>    providers<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;CPUExecutionProvider&#34;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get input and output names</span>
</span></span><span style="display:flex;"><span>input_name <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>get_inputs()[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>name
</span></span><span style="display:flex;"><span>output_name <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>get_outputs()[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>name
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run inference</span>
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>run(
</span></span><span style="display:flex;"><span>    [output_name], 
</span></span><span style="display:flex;"><span>    {input_name: transforms_numpy(img)}
</span></span><span style="display:flex;"><span>)[<span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><p>If we inspect the output shape, we can see that it&rsquo;s the same as the number of classes in the ImageNet dataset.</p>
<pre tabindex="0"><code>output.shape
&gt;&gt;&gt; (1, 1000)
</code></pre><p>And the results are</p>
<pre tabindex="0"><code>&gt;&gt;&gt; espresso: 28.65%
&gt;&gt;&gt; cup: 2.77%
&gt;&gt;&gt; eggnog: 2.28%
&gt;&gt;&gt; chocolate sauce, chocolate syrup: 2.13%
&gt;&gt;&gt; bakery, bakeshop, bakehouse: 1.42%
</code></pre><p>While the results aren&rsquo;t an exact match to the PyTorch model, they&rsquo;re sufficiently similar. This slight variation can be attributed to differences in how normalization is implemented, leading to minor discrepancies in the precise values.</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>One of the benefits of using ONNX Runtime is we can get rid of the PyTorch dependency - which is a pain to install on some systems. Plus it&rsquo;s a huge dependency to have in your project. </p></div>
<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>You can find the code for the ONNX Runtime CPU inference on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort/blob/main/04_onnx_cpu_inference.py" target="_blank" rel="nofollow noopener noreferrer">here</a>. </p></div>
<p>Now let&rsquo;s repeat the CPU inference benchmark but using ONNX Runtime.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>num_images <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_images):
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>run([output_name], {input_name: transforms_numpy(img)})[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>end <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>time_taken <span style="color:#f92672">=</span> end <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ms_per_image <span style="color:#f92672">=</span> time_taken <span style="color:#f92672">/</span> num_images <span style="color:#f92672">*</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>fps <span style="color:#f92672">=</span> num_images <span style="color:#f92672">/</span> time_taken
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Onnxruntime CPU: </span><span style="color:#e6db74">{</span>ms_per_image<span style="color:#e6db74">:</span><span style="color:#e6db74">.3f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms per image, FPS: </span><span style="color:#e6db74">{</span>fps<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre tabindex="0"><code>&gt;&gt;&gt; Onnxruntime CPU: 2002.446 ms per image, FPS: 0.50
</code></pre><p>Ouch! That&rsquo;s slower than the PyTorch model. What a bummer!
We have to do better on the GPU. Let&rsquo;s try ONNX Runtime on the GPU.</p>
<h3 id="-onnx-runtime---gpu">🖼️ ONNX Runtime - GPU</h3>
<p>ONNX Runtime offers other backends for inference. We can easily swap to a different backend by changing the provider.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>providers <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;CUDAExecutionProvider&#39;</span>]
</span></span><span style="display:flex;"><span>onnx_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style="display:flex;"><span>session <span style="color:#f92672">=</span> ort<span style="color:#f92672">.</span>InferenceSession(onnx_filename, providers<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;CUDAExecutionProvider&#34;</span>])
</span></span></code></pre></div><p>The rest of the code is the same as the CPU inference.</p>
<p>Just with that change, the benchmarks are as follows:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; Onnxruntime CUDA numpy transforms: 56.430 ms per image, FPS: 17.72
</code></pre><p>Theres is one more trick we can use to squeeze out more performance - using <a href="https://cupy.dev/" target="_blank" rel="nofollow noopener noreferrer">CuPy</a> for the transforms instead of numpy.</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>CuPy is a library that lets us run NumPy code on the GPU. It&rsquo;s a drop-in replacement for NumPy, so you can just replace <code>numpy</code> with <code>cupy</code> in your code and it will run on the GPU. </p></div>
<p>To use CuPy, we need to install it first.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install cupy-cuda12x
</span></span></code></pre></div><p>And we can use it to run the transforms.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transforms_cupy</span>(image: Image<span style="color:#f92672">.</span>Image):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Convert image to RGB and resize</span>
</span></span><span style="display:flex;"><span>    image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>convert(<span style="color:#e6db74">&#34;RGB&#34;</span>)
</span></span><span style="display:flex;"><span>    image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>resize((<span style="color:#ae81ff">448</span>, <span style="color:#ae81ff">448</span>), Image<span style="color:#f92672">.</span>BICUBIC)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Convert to CuPy array and normalize</span>
</span></span><span style="display:flex;"><span>    img_cupy <span style="color:#f92672">=</span> cp<span style="color:#f92672">.</span>array(image, dtype<span style="color:#f92672">=</span>cp<span style="color:#f92672">.</span>float32) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>    img_cupy <span style="color:#f92672">=</span> img_cupy<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Apply mean and std normalization</span>
</span></span><span style="display:flex;"><span>    mean <span style="color:#f92672">=</span> cp<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.485</span>, <span style="color:#ae81ff">0.456</span>, <span style="color:#ae81ff">0.406</span>], dtype<span style="color:#f92672">=</span>cp<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    std <span style="color:#f92672">=</span> cp<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.229</span>, <span style="color:#ae81ff">0.224</span>, <span style="color:#ae81ff">0.225</span>], dtype<span style="color:#f92672">=</span>cp<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    img_cupy <span style="color:#f92672">=</span> (img_cupy <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> std
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Add batch dimension</span>
</span></span><span style="display:flex;"><span>    img_cupy <span style="color:#f92672">=</span> cp<span style="color:#f92672">.</span>expand_dims(img_cupy, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> img_cupy
</span></span></code></pre></div><p>With CuPy, we got a tiny bit of performance improvement:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; Onnxruntime CUDA cupy transforms: 54.267 ms per image, FPS: 18.43
</code></pre><div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>You can find the code for the ONNX Runtime CUDA cupy inference on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort/blob/main/05_onnx_cuda_inference.py" target="_blank" rel="nofollow noopener noreferrer">here</a>. </p></div>
<p>Using Onnx Runtime with CUDA is a little better than the PyTorch model on the GPU, but still not fast enough for real-time inference.</p>
<p>We have one more trick up our sleeve.</p>
<h3 id="-onnx-runtime---tensorrt">📊 ONNX Runtime - TensorRT</h3>
<p>The TensorRT EP is a specialized provider for TensorRT. It lets us run the model with TensorRT optimizations.</p>
<p>Add in TensorRT parameters for final performance gains.</p>
<p>Building on from the previous example, we can add in TensorRT parameters for final performance gains.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>providers <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    (
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;TensorrtExecutionProvider&#34;</span>,
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;device_id&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_max_workspace_size&#34;</span>: <span style="color:#ae81ff">8589934592</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_fp16_enable&#34;</span>: <span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_engine_cache_enable&#34;</span>: <span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_engine_cache_path&#34;</span>: <span style="color:#e6db74">&#34;./trt_cache&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_force_sequential_engine_build&#34;</span>: <span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_max_partition_iterations&#34;</span>: <span style="color:#ae81ff">10000</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_min_subgraph_size&#34;</span>: <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_builder_optimization_level&#34;</span>: <span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;trt_timing_cache_enable&#34;</span>: <span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>onnx_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style="display:flex;"><span>session <span style="color:#f92672">=</span> ort<span style="color:#f92672">.</span>InferenceSession(onnx_filename, providers<span style="color:#f92672">=</span>providers)
</span></span></code></pre></div><p>And running the benchmark:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; Onnxruntime CUDA numpy transforms: 19.898 ms per image, FPS: 50.26
&gt;&gt;&gt; Onnxruntime CUDA cupy transforms: 16.836 ms per image, FPS: 59.40
</code></pre><p>That&rsquo;s a 4x speedup over the PyTorch model on the GPU and 84x speedup over the PyTorch model on the CPU!</p>
<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>You can find the code for the ONNX Runtime TensorRT inference on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort/blob/main/06_onnx_trt_inference.py" target="_blank" rel="nofollow noopener noreferrer">here</a>. </p></div>
<p>That&rsquo;s the end of this post. Or is it?</p>
<p>Not yet. You could stop here and be happy with the results. After all we already got a 84x speedup over the PyTorch model.</p>
<p>But.. if you&rsquo;re like me and you want to squeeze out every last bit of performance, there&rsquo;s one final trick up our sleeve.</p>
<h3 id="-turbocharge-by-baking-pre-processing-into-onnx">🧠 Turbocharge by Baking Pre-processing into ONNX</h3>
<p>If you recall, we did our pre-processing transforms outside of the ONNX model in CuPy or Numpy.</p>
<p>This incurs some overhead because we need to transfer the data to and from the GPU for the transforms.</p>
<p>We can avoid this overhead by baking the transforms operations into the ONNX model. This lets us run the inference faster because we don&rsquo;t need to do the transforms separately.</p>
<p>To do this we need to write some custom code to convert the transforms to an ONNX model.
If you recall, the numpy transforms we used earlier uses the resize and normalization operations. These operations are supported in ONNX and we can add them to the model.</p>
<p>To do this we need to write the preprocessing code as a PyTorch model and export it to ONNX.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Preprocess</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, input_shape: List[int]):
</span></span><span style="display:flex;"><span>        super(Preprocess, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_shape <span style="color:#f92672">=</span> tuple(input_shape)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mean <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0.485</span>, <span style="color:#ae81ff">0.456</span>, <span style="color:#ae81ff">0.406</span>])<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>std <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0.229</span>, <span style="color:#ae81ff">0.224</span>, <span style="color:#ae81ff">0.225</span>])<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Resize the image to the input shape</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>interpolate(input<span style="color:#f92672">=</span>x, 
</span></span><span style="display:flex;"><span>                                           size<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>input_shape[<span style="color:#ae81ff">2</span>:], 
</span></span><span style="display:flex;"><span>                                           mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bicubic&#39;</span>, 
</span></span><span style="display:flex;"><span>                                           align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Normalize the image</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>mean) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>std
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>And now export the <code>Preprocess</code> module to ONNX.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>input_shape <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">448</span>, <span style="color:#ae81ff">448</span>]
</span></span><span style="display:flex;"><span>output_onnx_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;preprocessing.onnx&#34;</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Preprocess(input_shape<span style="color:#f92672">=</span>input_shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(
</span></span><span style="display:flex;"><span>        model,
</span></span><span style="display:flex;"><span>        torch<span style="color:#f92672">.</span>randn(input_shape),
</span></span><span style="display:flex;"><span>        output_onnx_file,
</span></span><span style="display:flex;"><span>        opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,
</span></span><span style="display:flex;"><span>        input_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;input_rgb&#34;</span>],
</span></span><span style="display:flex;"><span>        output_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;output_prep&#34;</span>],
</span></span><span style="display:flex;"><span>        dynamic_axes<span style="color:#f92672">=</span>{
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;input_rgb&#34;</span>: {
</span></span><span style="display:flex;"><span>                <span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#34;batch_size&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#ae81ff">2</span>: <span style="color:#e6db74">&#34;height&#34;</span>,
</span></span><span style="display:flex;"><span>                <span style="color:#ae81ff">3</span>: <span style="color:#e6db74">&#34;width&#34;</span>,
</span></span><span style="display:flex;"><span>            },
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>Let&rsquo;s visualize the preprocess model on Netron.</p>
<p>




<figure style="text-align: center;">
    
    <a href="/portfolio/supercharge_timm_tensorrt/preprocess_model.png" class="image-popup">
        <img src="/portfolio/supercharge_timm_tensorrt/preprocess_model.png" 
             srcset="/portfolio/supercharge_timm_tensorrt/preprocess_model_hu4607733054267023607.png 360w, /portfolio/supercharge_timm_tensorrt/preprocess_model_hu9920966677818134940.png 720w, /portfolio/supercharge_timm_tensorrt/preprocess_model_hu12152896841002843523.png 1920w" 
             sizes="(max-width: auto) 100vw, auto"
             
             width="auto" 
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


Looks like the input and output shapes are correct.</p>
<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>You can find the code for the export of the preprocessing model on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort/blob/main/07_export_preprocessing_onnx.py" target="_blank" rel="nofollow noopener noreferrer">here</a>. </p></div>
<p>And let&rsquo;s visualize the original model on Netron.</p>





<figure style="text-align: center;">
    
    <a href="/portfolio/supercharge_timm_tensorrt/original_model.png" class="image-popup">
        <img src="/portfolio/supercharge_timm_tensorrt/original_model.png" 
             srcset="/portfolio/supercharge_timm_tensorrt/original_model_hu3260121330379215308.png 360w, /portfolio/supercharge_timm_tensorrt/original_model_hu3446264274603139553.png 720w, /portfolio/supercharge_timm_tensorrt/original_model_hu4600744048803685800.png 1920w" 
             sizes="(max-width: auto) 100vw, auto"
             
             width="auto" 
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


<p>Now we need to merge the preprocess model with the original model.
Note the name of the input and output nodes from Netron. We will need this for the merge.</p>
<p>To merge the models,</p>
<p>Let&rsquo;s visualize the merged model on Netron.</p>





<figure style="text-align: center;">
    
    <a href="/portfolio/supercharge_timm_tensorrt/merged_model.png" class="image-popup">
        <img src="/portfolio/supercharge_timm_tensorrt/merged_model.png" 
             srcset="/portfolio/supercharge_timm_tensorrt/merged_model_hu16587760453168987918.png 360w, /portfolio/supercharge_timm_tensorrt/merged_model_hu5901211062648267693.png 720w, /portfolio/supercharge_timm_tensorrt/merged_model_hu5831606665720037686.png 1920w" 
             sizes="(max-width: auto) 100vw, auto"
             
             width="auto" 
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>Note the input to the merged model is <code>[batch_size, 3, height, width]</code>. This model can be given any input of size height x width and the batch size can vary. As we&rsquo;ve seen in the Preprocess module earlier, the height and width are resized to 448x448 internally. </p></div>
<p>And the results are in!</p>
<pre tabindex="0"><code>TensorRT with pre-processing: 12.875 ms per image, FPS: 77.67
</code></pre><p>Let&rsquo;s do a final sanity check on the predictions.</p>
<pre tabindex="0"><code>&gt;&gt;&gt; espresso: 34.25%
&gt;&gt;&gt; cup: 2.06%
&gt;&gt;&gt; chocolate sauce, chocolate syrup: 1.31%
&gt;&gt;&gt; bakery, bakeshop, bakehouse: 0.97%
&gt;&gt;&gt; coffee mug: 0.85%
</code></pre><p>Looks like the predictions are close to the original model. We can sign off and say that the model is working as expected.</p>
<h3 id="-conclusion">🔥 Conclusion</h3>
<p>In this post we have seen how we can supercharge our TIMM models for faster inference using ONNX Runtime and TensorRT.</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>In this post you&rsquo;ve learned how to:</p>
<ul>
<li>📥 Load any pre-trained model from <a href="https://huggingface.co/docs/timm/index" target="_blank" rel="nofollow noopener noreferrer">TIMM</a></li>
<li>🔄 Convert the model to ONNX format</li>
<li>🖥️ Run inference with ONNX Runtime (CPU &amp; GPU)</li>
<li>🎮 Run inference with TensorRT (GPU)</li>
<li>🛠️ Tweak the TensorRT parameters for better performance</li>
<li>🧠 Bake the pre-processing into the ONNX model</li>
</ul>
<p>You can find the code for this post on my GitHub repository <a href="https://github.com/dnth/timm_onnx_tensort" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div>

          





<section class="social-share">

    <ul class="share-icons">
        <hr>

        <h5>🤟 Follow me</h5>

        <p>
            Don't want to miss any of my future content? Follow me on Twitter and LinkedIn where I share these tips in
            bite-size posts.
        </p>

        
        <li>
            <a href="https://twitter.com/dicksonneoh7" target="_blank" rel="noopener" aria-label="Follow on Twitter"
                class="share-btn x">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path fill="#ffffff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
</svg>&nbsp;
                Twitter
            </a>
        </li>
        &nbsp;

        <li>
            <a href="https://www.linkedin.com/in/dickson-neoh/" target="_blank" rel="noopener"
                aria-label="Follow on LinkedIn" class="share-btn linkedin">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="linkedin_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.2264764"
     inkscape:cy="40.603642"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-129.66672,-101.87176)">
    <path
       d="m 129.66672,102.59335 v 4.90682 c 0,0.39507 0.32652,0.72159 0.72159,0.72159 h 4.90682 c 0.39507,0 0.72159,-0.32652 0.72159,-0.72159 v -4.90682 c 0,-0.39507 -0.32652,-0.72159 -0.72159,-0.72159 h -4.90682 c -0.39507,0 -0.72159,0.32652 -0.72159,0.72159 z m 5.62841,-0.14432 c 0.083,0 0.14432,0.0613 0.14432,0.14432 v 4.90682 c 0,0.083 -0.0613,0.14431 -0.14432,0.14431 h -4.90682 c -0.083,0 -0.14432,-0.0613 -0.14432,-0.14431 v -4.90682 c 0,-0.083 0.0613,-0.14432 0.14432,-0.14432 z m -4.55504,0.99219 c 0,0.2742 0.22189,0.49609 0.49609,0.49609 0.27421,0 0.4961,-0.22189 0.4961,-0.49609 0,-0.27421 -0.22189,-0.4961 -0.4961,-0.4961 -0.2742,0 -0.49609,0.22189 -0.49609,0.4961 z m 2.30007,1.26278 h -0.018 v -0.37883 h -0.81179 v 2.74204 h 0.84787 v -1.35298 c 0,-0.35719 0.0703,-0.70355 0.51413,-0.70355 0.43657,0 0.44198,0.40409 0.44198,0.72159 v 1.33494 h 0.84787 v -1.50632 c 0,-0.73783 -0.15695,-1.29886 -1.01925,-1.29886 -0.41492,0 -0.68912,0.2273 -0.80277,0.44197 z m -2.21889,2.36321 h 0.85689 v -2.74204 h -0.85689 z"
       id="path1321"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                LinkedIn
            </a>
        </li>
        &nbsp;

        <li>
            <a href="https://github.com/dnth/" target="_blank" rel="noopener" aria-label="Follow on GitHub"
                class="share-btn github">
                <svg width="24" height="24" viewBox="0 0 256 250" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid">
    <g>
        <path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#FFFFFF"></path>
    </g>
</svg>&nbsp;
                GitHub
            </a>
        </li>
        &nbsp;
        <hr>
        <h5>🔄 Share this post</h5>
        
        
        <li>
            <a href="https://twitter.com/intent/tweet?&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f&amp;text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime @dicksonneoh7"
                target="_blank" rel="noopener" aria-label="Share on Twitter" class="share-btn x">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path fill="#ffffff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
</svg>&nbsp;
                Twitter
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f&amp;source=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f&amp;title=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime&amp;summary=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime"
                target="_blank" rel="noopener" aria-label="Share on LinkedIn" class="share-btn linkedin">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="linkedin_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.2264764"
     inkscape:cy="40.603642"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-129.66672,-101.87176)">
    <path
       d="m 129.66672,102.59335 v 4.90682 c 0,0.39507 0.32652,0.72159 0.72159,0.72159 h 4.90682 c 0.39507,0 0.72159,-0.32652 0.72159,-0.72159 v -4.90682 c 0,-0.39507 -0.32652,-0.72159 -0.72159,-0.72159 h -4.90682 c -0.39507,0 -0.72159,0.32652 -0.72159,0.72159 z m 5.62841,-0.14432 c 0.083,0 0.14432,0.0613 0.14432,0.14432 v 4.90682 c 0,0.083 -0.0613,0.14431 -0.14432,0.14431 h -4.90682 c -0.083,0 -0.14432,-0.0613 -0.14432,-0.14431 v -4.90682 c 0,-0.083 0.0613,-0.14432 0.14432,-0.14432 z m -4.55504,0.99219 c 0,0.2742 0.22189,0.49609 0.49609,0.49609 0.27421,0 0.4961,-0.22189 0.4961,-0.49609 0,-0.27421 -0.22189,-0.4961 -0.4961,-0.4961 -0.2742,0 -0.49609,0.22189 -0.49609,0.4961 z m 2.30007,1.26278 h -0.018 v -0.37883 h -0.81179 v 2.74204 h 0.84787 v -1.35298 c 0,-0.35719 0.0703,-0.70355 0.51413,-0.70355 0.43657,0 0.44198,0.40409 0.44198,0.72159 v 1.33494 h 0.84787 v -1.50632 c 0,-0.73783 -0.15695,-1.29886 -1.01925,-1.29886 -0.41492,0 -0.68912,0.2273 -0.80277,0.44197 z m -2.21889,2.36321 h 0.85689 v -2.74204 h -0.85689 z"
       id="path1321"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                LinkedIn
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f" target="_blank" rel="noopener"
                aria-label="Share on Facebook" class="share-btn facebook">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="facebook_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-7.8050197"
     inkscape:cy="32.710925"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-130.36281,-104.14567)">
    <path
       d="m 130.36281,104.72294 v 5.19545 c 0,0.3157 0.26158,0.57728 0.57727,0.57728 h 5.19546 c 0.3157,0 0.57727,-0.26158 0.57727,-0.57728 v -5.19545 c 0,-0.3157 -0.26157,-0.57727 -0.57727,-0.57727 h -5.19546 c -0.31569,0 -0.57727,0.26157 -0.57727,0.57727 z m 5.77273,0 v 5.19545 h -1.4973 v -1.94829 h 0.74865 l 0.10824,-0.86591 h -0.85689 v -0.55923 c 0,-0.25256 0.0631,-0.42394 0.42393,-0.42394 h 0.46904 v -0.78473 c -0.0794,-0.0108 -0.35719,-0.0271 -0.67649,-0.0271 -0.66567,0 -1.11847,0.40048 -1.11847,1.14553 v 0.64943 h -0.75767 v 0.86591 h 0.75767 v 1.94829 h -2.79617 v -5.19545 z"
       id="path1085"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                Facebook
            </a>
        </li>
         &nbsp;

        <br>

        
        
        <li>
            <a href="https://telegram.me/share/url?text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f" target="_blank"
                rel="noopener" aria-label="Share on Telegram" class="share-btn telegram">
                <svg width="7.3503098mm"
   height="7.1592798mm" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473;" xml:space="preserve">
<g>
	<path d="M152.531,179.476c-1.48,0-2.95-0.438-4.211-1.293l-47.641-32.316l-25.552,18.386c-2.004,1.441-4.587,1.804-6.914,0.972
		c-2.324-0.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821,93.928c-2.886-1.104-4.8-3.865-4.821-6.955
		c-0.021-3.09,1.855-5.877,4.727-7.02l174.312-69.36c0.791-0.336,1.628-0.53,2.472-0.582c0.302-0.018,0.605-0.018,0.906-0.001
		c1.748,0.104,3.465,0.816,4.805,2.13c0.139,0.136,0.271,0.275,0.396,0.42c1.11,1.268,1.72,2.814,1.835,4.389
		c0.028,0.396,0.026,0.797-0.009,1.198c-0.024,0.286-0.065,0.571-0.123,0.854L159.898,173.38c-0.473,2.48-2.161,4.556-4.493,5.523
		C154.48,179.287,153.503,179.476,152.531,179.476z M104.862,130.579l42.437,28.785L170.193,39.24l-82.687,79.566l17.156,11.638
		C104.731,130.487,104.797,130.533,104.862,130.579z M69.535,124.178l5.682,21.53l12.242-8.809l-16.03-10.874
		C70.684,125.521,70.046,124.893,69.535,124.178z M28.136,86.782l31.478,12.035c2.255,0.862,3.957,2.758,4.573,5.092l3.992,15.129
		c0.183-1.745,0.974-3.387,2.259-4.624L149.227,38.6L28.136,86.782z"
		id="path1039"
       style="fill:#ffffff;stroke-width:0.0165365"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>
&nbsp;
                Telegram
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="whatsapp://send?text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20how%20to%20accelerate%20TIMM%20model%20inference%20up%20to%2084x%20faster%20using%20ONNX%20Runtime%20and%20TensorRT%20optimization%20techniques%21%0a%0ahttp%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f%0a" target="_blank" aria-label="Share on WhatsApp"
                class="share-btn whatsapp">
                <svg
   width="6.0324998mm"
   height="6.05896mm"
   viewBox="0 0 6.0324997 6.05896"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="whatsapp_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="4.9987205"
     inkscape:cy="35.692618"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-126.67735,-103.17712)">
    <path
       d="m 131.83672,104.07671 c -0.58208,-0.58209 -1.34937,-0.89959 -2.14312,-0.89959 -1.64042,0 -2.98979,1.34938 -2.98979,3.01625 0,0.52917 0.13229,1.03188 0.39687,1.48167 l -0.42333,1.56104 1.5875,-0.42333 c 0.42333,0.23812 0.92604,0.34396 1.42875,0.34396 1.66687,0 3.01625,-1.34938 3.01625,-3.01625 -0.0265,-0.74084 -0.3175,-1.50813 -0.87313,-2.06375 z m -2.14312,4.6302 c -0.44979,0 -0.87313,-0.13229 -1.27,-0.34395 l -0.10583,-0.0529 -0.92605,0.26458 0.26459,-0.89958 -0.0794,-0.13229 c -0.26458,-0.39688 -0.37041,-0.84667 -0.37041,-1.34938 0,-1.37583 1.11125,-2.48708 2.48708,-2.48708 0.66146,0 1.29646,0.26458 1.77271,0.74083 0.47625,0.47625 0.74083,1.11125 0.74083,1.77271 -0.0265,1.37583 -1.13771,2.48708 -2.51354,2.48708 z m 1.34937,-1.87854 c -0.0794,-0.0265 -0.44979,-0.23812 -0.5027,-0.26458 -0.0794,-0.0265 -0.1323,-0.0265 -0.18521,0.0265 -0.0529,0.0794 -0.21167,0.26458 -0.23813,0.29104 -0.0529,0.0529 -0.0794,0.0529 -0.15875,0.0265 -0.0794,-0.0265 -0.3175,-0.13229 -0.60854,-0.37042 -0.23812,-0.21167 -0.37042,-0.44979 -0.42333,-0.52917 -0.0529,-0.0794 0,-0.13229 0.0265,-0.15875 0.0265,-0.0264 0.0794,-0.0794 0.10583,-0.13229 0.0529,-0.0265 0.0794,-0.0794 0.10583,-0.13229 0.0265,-0.0529 0,-0.10583 0,-0.13229 0,-0.0265 -0.18521,-0.39688 -0.26458,-0.55563 -0.0265,-0.10583 -0.10583,-0.0794 -0.13229,-0.0794 h -0.15875 c 0,0 -0.10584,0.0265 -0.18521,0.0794 -0.0794,0.0794 -0.26458,0.26459 -0.26458,0.635 0,0.37042 0.26458,0.74084 0.29104,0.79375 0.0265,0.0529 0.52916,0.82021 1.29646,1.13771 0.1852,0.0794 0.3175,0.13229 0.42333,0.15875 0.18521,0.0529 0.34396,0.0529 0.47625,0.0265 0.15875,-0.0265 0.44979,-0.18521 0.50271,-0.34396 0.0529,-0.18521 0.0529,-0.3175 0.0529,-0.34396 -0.0264,-0.0794 -0.0794,-0.10583 -0.15875,-0.13229 z"
       id="path1793"
       style="stroke-width:0.264583;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                WhatsApp
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime.&amp;body=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%20123x%20Faster%20Inference%20with%20ONNX%20Runtime%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20how%20to%20accelerate%20TIMM%20model%20inference%20up%20to%2084x%20faster%20using%20ONNX%20Runtime%20and%20TensorRT%20optimization%20techniques%21%0a%0ahttp%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharge_timm_tensorrt%2f%0a" target="_blank"
                class="share-btn email" aria-label="Share via Email">
                <svg
   width="6.3499999mm"
   height="4.3961601mm"
   viewBox="0 0 6.3499999 4.3961601"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="email_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.7526575"
     inkscape:cy="33.4125"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-130.10375,-103.97942)">
    <path
       d="m 130.10375,104.22365 v 3.9077 0.24423 h 0.24423 5.86154 0.24423 v -0.24423 -3.9077 -0.24423 h -0.24423 -5.86154 -0.24423 z m 5.29675,0.24423 -2.12175,1.41196 -2.12175,-1.41196 z m -2.25913,1.91569 0.13738,0.0839 0.13738,-0.0839 2.54916,-1.70198 v 3.20553 h -5.37308 v -3.20553 z"
       id="path824"
       style="stroke-width:0.0152644;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                Email
            </a>
        </li>
        <hr>

        
        <section>
            <h5>❤️ Show some love</h5>
            <p>
                Creating free ML contents doesn't pay my bills. Support me in creating more free contents like these. 
                Consider buying me a coffee. Your support means a lot to me.
            </p>
            <div style="text-align:center">
                <a href="https://www.buymeacoffee.com/dicksonneoh" target="_blank"><img
                        src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" alt="Buy Me A Coffee"
                        style="height: 60px !important;width: 217px !important;"></a>
            </div>
        </section>
        <hr>
    </ul>
</section>

        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-10 offset-lg-1">
        <nav class="case-details-nav d-flex justify-content-between align-items-start">
          
            <div class="previous">
              <div class="d-flex align-items-center mb-3">
                <div class="icon mr-3">
                  <svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285">
                    <g data-name="Group 1243" fill="#2d2d2d">
                      <path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z" />
                      <path data-name="Path 1455" d="M13.137 1.41L3.39 15.558l-.975-1.415L12.166 0z" />
                    </g>
                  </svg>
                </div>
                <span class="small">Prev blog</span>
              </div>
              <div class="blog-nav-item">
                <div class="blog-nav-thumb">
                  <a href="http://localhost:1313/portfolio/unlocking_edge_ml_from_pytorch_to_edge_deployment/">
                    <img src="http://localhost:1313/images/portfolio/unlocking_edge_ml_from_pytorch_to_edge_deployment/thumbnail.png" alt="post-image">
                  </a>
                </div>
                <h5 class="title"><a class="text-dark" href="http://localhost:1313/portfolio/unlocking_edge_ml_from_pytorch_to_edge_deployment/">Unlocking Edge ML: From PyTorch to Edge Deployment</a></h5>
              </div>
            </div>
          
          
        </nav>
      </div>
    </div>
  </div>
</section>


  </div>
  <section class="footer" id="contact">
	<div class="footer__background_shape">
		<svg viewBox="0 0 1920 79">
			<path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
		</svg>
	</div>
	<div class="container">
		<div class="row">
			<div class="col-lg-12">
				<div class="footer__cta">
					<div class="shape-1">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="shape-2">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="text-light footer__cta_content">
						<span>Contact me</span>
						<h2 class="mb-0 mb-3">Let’s Start a Project</h2>
					</div>
					<div class="footer__cta_action">
						
					</div>
					<a href="https://api.whatsapp.com/send?phone=60133250827" rel="noopener" target="_blank" class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-whatsapp"></i>&nbsp;&nbsp;Chat on WhatsApp</a>
                <a href="https://t.me/dicksonneoh" rel="noopener" target="_blank" class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-telegram"></i>&nbsp;&nbsp;Chat on Telegram</a>
				</div>
			</div>
		</div>
		<div class="row footer__widget">
			<div class="col-lg-4">
				<div class="footer__widget_logo mb-5">
					<img src="http://localhost:1313/images/site-navigation/logo_resized.png" alt="widget-logo">
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_sitemap mb-5">
					<h4 class="base-font">Sitemap</h4>
					<ul class="unstyle-list small">
						
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/#about">About me</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/">Frequently Ask Question</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/">Privacy &amp; Policy</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/#portfolio">Latest Article</a></li>
						
					</ul>
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_address mb-5">
					<h4 class="base-font">Address</h4>
					
					<ul class="fa-ul small">
						<li class="mb-2"><a class="text-light" href="tel:&#43;%2860%29%203%208921%202020"><span class="fa-li"><i
										class="fa fa-phone"></i></span>&#43;(60) 3 8921 2020</a></li>
						<li class="mb-2"><a class="text-light" href="mailto:dickson.neoh@gmail.com"><span class="fa-li"><i
										class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li>
						<li class="mb-2">
							<span class="fa-li"><i class="fa fa-map-marker"></i></span>Kuala Lumpur, Malaysia.</a>
						</li>
					</ul>
				</div>
			</div>
		</div>
		<div class="row footer__footer">
			<div class="col-lg-6">
				<div class="footer__footer_copy text-light">
					<p>All right reserved copyright © Dickson Neoh 2024</p>
				</div>
			</div>
			<div class="col-lg-6">
				<div class="footer__footer_social">
					<ul class="unstyle-list">
						
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://www.linkedin.com/in/dickson-neoh/"><i
									class="fa fa-linkedin-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://twitter.com/dicksonneoh7"><i
									class="fa fa-twitter-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://github.com/dnth"><i
									class="fa fa-github-square"></i></a>
						</li>
						
					</ul>
				</div>
			</div>
		</div>
	</div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script>
<script src="http://localhost:1313/plugins/jQuery/jquery.min.js"></script>
<script src="http://localhost:1313/plugins/bootstrap/bootstrap.min.js"></script>
<script src="http://localhost:1313/plugins/slick/slick.min.js"></script>
<script src="http://localhost:1313/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="http://localhost:1313/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="http://localhost:1313/plugins/tweenmax/TweenMax.min.js"></script>
<script src="http://localhost:1313/plugins/imagesloaded/imagesloaded.min.js"></script>
<script src="http://localhost:1313/plugins/masonry/masonry.min.js"></script>

<script src="http://localhost:1313/js/form-handler.min.js"></script>

<script src="http://localhost:1313/js/script.min.js"></script>

</body>

</html>