<!DOCTYPE html>
<html>
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <title>Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU</title>
  <meta name="description" content="Learn the tips and tricks to accelerate YOLOv5 inference up to 180&#43; FPS on a CPU! For free!" />

  <meta property="og:url" content="http://localhost:1313/portfolio/supercharging_yolov5_180_fps_cpu/">
  <meta property="og:site_name" content="Dickson Neoh - Personal Portfolio">
  <meta property="og:title" content="Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU">
  <meta property="og:description" content="Learn the tips and tricks to accelerate YOLOv5 inference up to 180&#43; FPS on a CPU! For free!">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="portfolio">
    <meta property="article:published_time" content="2022-06-07T11:00:15+08:00">
    <meta property="article:modified_time" content="2022-06-07T11:00:15+08:00">
    <meta property="article:tag" content="DeepSparse">
    <meta property="article:tag" content="ONNX">
    <meta property="article:tag" content="YOLOv5">
    <meta property="article:tag" content="Real-Time">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Pistol">
    <meta property="og:image" content="http://localhost:1313/images/portfolio/supercharging_yolov5/post_image.png">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/images/portfolio/supercharging_yolov5/post_image.png">
  <meta name="twitter:title" content="Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU">
  <meta name="twitter:description" content="Learn the tips and tricks to accelerate YOLOv5 inference up to 180&#43; FPS on a CPU! For free!">


  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/slick/slick.css" />
  <link rel="stylesheet" href="http://localhost:1313/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/font-awesome/css/font-awesome.min.css" />

  <!-- Magnific Popup -->
  <link rel="stylesheet" href="http://localhost:1313/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="http://localhost:1313/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="http://localhost:1313/images/favicon-purple.ico" type="image/x-icon" />
  <link rel="icon" href="http://localhost:1313/images/favicon.gif" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-54500366-2');
  </script>
  
</head>

<body>
  <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
    <a href="http://localhost:1313/" class="navbar-brand">
      <img src="http://localhost:1313/images/site-navigation/logo_resized.png" alt="site-logo">
    </a>
    <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
      <ul class="nav navbar-nav main-navigation my-0 mx-auto">
        
        
        <li class="nav-item">
          <a href="http://localhost:1313/#home"
            class="nav-link text-dark text-sm-center p-2 ">Home</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#about"
            class="nav-link text-dark text-sm-center p-2 ">About</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#service"
            class="nav-link text-dark text-sm-center p-2 ">Services</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#portfolio"
            class="nav-link text-dark text-sm-center p-2 ">Projects</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#resume"
            class="nav-link text-dark text-sm-center p-2 ">Resume</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#skills"
            class="nav-link text-dark text-sm-center p-2 ">Skills</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#blog"
            class="nav-link text-dark text-sm-center p-2 ">Blogs</a>
        </li>
        
        <li class="nav-item">
          <a href="http://localhost:1313/#contact"
            class="nav-link text-dark text-sm-center p-2 ">Contact</a>
        </li>
        
      </ul>
      <div class="navbar-nav">
        <a href="http://localhost:1313/contact" class="btn btn-primary btn-zoom hire_button">Let's Talk</a>
      </div>
       

    </div>
  </div>
</nav>
  <div id="content">
    

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          
        </nav>
      </div>
    </div>

    <div class="row p-3">
      <div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center">
        <i class="fa fa-calendar"></i> &ensp;
        June 7, 2022 &ensp; &ensp;
        <i class="fa fa-clock-o"></i> &ensp;
        15 mins read
      </div>
      
        <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
          <div class="portfolio-item-tags-categories">
            <i class="fa fa-tags"></i> &ensp;
            
              <a href="/tags/deepsparse" class="tag-category">DeepSparse</a>
            
              <a href="/tags/onnx" class="tag-category">ONNX</a>
            
              <a href="/tags/yolov5" class="tag-category">YOLOv5</a>
            
              <a href="/tags/real-time" class="tag-category">real-time</a>
            
              <a href="/tags/optimization" class="tag-category">optimization</a>
            
              <a href="/tags/pistol" class="tag-category">pistol</a>
            
          </div>
        </div>
      
      
        <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
          <div class="portfolio-item-tags-categories">
            <i class="fa fa-folder-open"></i> &ensp;
            
              <a href="/categories/deployment" class="tag-category">deployment</a>
            
              <a href="/categories/object-detection" class="tag-category">object-detection</a>
            
              <a href="/categories/modeling" class="tag-category">modeling</a>
            
          </div>
        </div>
      
    </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
    <img src=http://localhost:1313/images/hero/figure-svg.svg alt="">
  </div>
  <div class="animate-shape">
    <img src=http://localhost:1313/images/skill/skill-background-shape.svg alt="">
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600">
      <defs>
        <linearGradient id="d" x1="0.929" y1="0.111" x2="0.263" y2="0.935" gradientUnits="objectBoundingBox">
          <stop offset="0" stop-color="#f1f6f9" />
          <stop offset="1" stop-color="#f1f6f9" stop-opacity="0" />
        </linearGradient>
      </defs>
      <g data-name="blob-shape (3)">
        <path class="blob" fill="url(#d)"
          d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z" />
      </g>
    </svg>
  </div>
  <div class="animate-pattern">
    <img src=http://localhost:1313/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <div class="singleBlog__feature">
          <img src=http://localhost:1313/images/portfolio/supercharging_yolov5/post_image.png alt="feature-image">
        </div>
      </div>
    </div>
    <div class="row mt-5">
      <div class="col-lg-12">
        <div class="singleBlog__content">
          
            <h3>Table of Contents</h3>
            <div id="floating-toc" class="floating-toc">
              <div id="toc-handle" class="toc-handle">☰ Table of Contents</div>
              <nav id="TableOfContents">
  <ul>
        <li><a class="toc-link" href="#-motivation">🔥 Motivation</a></li>
        <li><a class="toc-link" href="#-setting-up">🔩 Setting Up</a>
          <ul>
            <li><a class="toc-link" href="#-dataset">🔫 Dataset</a></li>
            <li><a class="toc-link" href="#-installation">🦸 Installation</a></li>
          </ul>
        </li>
        <li><a class="toc-link" href="#-baseline-performance">⛳ Baseline Performance</a>
          <ul>
            <li><a class="toc-link" href="#-pytorch">🔦 PyTorch</a></li>
            <li><a class="toc-link" href="#-deepsparse-engine">🕸 DeepSparse Engine</a></li>
          </ul>
        </li>
        <li><a class="toc-link" href="#-sparseml-and-recipes">👨‍🍳 SparseML and Recipes</a>
          <ul>
            <li><a class="toc-link" href="#-one-shot">☝️ One-Shot</a></li>
            <li><a class="toc-link" href="#-sparse-transfer-learning">🤹‍♂️ Sparse Transfer Learning</a></li>
            <li><a class="toc-link" href="#-pruned-yolov5-s">✂ Pruned YOLOv5-S</a></li>
            <li><a class="toc-link" href="#-quantized-yolov5-s">🔬 Quantized YOLOv5-S</a></li>
            <li><a class="toc-link" href="#-pruned--quantized-yolov5-s">🪚 Pruned + Quantized YOLOv5-S</a></li>
          </ul>
        </li>
        <li><a class="toc-link" href="#-supercharging-with-smaller-models">🚀 Supercharging with Smaller Models</a></li>
        <li><a class="toc-link" href="#-conclusion">🚧 Conclusion</a></li>
        <li><a class="toc-link" href="#-comments--feedback">🙏 Comments &amp; Feedback</a></li>
      </ul>
</nav>
            </div>
          
          <hr>
          <h3 id="-motivation">🔥 Motivation</h3>
<p>After months of searching, you&rsquo;ve finally found <em>the one</em>.</p>
<p>The one object detection library that just works.
No installation hassle, no package version mismatch, and no <code>CUDA</code> errors.</p>
<p>I&rsquo;m talking about the amazingly engineered <a href="https://github.com/ultralytics/yolov5" target="_blank" rel="nofollow noopener noreferrer">YOLOv5</a> object detection library by <a href="https://ultralytics.com/yolov5" target="_blank" rel="nofollow noopener noreferrer">Ultralytics</a>.</p>
<p>Elated, you quickly find an interesting dataset from <a href="https://roboflow.com/" target="_blank" rel="nofollow noopener noreferrer">Roboflow</a> and finally trained a state-of-the-art (SOTA) YOLOv5 model to detect firearms from image streams.</p>
<p>You ran through a quick checklist &ndash;</p>
<ul>
<li>Inference results, checked ✅</li>
<li><code>COCO</code> mAP, checked ✅</li>
<li>Live inference latency, checked ✅</li>
</ul>
<p>You&rsquo;re on top of the world.</p>
<iframe src="https://giphy.com/embed/zEJRrMkDvRe5G" width="480" height="360" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/win-zEJRrMkDvRe5G"></a></p>
<p>You can finally pitch the results to your clients next Monday.
At the back of your mind, you can already see your clients&rsquo; impressed look on the astonishing feat.</p>
<p>On the pitching day, just when you thought things are going in the right direction.
One of the clients asked,</p>
<p>&ldquo;<strong>Does your model run on our existing CPU?</strong>&rdquo;</p>
<p>You flinched.</p>
<p>That wasn&rsquo;t something you anticipated. You tried to convince them that GPUs are <em>&ldquo;the way forward&rdquo;</em> and it&rsquo;s <em>&ldquo;the best way&rdquo;</em> to run your model in real-time.</p>
<p>You scanned the room and begin to notice the looks on their faces every time you said the word <em>GPU</em> and <em>CPU</em>.</p>





<figure>
    
    <a href="/portfolio/supercharging_yolov5_180_fps_cpu/meme.jpg" class="image-popup">
        <img src="/portfolio/supercharging_yolov5_180_fps_cpu/meme.jpg" 
             srcset="/portfolio/supercharging_yolov5_180_fps_cpu/meme_hu2438776333772541300.jpg 360w, /portfolio/supercharging_yolov5_180_fps_cpu/meme_hu7026195110737784285.jpg 720w, /portfolio/supercharging_yolov5_180_fps_cpu/meme_hu3718389831916567219.jpg 1920w" 
             sizes="(max-width: ) 100vw, "
             
             
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


<p>Needless to say it didn&rsquo;t go well.
I hope nobody will have to face this awkward situation in a pitching session, ever.
You don&rsquo;t have to learn it the hard way, like I did.</p>
<p>You may wonder, can we really use consumer grade CPUs to run models in real-time?</p>
<p>🦾<strong>YES we can!</strong></p>
<p>I wasn&rsquo;t a believer, but now I am, after discovering <a href="https://neuralmagic.com/" target="_blank" rel="nofollow noopener noreferrer">Neural Magic</a>.</p>
<p>In this post, I show you how you can supercharge your YOLOv5 inference performance running on CPUs using <strong>free</strong> and open-source tools by Neural Magic.</p>
<style type="text/css">
    .notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice
    p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0
    0;font-weight:700;color:#fff;background:#6ab0de; text-transform:uppercase}.notice.warning
    .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info
    .notice-title{background:#f0b37e;}.notice.info{background:#fff2db}.notice.note
    .notice-title{background:#6ab0de}.notice.note{background:#e7f2fA}.notice.tip
    .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice
    img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline
    svg{top:0.125em;position:relative}</style>
    <div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg">
            <symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z" />
            </symbol>
            <symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
            </symbol>
            <symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z" />
            </symbol>
            <symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet">
                <path
                    d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" />
            </symbol>
        </svg></div><div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>By the end of this post, you will learn how to:</p>
<ul>
<li>Train a SOTA YOLOv5 model on your own data.</li>
<li>Sparsify the model using SparseML quantization aware training, sparse transfer learning, and one-shot quantization.</li>
<li>Export the sparsified model and run it using the DeepSparse engine at insane speeds.</li>
</ul>
<p><strong>P/S</strong>: The end result - YOLOv5 on CPU at 180+ FPS using only 4 CPU cores! 🚀</p></div>
<div class="notice info" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#info-notice"></use>
                </svg></span>info</p><p>If you&rsquo;re completely new to YOLOv5, get up to speed by reading <a href="https://www.exxactcorp.com/blog/Deep-Learning/YOLOv5-PyTorch-Tutorial?utm_source=web%20referral&amp;utm_medium=backlink&amp;utm_campaign=BP%20YOLOv5%20PyTorch%20Tutorial&amp;utm_term=Dickson%20Neoh%20Supercharging%20YOLOv5" target="_blank" rel="nofollow noopener noreferrer">this tutorial</a> by Exxact. </p></div>
<p>If that sounds exciting let&rsquo;s dive in 🧙</p>
<h3 id="-setting-up">🔩 Setting Up</h3>
<h4 id="-dataset">🔫 Dataset</h4>
<p>The <a href="https://edition.cnn.com/2022/05/25/us/uvalde-texas-elementary-school-shooting-what-we-know/index.html" target="_blank" rel="nofollow noopener noreferrer">recent gun violence</a> news had me thinking deeply about how we can prevent incidents like these again.
This is the worst gun violence since 2012, and 21 innocent lives were lost.</p>
<p>I&rsquo;m deeply saddened, and my heart goes out to all victims of the violence and their loved ones.</p>
<p>I&rsquo;m not a lawmaker, so there is little I can do there.
But, I think I know something in computer vision that might help.
That&rsquo;s when I came across the <a href="https://public.roboflow.com/object-detection/pistols" target="_blank" rel="nofollow noopener noreferrer">Pistols Dataset</a> from Roboflow.</p>
<p>This dataset contains 2986 images and 3448 labels across a single annotation class: pistols.
Images are wide-ranging: pistols in hand, cartoons, and staged studio-quality images of guns.
The dataset was originally released by the University of Grenada.</p>





<figure>
    
    <a href="/portfolio/supercharging_yolov5_180_fps_cpu/pistol.png" class="image-popup">
        <img src="/portfolio/supercharging_yolov5_180_fps_cpu/pistol.png" 
             srcset="/portfolio/supercharging_yolov5_180_fps_cpu/pistol_hu2819809653530983829.png 360w, /portfolio/supercharging_yolov5_180_fps_cpu/pistol_hu520984128970553257.png 720w, /portfolio/supercharging_yolov5_180_fps_cpu/pistol_hu9412877713656530013.png 1920w" 
             sizes="(max-width: ) 100vw, "
             
             
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


<h4 id="-installation">🦸 Installation</h4>
<p>Now let&rsquo;s put the downloaded Pistols Dataset into the appropriate folder first.
I will put the downloaded images and labels into the <code>datasets/</code> folder.</p>
<p>Let&rsquo;s also put the sparsification recipes from <a href="https://github.com/neuralmagic/sparseml/tree/main/integrations/ultralytics-yolov5/recipes" target="_blank" rel="nofollow noopener noreferrer">SparseML</a> into the <code>recipes/</code> folder. More on <code>recipes</code> later.</p>
<p>Here&rsquo;s a high-level overview of my directory.</p>
<pre tabindex="0"><code class="language-tree" data-lang="tree">├── req.txt
├── datasets
│   ├── pistols
│   │   ├── train
|   |   ├── valid
├── recipes
│   ├── yolov5s.pruned.md
│   ├── yolov5.transfer_learn_pruned.md
│   ├── yolov5.transfer_learn_pruned_quantized.md
|   └── ...
└── yolov5-train
        ├── data
        |   ├── hyps
        |   |   ├── hyps.scratch.yaml
        |   |   └── ...
        |   ├── pistols.yaml
        |   └── ...
        ├── models_v5.0
        |   ├── yolov5s.yaml
        |   └── ...
        ├── train.py
        ├── export.py
        ├── annotate.py
        └── ...
</code></pre><div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><ul>
<li>
<p><code>req.txt</code> - Requirement file to install all packages used in this post.</p>
</li>
<li>
<p><code>datasets/</code> - Contains the train and validation images/labels downloaded from Roboflow.</p>
</li>
<li>
<p><code>recipes/</code> - Contains sparsification recipes from the <a href="https://github.com/neuralmagic/sparseml/tree/main/integrations/ultralytics-yolov5/recipes" target="_blank" rel="nofollow noopener noreferrer">SparseML</a> repo.</p>
</li>
<li>
<p><code>yolov5-train/</code> - Cloned directory from Neural Magic&rsquo;s YOLOv5 <a href="https://github.com/neuralmagic/yolov5" target="_blank" rel="nofollow noopener noreferrer">fork</a>.</p>
</li>
</ul>
<p><strong>NOTE</strong>: You can explore further into the folder structure on my <a href="https://github.com/dnth/yolov5-deepsparse-blogpost" target="_blank" rel="nofollow noopener noreferrer">Github repo</a>.</p></div>
<div class="notice warning" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#warning-notice"></use>
                </svg></span>warning</p><p><strong>IMPORTANT</strong>: The sparsification recipes will only work with Neural Magic&rsquo;s YOLOv5 fork and will <strong>NOT WORK</strong> with the original YOLOv5 by Ultralytics. </p></div>
<p>For this post, we are going to use a <a href="https://github.com/neuralmagic/yolov5" target="_blank" rel="nofollow noopener noreferrer">forked version</a> of the YOLOv5 library that will allow us to do custom optimizations in the upcoming section.</p>
<p>To install, all packages in this blog post, run the following commands</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/dnth/yolov5-deepsparse-blogpost
</span></span><span style="display:flex;"><span>cd yolov5-deepsparse-blogpost/
</span></span><span style="display:flex;"><span>pip install torch<span style="color:#f92672">==</span>1.9.0 torchvision<span style="color:#f92672">==</span>0.10.0 --extra-index-url https://download.pytorch.org/whl/cu111
</span></span><span style="display:flex;"><span>pip install -r req.txt
</span></span></code></pre></div><p>Or, if you&rsquo;re just getting started, I&rsquo;d recommend 👇</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>🔥 Run everything in Colab with a notebook I made <a href="https://colab.research.google.com/github/dnth/yolov5-deepsparse-blogpost/blob/master/notebooks/deepsparse_blogpost.ipynb" target="_blank" rel="nofollow noopener noreferrer">here</a>. </p></div>
<!-- ```bash
git clone https://github.com/neuralmagic/yolov5.git
cd yolov5
git checkout release/0.12
pip install -r requirements.txt
``` -->
<h3 id="-baseline-performance">⛳ Baseline Performance</h3>
<h4 id="-pytorch">🔦 PyTorch</h4>
<p>Now that everything&rsquo;s in place, let&rsquo;s start by training a baseline model with no optimization.</p>
<p>For that, run the <code>train.py</code> script in the <code>yolov5-train/</code> folder.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py --cfg ./models_v5.0/yolov5s.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --data pistols.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --hyp data/hyps/hyp.scratch.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --weights yolov5s.pt --img <span style="color:#ae81ff">416</span> --batch-size <span style="color:#ae81ff">64</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --optimizer SGD --epochs <span style="color:#ae81ff">240</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --project yolov5-deepsparse --name yolov5s-sgd
</span></span></code></pre></div><div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><ul>
<li>
<p><code>--cfg</code> &ndash; Path to the configuration file which stores the model architecture.</p>
</li>
<li>
<p><code>--data</code> &ndash; Path to the <code>.yaml</code> file that stores the details of the Pistols dataset.</p>
</li>
<li>
<p><code>--hyp</code> &ndash; Path to the <code>.yaml</code> file that stores the training hyperparameter configurations.</p>
</li>
<li>
<p><code>--weights</code> &ndash; Path to a pretrained weight.</p>
</li>
<li>
<p><code>--img</code> &ndash; Input image size.</p>
</li>
<li>
<p><code>--batch-size</code> &ndash; Batch size used in training.</p>
</li>
<li>
<p><code>--optimizer</code> &ndash; Type of optimizer. Options include <code>SGD</code>, <code>Adam</code>, <code>AdamW</code>.</p>
</li>
<li>
<p><code>--epochs</code> &ndash; Number of training epochs.</p>
</li>
<li>
<p><code>--project</code> &ndash; Wandb project name.</p>
</li>
<li>
<p><code>--name</code> &ndash; Wandb run id.</p>
</li>
</ul></div>
<p>All metrics are logged to Weights &amp; Biases (Wandb) <a href="https://wandb.ai/dnth/yolov5-deepsparse" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p>
<p>Once training&rsquo;s done, let&rsquo;s run inference on a video with the <code>annotate.py</code> script.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python annotate.py yolov5-deepsparse/yolov5s-sgd/weights/best.pt <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --source data/pexels-cottonbro-8717592.mp4 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --engine torch <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --image-shape <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --device cpu <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --conf-thres 0.7
</span></span></code></pre></div><div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>The first argument points to the <code>.pt</code> saved checkpoint.</p>
<ul>
<li>
<p><code>--source</code> - The input to run inference on. Options: path to video/images or just specify <code>0</code> to infer on your webcam.</p>
</li>
<li>
<p><code>--engine</code> - Which engine to use. Options: <code>torch</code>, <code>deepsparse</code>, <code>onnxruntime</code>.</p>
</li>
<li>
<p><code>--image-size</code> &ndash; Input resolution.</p>
</li>
<li>
<p><code>--device</code> &ndash; Device to use for inference. Options: <code>cpu</code> or <code>0</code> (GPU).</p>
</li>
<li>
<p><code>--conf-thres</code> &ndash; Confidence threshold for inference.</p>
</li>
</ul>
<p><strong>NOTE</strong>: The inference output will be saved in the <code>annotation_results/</code> folder.</p></div>
<p>Here&rsquo;s how it looks like running the baseline YOLOv5-S on an Intel i9-11900 using all 8 CPU cores.</p>
<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/torch-annotation/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 21.91</li>
<li>Average inference time (ms) : 45.58</li>
</ul>
<!-- And for reference, inference on an RTX3090 GPU.

<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/torch-gpu/results_.mp4" type="video/mp4">
  <span></span>
</video>

+ Average FPS : 89.20
+ Average inference time (ms) : 11.21 -->
<p>Actually, the FPS looks quite decent already and might suit some applications even without further optimization.</p>
<p>But why settle when you can get something better?
After all, that&rsquo;s why you&rsquo;re here, right? 😉</p>
<p>Meet 👇</p>
<h4 id="-deepsparse-engine">🕸 DeepSparse Engine</h4>
<p>DeepSparse is an inference engine by Neural Magic that runs optimally on CPUs.
It&rsquo;s incredibly easy to use. Just give it an ONNX model and you&rsquo;re ready to roll.</p>
<p>Let&rsquo;s export our <code>.pt</code> file into ONNX using the <code>export.py</code> script.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python export.py --weights yolov5-deepsparse/yolov5s-sgd/weights/best.pt <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --include onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --imgsz <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --dynamic <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --simplify
</span></span></code></pre></div><div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p><code>--weight</code> &ndash; Path to the <code>.pt</code> checkpoint.</p>
<p><code>--include</code> &ndash; Which format to export to. Options: <code>torchscript</code>, <code>onnx</code>, <a href="https://github.com/dnth/yolov5-deepsparse-blogpost/blob/4d44b32909bbc9e8b3bb7f8bf89f0e50361872f7/yolov5-train/export.py#L694" target="_blank" rel="nofollow noopener noreferrer">etc</a>.</p>
<p><code>--imgsz</code> &ndash; Image size.</p>
<p><code>--dynamic</code> &ndash; Dynamic axes.</p>
<p><code>--simplify</code> &ndash; Simplify the ONNX model.</p></div>
<p>And now, run the inference script again, this time using the <code>deepsparse</code> engine and with only 4 CPU cores in the <code>--num-cores</code> argument.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python annotate.py yolov5-deepsparse/yolov5s-sgd/weights/best.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --source data/pexels-cottonbro-8717592.mp4 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --image-shape <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --conf-thres 0.7 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --engine deepsparse <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --device cpu <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --num-cores <span style="color:#ae81ff">4</span>
</span></span></code></pre></div><video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/onnx-annotation/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 29.48</li>
<li>Average inference time (ms) : 33.91</li>
</ul>
<p>Just like that, we improved the average FPS from 21+ (PyTorch engine on CPU using 8 cores) to 29+ FPS.
All we did was use the ONNX model with the DeepSparse engine.</p>
<p><strong>P/S</strong>: We are done with <strong>just the baselines</strong> here!
The real action only happens next - when we run sparsification with 👇</p>
<h3 id="-sparseml-and-recipes">👨‍🍳 SparseML and Recipes</h3>





<figure>
    
    <a href="/portfolio/supercharging_yolov5_180_fps_cpu/sparseml.png" class="image-popup">
        <img src="/portfolio/supercharging_yolov5_180_fps_cpu/sparseml.png" 
             srcset="/portfolio/supercharging_yolov5_180_fps_cpu/sparseml_hu9752216008916359204.png 360w, /portfolio/supercharging_yolov5_180_fps_cpu/sparseml_hu18172463315183999434.png 720w, /portfolio/supercharging_yolov5_180_fps_cpu/sparseml_hu15553791564851003551.png 1920w" 
             sizes="(max-width: ) 100vw, "
             alt="Image from SparseML documentation page." 
             
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
    <figcaption style="font-size: 0.8em;">
        <p>
        Image from SparseML documentation page.
        
            
        
        </p> 
    </figcaption>
    
</figure>


<p>Sparsification is the process of removing redundant information from a model.
The result is a <strong>smaller and faster</strong> model.</p>
<p>This is how we speed up our YOLOv5 model, by a lot!</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>In general, there are 2 methods to sparsify a model - Pruning and Quantization.</p>
<ul>
<li>
<p><strong>Pruning</strong> - Removing unused weights in the model.</p>
</li>
<li>
<p><strong>Quantization</strong> - Forcing a model to use a less accurate storage format i.e. from 32-bit floating point (FP32) to 8-bit integer (INT8).</p>
</li>
</ul>
<p>⚡ <strong>P/S</strong>: Used together or separately both pruning and quantization result in a smaller and faster model!</p></div>
<p>How do we sparsify models?</p>
<p>Using <a href="https://github.com/neuralmagic/sparseml" target="_blank" rel="nofollow noopener noreferrer">SparseML</a> - an open-source library by Neural Magic.
With SparseML you can sparsify neural networks by applying pre-made <strong>recipes</strong> to the model.
You can also modify the recipes to suit your needs.</p>
<!-- It currently supports integration with several well known libraries from computer vision and natural language processing domain. -->
<div class="notice note" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#note-notice"></use>
                </svg></span>note</p><p>There are 3 methods to sparsify models with SparseML:</p>
<ul>
<li>
<p>1️⃣ Post-training (One-shot).</p>
</li>
<li>
<p>2️⃣ Sparse Transfer Learning.</p>
</li>
<li>
<p>3️⃣ Training Aware.</p>
</li>
</ul>
<p><strong>NOTE</strong>: 1️⃣ does not require re-training but only supports dynamic quantization.
2️⃣ and 3️⃣ requires re-training and supports pruning and quantization which may give better results.</p></div>
<p>You may wonder, this sounds too good to be true!</p>
<p><em>What&rsquo;s the caveat</em>?</p>
<p>Good question!</p>
<p>With sparsification, you can expect a slight loss in accuracy depending on the degree of sparsification.
Highly sparse models are usually less accurate than the original model but gains significant boost in speed and latency.</p>
<p>With the recipes from SparseML, the loss of accuracy ranges from 2% to 6%.
In other words the <em>recovery</em> is <a href="https://github.com/neuralmagic/deepsparse/tree/main/examples/ultralytics-yolo" target="_blank" rel="nofollow noopener noreferrer">94% to 98%</a> compared to the performance of the original model.
In exchange, we gain phenomenal speedups, ranging from 2x to 10x faster!</p>
<p>In most situations, this is not a big deal.
If the accuracy loss is something you can tolerate, then let&rsquo;s sparsify some models already! 🤏.</p>
<h4 id="-one-shot">☝️ One-Shot</h4>
<p>The one-shot method is the easiest way to sparsify an existing model as it doesn&rsquo;t require re-training.</p>
<p>But this only works well for dynamic quantization, for now.
There are ongoing works in making one-shot work well for pruning.</p>
<p>Let&rsquo;s run the one-shot method on the baseline model we trained earlier.
All you need to do is add a <code>--one-shot</code> argument to the training script, and specify a pruning <code>--recipe</code>.
Remember to specify <code>--weights</code> to the location of the best checkpoint from the training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py --cfg ./models_v5.0/yolov5s.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --recipe ../recipes/yolov5s.pruned.md <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --data pistols.yaml --hyp data/hyps/hyp.scratch.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --weights yolov5-deepsparse/yolov5s-sgd/weights/best.pt <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --img <span style="color:#ae81ff">416</span> --batch-size <span style="color:#ae81ff">64</span> --optimizer SGD --epochs <span style="color:#ae81ff">240</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --project yolov5-deepsparse --name yolov5s-sgd-one-shot <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --one-shot
</span></span></code></pre></div><p>It should generate another <code>.pt</code> in the directory specified in <code>--name</code>.
This <code>.pt</code> file stores the quantized weights in <code>int8</code> format instead of <code>fp32</code> resulting in a reduction in model size and inference speedups.</p>
<p>Next, let&rsquo;s export the quantized <code>.pt</code> file into <code>ONNX</code> format.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python export.py --weights yolov5-deepsparse/yolov5s-sgd-one-shot/weights/checkpoint-one-shot.pt <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                 --include onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                 --imgsz <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                 --dynamic <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                 --simplify
</span></span></code></pre></div><p>And run an inference</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python annotate.py yolov5-deepsparse/yolov5s-sgd-one-shot/weights/checkpoint-one-shot.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --source data/pexels-cottonbro-8717592.mp4 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --image-shape <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --conf-thres 0.7 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --engine deepsparse <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --device cpu <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --num-cores <span style="color:#ae81ff">4</span>
</span></span></code></pre></div><video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/one-shot/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 32.00</li>
<li>Average inference time (ms) : 31.24</li>
</ul>
<p>At no re-training cost, we are performing 10 FPS better than the original model.
We maxed out at about 40 FPS!</p>
<p>The one-shot method only took seconds to complete.
If you&rsquo;re looking for the easiest method for performance gain, one-shot is the way to go.</p>
<p>But, if you&rsquo;re willing to re-train the model to double its performance and speed, read on 👇</p>
<h4 id="-sparse-transfer-learning">🤹‍♂️ Sparse Transfer Learning</h4>
<p>With SparseML you can take an already sparsified model (pruned and quantized) and fine-tune it on your own dataset.
This is known as <em>Sparse Transfer Learning</em>.</p>
<p>This can be done by running</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py --data pistols.yaml --cfg ./models_v5.0/yolov5s.yaml 
</span></span><span style="display:flex;"><span>                --weights zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned_quant-aggressive_94?recipe_type<span style="color:#f92672">=</span>transfer 
</span></span><span style="display:flex;"><span>                --img <span style="color:#ae81ff">416</span> --batch-size <span style="color:#ae81ff">64</span> --hyp data/hyps/hyp.scratch.yaml 
</span></span><span style="display:flex;"><span>                --recipe ../recipes/yolov5.transfer_learn_pruned_quantized.md 
</span></span><span style="display:flex;"><span>                --optimizer SGD
</span></span><span style="display:flex;"><span>                --project yolov5-deepsparse --name yolov5s-sgd-pruned-quantized-transfer
</span></span></code></pre></div><p>The above command loads a sparse YOLOv5-S from Neural Magic&rsquo;s <a href="https://github.com/neuralmagic/sparsezoo" target="_blank" rel="nofollow noopener noreferrer">SparseZoo</a> and runs the training on your dataset.</p>
<p>The <code>--weights</code> argument points to a model from the SparseZoo.
There are more sparsified <a href="https://docs.neuralmagic.com/sparsezoo/source/models.html" target="_blank" rel="nofollow noopener noreferrer">models available</a> in SparseZoo.
I will leave it to you to explore which model works best.</p>
<p>Running inference with <code>annotate.py</code> results in
<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/yolov5s-pruned-quant-tl/results_.mp4" type="video/mp4">
  <span></span>
</video></p>
<ul>
<li>Average FPS : 51.56</li>
<li>Average inference time (ms) : 19.39</li>
</ul>
<p>We almost 2x the FPS from the previous one-shot method!
Judging from the FPS value and <code>mAP</code> <a href="%28https://wandb.ai/dnth/yolov5-deepsparse%29">scores</a>, Sparse Transfer Learning makes a lot of sense for most applications.</p>
<p>But, if you scrutinize further into the <code>mAP</code> metric on the <a href="https://wandb.ai/dnth/yolov5-deepsparse" target="_blank" rel="nofollow noopener noreferrer">Wandb dashboard</a>, you&rsquo;ll notice it&rsquo;s slightly lower than the next method 💪.</p>
<h4 id="-pruned-yolov5-s">✂ Pruned YOLOv5-S</h4>
<p>Here, instead of taking an already sparsified model, we are going to sparsify our model by pruning it ourselves.</p>
<p>To do that we will use a pre-made recipe on the SparseML <a href="https://github.com/neuralmagic/sparseml/tree/main/integrations/ultralytics-yolov5/recipes" target="_blank" rel="nofollow noopener noreferrer">repo</a>.
This recipe tells the training script how to prune the model during training.</p>
<p>For that, we slightly modify the arguments of <code>train.py</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py --cfg ./models_v5.0/yolov5s.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --recipe ../recipes/yolov5s.pruned.md
</span></span><span style="display:flex;"><span>                --data pistols.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --hyp data/hyps/hyp.scratch.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --weights yolov5s.pt --img <span style="color:#ae81ff">416</span> 
</span></span><span style="display:flex;"><span>                --batch-size <span style="color:#ae81ff">64</span> --optimizer SGD <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --project yolov5-deepsparse --name yolov5s-sgd-pruned
</span></span></code></pre></div><p>The only change here is the <code>--recipe</code> and the <code>--name</code> argument.
Also, there is no need to specify the <code>--epoch</code> argument because the number of training epochs is specified in the recipe.</p>
<p><code>--recipe</code> tells the training script which recipe to use for the YOLOv5-S model.
In this case, we are using the <code>yolov5s.pruned.md</code> recipe which only prunes the model as it trains.
You can change how aggressive your model is pruned by modifying the <code>yolov5s.pruned.md</code> recipe.</p>
<div class="notice warning" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#warning-notice"></use>
                </svg></span>warning</p><p><strong>IMPORTANT</strong>: The sparsification recipes are model dependent. Eg. YOLOv5-S recipes will not work with YOLOv5-L.</p>
<p>So make sure you get the right recipe for the right model. Check out other YOLOv5 pre-made recipes <a href="https://github.com/neuralmagic/sparseml/tree/main/integrations/ultralytics-yolov5/recipes" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div>
<p>Running inference, we find</p>
<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/yolov5s-pruned/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 35.50</li>
<li>Average inference time (ms) : 31.73</li>
</ul>
<p>The drop in FPS is expected compared to the Sparse Transfer Learning method because this model is only pruned and <strong>not quantized</strong>.
But we gain higher <code>mAP</code> values.</p>
<h4 id="-quantized-yolov5-s">🔬 Quantized YOLOv5-S</h4>
<p>We&rsquo;ve seen the effects of pruning, what about quantization? Let&rsquo;s run quantization for the YOLOv5-S model and see how it behaves.</p>
<p>We could run the quantization without training (one-shot). But for better effects let&rsquo;s train the model for 2 epochs.
Re-training for 2 epochs allow the weights to re-adjust to the quantized values and hence produce better results.</p>
<p>The number of training epochs is specified in the <code>yolov5s.quantized.md</code> file.</p>
<p>Let&rsquo;s run the <code>train.py</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py --cfg ./models_v5.0/yolov5s.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --recipe ../recipes/yolov5s.quantized.md <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --data pistols.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --hyp data/hyps/hyp.scratch.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --weights yolov5-deepsparse/yolov5s-sgd/weights/best.pt --img <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --batch-size <span style="color:#ae81ff">64</span> --project yolov5-deepsparse --name yolov5s-sgd-quantized
</span></span></code></pre></div><p>Inferencing with <code>annotate.py</code></p>
<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/yolov5-quant/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 43.29</li>
<li>Average inference time (ms) : 23.09</li>
</ul>
<p>We have a bump in the FPS compared to the pruned model.
With careful observation you&rsquo;d notice a misdetection at the <code>0:03</code> second.</p>
<p>Here, we see that the quantized model is faster than the pruned model at the cost of detection accuracy.
But note, in this model we&rsquo;ve only trained for 2 epochs compared to 240 epochs with the pruned model.
Re-training for longer may solve the misdetection issue.</p>
<p>We&rsquo;ve seen how the YOLOv5-S model performs when it is</p>
<ul>
<li>Only pruned</li>
<li>Only quantized</li>
</ul>
<p>But, can we run both pruning and quantization?</p>
<p>Of course, why not? 🤖</p>
<h4 id="-pruned--quantized-yolov5-s">🪚 Pruned + Quantized YOLOv5-S</h4>
<p>Now, let&rsquo;s take it to the next level by running both pruning and quantization.
Note the difference <code>--recipe</code> I&rsquo;m using.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python train.py --cfg ./models_v5.0/yolov5s.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --recipe ../recipes/yolov5.transfer_learn_pruned_quantized.md <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --data pistols.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --hyp data/hyps/hyp.scratch.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --weights yolov5s.pt --img <span style="color:#ae81ff">416</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --batch-size <span style="color:#ae81ff">64</span> --optimizer SGD <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>                --project yolov5-deepsparse --name yolov5s-sgd-pruned-quantized
</span></span></code></pre></div><p>Export with <code>export.py</code> and run inference with <code>annotate.py</code>.
We get</p>
<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/yolov5s-pruned-quant/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 58.06</li>
<li>Average inference time (ms) : 17.22</li>
</ul>
<p>On our Wandb <a href="https://wandb.ai/dnth/yolov5-deepsparse?workspace=user-dnth" target="_blank" rel="nofollow noopener noreferrer">dashboard</a> this model scores the highest <code>mAP</code> and is also the fastest.</p>
<p>It&rsquo;s getting the best of both! 🎯</p>





<figure>
    
    <a href="/portfolio/supercharging_yolov5_180_fps_cpu/mAP.png" class="image-popup">
        <img src="/portfolio/supercharging_yolov5_180_fps_cpu/mAP.png" 
             srcset="/portfolio/supercharging_yolov5_180_fps_cpu/mAP_hu17936792787062132947.png 360w, /portfolio/supercharging_yolov5_180_fps_cpu/mAP_hu7220461479128849658.png 720w, /portfolio/supercharging_yolov5_180_fps_cpu/mAP_hu5690272300255184743.png 1920w" 
             sizes="(max-width: ) 100vw, "
             
             
             
             style="max-width: 100%; height: auto;"/>
    </a>
    
</figure>


<p>I wanted to end the post here. But there is still this nagging thought that I can&rsquo;t ignore.
It&rsquo;s keeping me awake at night. So I had to do this 🤷‍♂️.</p>
<blockquote class="blockquote">
  <p class="mb-0">Every night I wonder how fast can we run YOLOv5 on CPUs? I mean the maximum possible FPS with SparseML + DeepSparse.</p>
</blockquote>
<p>This led me to 👇</p>
<h3 id="-supercharging-with-smaller-models">🚀 Supercharging with Smaller Models</h3>
<p>In the YOLOv5 series, the YOLOv5-Nano is the smallest model of all. In theory, this should be the fastest.</p>
<p>So I&rsquo;m putting my bets on this model.
Let&rsquo;s apply the same steps again with the YOLOv5-Nano model.</p>
<p>And</p>
<p>..</p>
<p>..</p>
<p>🚀🚀🚀</p>
<video controls preload="auto" width="700px"  autoplay loop muted playsinline class="html-video">
    <source src="/portfolio/supercharging_yolov5_180_fps_cpu/vids/yolov5n-pruned-quant/results_.mp4" type="video/mp4">
  <span></span>
</video>
<ul>
<li>Average FPS : 101.52</li>
<li>Average inference time (ms) : 9.84</li>
</ul>
<p>🤯 This is mindblowing! The max FPS hit the 180+ range.
I never imagine these numbers are possible, especially using only 4 CPU cores.</p>
<p>Seeing this, I can now sleep peacefully at night 😴</p>
<h3 id="-conclusion">🚧 Conclusion</h3>
<p>What a journey this has been.</p>
<p>Gone are the days when we need GPUs to run models in real-time.
With DeepSparse and SparseML, you can get GPU-class performance on commodity CPUs.</p>
<div class="notice tip" >
        <p class="first notice-title"><span class="icon-notice baseline"><svg>
                    <use href="#tip-notice"></use>
                </svg></span>tip</p><p>In this post I&rsquo;ve shown you how to:</p>
<ul>
<li>Train a SOTA YOLOv5 model with your own data.</li>
<li>Sparsify the model using SparseML quantization aware training and one-shot quantization.</li>
<li>Export the sparsified model and run it using the DeepSparse engine at insane speeds.</li>
</ul>
<p><strong>P/S</strong>: Check out codes on my GitHub <a href="https://github.com/dnth/yolov5-deepsparse-blogpost" target="_blank" rel="nofollow noopener noreferrer">repo</a>.</p></div>
<p>If you ever get lost in the commands that I used for this post, fear not.
I listed all commands I used to train all models on the <a href="https://github.com/dnth/yolov5-deepsparse-blogpost" target="_blank" rel="nofollow noopener noreferrer">README</a> of the repo.</p>
<p>Also, feel to use the repo with your own dataset and give it a ⭐ if it helps your work.</p>
<p>If you enjoyed this post, you might also like the following post where I show how to accelerate your PyTorch Image Models (TIMM) 8x faster with ONNX Runtime and TensorRT.</p>





    
    <div class="blog-page__item" style="margin-bottom: 20px;">
        <div class="blog-page__item-thumb" style="margin-bottom: 10px;">
            <a href="http://localhost:1313/portfolio/supercharge_your_pytorch_image_models/">
                <img src="http://localhost:1313/images/portfolio/supercharge_your_pytorch_image_models/post_image.png" alt="post-image" style="width: 100%; height: auto; display: block;">
            </a>
        </div>
        <span class="small" style="display: block; margin-bottom: 5px;">September 30, 2024</span>
        <h5 style="margin: 0 0 10px 0;">
            <a class="text-dark" href="http://localhost:1313/portfolio/supercharge_your_pytorch_image_models/">Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime &amp; Optimizations</a>
        </h5>
    </div>
    

<h3 id="-comments--feedback">🙏 Comments &amp; Feedback</h3>
<p>I hope you&rsquo;ve learned a thing or two from this blog post.
If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or <a href="https://dicksonneoh.com/contact/" target="_blank" rel="nofollow noopener noreferrer">drop me a message</a>.
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Anyone can train a YOLOv5 (<a href="https://twitter.com/ultralytics?ref_src=twsrc%5Etfw">@ultralytics</a>) nowadays. But deploying it on a CPU is such a PAIN.<br><br>The pain ends here.<br><br>In this🧵 I&#39;ll show you how I got insane speeds (180+ FPS) running YOLOv5 on a consumer CPU using 4 only cores🤯<br><br>🔥 P/S: I use open-source tools by <a href="https://twitter.com/neuralmagic?ref_src=twsrc%5Etfw">@neuralmagic</a>. <a href="https://t.co/b2vFOf57Ax">pic.twitter.com/b2vFOf57Ax</a></p>&mdash; Dickson Neoh 🚀 (@dicksonneoh7) <a href="https://twitter.com/dicksonneoh7/status/1534395572022480896?ref_src=twsrc%5Etfw">June 8, 2022</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</p>
<iframe src="https://www.linkedin.com/embed/feed/update/urn:li:share:6940225157286264834" height="2406" width="550" frameborder="0" allowfullscreen="" title="Embedded post"></iframe>

          





<section class="social-share">

    <ul class="share-icons">
        <hr>

        <h5>🤟 Follow me</h5>

        <p>
            Don't want to miss any of my future content? Follow me on Twitter and LinkedIn where I share these tips in
            bite-size posts.
        </p>

        
        <li>
            <a href="https://twitter.com/dicksonneoh7" target="_blank" rel="noopener" aria-label="Follow on Twitter"
                class="share-btn x">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path fill="#ffffff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
</svg>&nbsp;
                Twitter
            </a>
        </li>
        &nbsp;

        <li>
            <a href="https://www.linkedin.com/in/dickson-neoh/" target="_blank" rel="noopener"
                aria-label="Follow on LinkedIn" class="share-btn linkedin">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="linkedin_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.2264764"
     inkscape:cy="40.603642"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-129.66672,-101.87176)">
    <path
       d="m 129.66672,102.59335 v 4.90682 c 0,0.39507 0.32652,0.72159 0.72159,0.72159 h 4.90682 c 0.39507,0 0.72159,-0.32652 0.72159,-0.72159 v -4.90682 c 0,-0.39507 -0.32652,-0.72159 -0.72159,-0.72159 h -4.90682 c -0.39507,0 -0.72159,0.32652 -0.72159,0.72159 z m 5.62841,-0.14432 c 0.083,0 0.14432,0.0613 0.14432,0.14432 v 4.90682 c 0,0.083 -0.0613,0.14431 -0.14432,0.14431 h -4.90682 c -0.083,0 -0.14432,-0.0613 -0.14432,-0.14431 v -4.90682 c 0,-0.083 0.0613,-0.14432 0.14432,-0.14432 z m -4.55504,0.99219 c 0,0.2742 0.22189,0.49609 0.49609,0.49609 0.27421,0 0.4961,-0.22189 0.4961,-0.49609 0,-0.27421 -0.22189,-0.4961 -0.4961,-0.4961 -0.2742,0 -0.49609,0.22189 -0.49609,0.4961 z m 2.30007,1.26278 h -0.018 v -0.37883 h -0.81179 v 2.74204 h 0.84787 v -1.35298 c 0,-0.35719 0.0703,-0.70355 0.51413,-0.70355 0.43657,0 0.44198,0.40409 0.44198,0.72159 v 1.33494 h 0.84787 v -1.50632 c 0,-0.73783 -0.15695,-1.29886 -1.01925,-1.29886 -0.41492,0 -0.68912,0.2273 -0.80277,0.44197 z m -2.21889,2.36321 h 0.85689 v -2.74204 h -0.85689 z"
       id="path1321"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                LinkedIn
            </a>
        </li>
        &nbsp;

        <li>
            <a href="https://github.com/dnth/" target="_blank" rel="noopener" aria-label="Follow on GitHub"
                class="share-btn github">
                <svg width="24" height="24" viewBox="0 0 256 250" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid">
    <g>
        <path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#FFFFFF"></path>
    </g>
</svg>&nbsp;
                GitHub
            </a>
        </li>
        &nbsp;
        <hr>
        <h5>🔄 Share this post</h5>
        
        
        <li>
            <a href="https://twitter.com/intent/tweet?&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f&amp;text=Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU @dicksonneoh7"
                target="_blank" rel="noopener" aria-label="Share on Twitter" class="share-btn x">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path fill="#ffffff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
</svg>&nbsp;
                Twitter
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f&amp;source=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f&amp;title=Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU&amp;summary=Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU"
                target="_blank" rel="noopener" aria-label="Share on LinkedIn" class="share-btn linkedin">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="linkedin_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.2264764"
     inkscape:cy="40.603642"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-129.66672,-101.87176)">
    <path
       d="m 129.66672,102.59335 v 4.90682 c 0,0.39507 0.32652,0.72159 0.72159,0.72159 h 4.90682 c 0.39507,0 0.72159,-0.32652 0.72159,-0.72159 v -4.90682 c 0,-0.39507 -0.32652,-0.72159 -0.72159,-0.72159 h -4.90682 c -0.39507,0 -0.72159,0.32652 -0.72159,0.72159 z m 5.62841,-0.14432 c 0.083,0 0.14432,0.0613 0.14432,0.14432 v 4.90682 c 0,0.083 -0.0613,0.14431 -0.14432,0.14431 h -4.90682 c -0.083,0 -0.14432,-0.0613 -0.14432,-0.14431 v -4.90682 c 0,-0.083 0.0613,-0.14432 0.14432,-0.14432 z m -4.55504,0.99219 c 0,0.2742 0.22189,0.49609 0.49609,0.49609 0.27421,0 0.4961,-0.22189 0.4961,-0.49609 0,-0.27421 -0.22189,-0.4961 -0.4961,-0.4961 -0.2742,0 -0.49609,0.22189 -0.49609,0.4961 z m 2.30007,1.26278 h -0.018 v -0.37883 h -0.81179 v 2.74204 h 0.84787 v -1.35298 c 0,-0.35719 0.0703,-0.70355 0.51413,-0.70355 0.43657,0 0.44198,0.40409 0.44198,0.72159 v 1.33494 h 0.84787 v -1.50632 c 0,-0.73783 -0.15695,-1.29886 -1.01925,-1.29886 -0.41492,0 -0.68912,0.2273 -0.80277,0.44197 z m -2.21889,2.36321 h 0.85689 v -2.74204 h -0.85689 z"
       id="path1321"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                LinkedIn
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f" target="_blank" rel="noopener"
                aria-label="Share on Facebook" class="share-btn facebook">
                <svg
   width="6.3499999mm"
   height="6.3499999mm"
   viewBox="0 0 6.3499999 6.3499999"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="facebook_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-7.8050197"
     inkscape:cy="32.710925"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-130.36281,-104.14567)">
    <path
       d="m 130.36281,104.72294 v 5.19545 c 0,0.3157 0.26158,0.57728 0.57727,0.57728 h 5.19546 c 0.3157,0 0.57727,-0.26158 0.57727,-0.57728 v -5.19545 c 0,-0.3157 -0.26157,-0.57727 -0.57727,-0.57727 h -5.19546 c -0.31569,0 -0.57727,0.26157 -0.57727,0.57727 z m 5.77273,0 v 5.19545 h -1.4973 v -1.94829 h 0.74865 l 0.10824,-0.86591 h -0.85689 v -0.55923 c 0,-0.25256 0.0631,-0.42394 0.42393,-0.42394 h 0.46904 v -0.78473 c -0.0794,-0.0108 -0.35719,-0.0271 -0.67649,-0.0271 -0.66567,0 -1.11847,0.40048 -1.11847,1.14553 v 0.64943 h -0.75767 v 0.86591 h 0.75767 v 1.94829 h -2.79617 v -5.19545 z"
       id="path1085"
       style="stroke-width:0.0180398;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                Facebook
            </a>
        </li>
         &nbsp;

        <br>

        
        
        <li>
            <a href="https://telegram.me/share/url?text=Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU&amp;url=http%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f" target="_blank"
                rel="noopener" aria-label="Share on Telegram" class="share-btn telegram">
                <svg width="7.3503098mm"
   height="7.1592798mm" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473;" xml:space="preserve">
<g>
	<path d="M152.531,179.476c-1.48,0-2.95-0.438-4.211-1.293l-47.641-32.316l-25.552,18.386c-2.004,1.441-4.587,1.804-6.914,0.972
		c-2.324-0.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821,93.928c-2.886-1.104-4.8-3.865-4.821-6.955
		c-0.021-3.09,1.855-5.877,4.727-7.02l174.312-69.36c0.791-0.336,1.628-0.53,2.472-0.582c0.302-0.018,0.605-0.018,0.906-0.001
		c1.748,0.104,3.465,0.816,4.805,2.13c0.139,0.136,0.271,0.275,0.396,0.42c1.11,1.268,1.72,2.814,1.835,4.389
		c0.028,0.396,0.026,0.797-0.009,1.198c-0.024,0.286-0.065,0.571-0.123,0.854L159.898,173.38c-0.473,2.48-2.161,4.556-4.493,5.523
		C154.48,179.287,153.503,179.476,152.531,179.476z M104.862,130.579l42.437,28.785L170.193,39.24l-82.687,79.566l17.156,11.638
		C104.731,130.487,104.797,130.533,104.862,130.579z M69.535,124.178l5.682,21.53l12.242-8.809l-16.03-10.874
		C70.684,125.521,70.046,124.893,69.535,124.178z M28.136,86.782l31.478,12.035c2.255,0.862,3.957,2.758,4.573,5.092l3.992,15.129
		c0.183-1.745,0.974-3.387,2.259-4.624L149.227,38.6L28.136,86.782z"
		id="path1039"
       style="fill:#ffffff;stroke-width:0.0165365"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>
&nbsp;
                Telegram
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="whatsapp://send?text=Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20the%20tips%20and%20tricks%20to%20accelerate%20YOLOv5%20inference%20up%20to%20180%2b%20FPS%20on%20a%20CPU%21%20For%20free%21%0a%0ahttp%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f%0a" target="_blank" aria-label="Share on WhatsApp"
                class="share-btn whatsapp">
                <svg
   width="6.0324998mm"
   height="6.05896mm"
   viewBox="0 0 6.0324997 6.05896"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="whatsapp_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="4.9987205"
     inkscape:cy="35.692618"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-126.67735,-103.17712)">
    <path
       d="m 131.83672,104.07671 c -0.58208,-0.58209 -1.34937,-0.89959 -2.14312,-0.89959 -1.64042,0 -2.98979,1.34938 -2.98979,3.01625 0,0.52917 0.13229,1.03188 0.39687,1.48167 l -0.42333,1.56104 1.5875,-0.42333 c 0.42333,0.23812 0.92604,0.34396 1.42875,0.34396 1.66687,0 3.01625,-1.34938 3.01625,-3.01625 -0.0265,-0.74084 -0.3175,-1.50813 -0.87313,-2.06375 z m -2.14312,4.6302 c -0.44979,0 -0.87313,-0.13229 -1.27,-0.34395 l -0.10583,-0.0529 -0.92605,0.26458 0.26459,-0.89958 -0.0794,-0.13229 c -0.26458,-0.39688 -0.37041,-0.84667 -0.37041,-1.34938 0,-1.37583 1.11125,-2.48708 2.48708,-2.48708 0.66146,0 1.29646,0.26458 1.77271,0.74083 0.47625,0.47625 0.74083,1.11125 0.74083,1.77271 -0.0265,1.37583 -1.13771,2.48708 -2.51354,2.48708 z m 1.34937,-1.87854 c -0.0794,-0.0265 -0.44979,-0.23812 -0.5027,-0.26458 -0.0794,-0.0265 -0.1323,-0.0265 -0.18521,0.0265 -0.0529,0.0794 -0.21167,0.26458 -0.23813,0.29104 -0.0529,0.0529 -0.0794,0.0529 -0.15875,0.0265 -0.0794,-0.0265 -0.3175,-0.13229 -0.60854,-0.37042 -0.23812,-0.21167 -0.37042,-0.44979 -0.42333,-0.52917 -0.0529,-0.0794 0,-0.13229 0.0265,-0.15875 0.0265,-0.0264 0.0794,-0.0794 0.10583,-0.13229 0.0529,-0.0265 0.0794,-0.0794 0.10583,-0.13229 0.0265,-0.0529 0,-0.10583 0,-0.13229 0,-0.0265 -0.18521,-0.39688 -0.26458,-0.55563 -0.0265,-0.10583 -0.10583,-0.0794 -0.13229,-0.0794 h -0.15875 c 0,0 -0.10584,0.0265 -0.18521,0.0794 -0.0794,0.0794 -0.26458,0.26459 -0.26458,0.635 0,0.37042 0.26458,0.74084 0.29104,0.79375 0.0265,0.0529 0.52916,0.82021 1.29646,1.13771 0.1852,0.0794 0.3175,0.13229 0.42333,0.15875 0.18521,0.0529 0.34396,0.0529 0.47625,0.0265 0.15875,-0.0265 0.44979,-0.18521 0.50271,-0.34396 0.0529,-0.18521 0.0529,-0.3175 0.0529,-0.34396 -0.0264,-0.0794 -0.0794,-0.10583 -0.15875,-0.13229 z"
       id="path1793"
       style="stroke-width:0.264583;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                WhatsApp
            </a>
        </li>
         &nbsp;

        
        
        <li>
            <a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU.&amp;body=Supercharging%20YOLOv5%3a%20How%20I%20Got%20182.4%20FPS%20Inference%20Without%20a%20GPU%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20the%20tips%20and%20tricks%20to%20accelerate%20YOLOv5%20inference%20up%20to%20180%2b%20FPS%20on%20a%20CPU%21%20For%20free%21%0a%0ahttp%3a%2f%2flocalhost%3a1313%2fportfolio%2fsupercharging_yolov5_180_fps_cpu%2f%0a" target="_blank"
                class="share-btn email" aria-label="Share via Email">
                <svg
   width="6.3499999mm"
   height="4.3961601mm"
   viewBox="0 0 6.3499999 4.3961601"
   version="1.1"
   id="svg5"
   inkscape:version="1.1.2 (b8e25be833, 2022-02-05)"
   sodipodi:docname="email_white.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview7"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:document-units="mm"
     showgrid="false"
     inkscape:zoom="5.701459"
     inkscape:cx="-6.7526575"
     inkscape:cy="33.4125"
     inkscape:window-width="1920"
     inkscape:window-height="972"
     inkscape:window-x="1920"
     inkscape:window-y="1107"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs2" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-130.10375,-103.97942)">
    <path
       d="m 130.10375,104.22365 v 3.9077 0.24423 h 0.24423 5.86154 0.24423 v -0.24423 -3.9077 -0.24423 h -0.24423 -5.86154 -0.24423 z m 5.29675,0.24423 -2.12175,1.41196 -2.12175,-1.41196 z m -2.25913,1.91569 0.13738,0.0839 0.13738,-0.0839 2.54916,-1.70198 v 3.20553 h -5.37308 v -3.20553 z"
       id="path824"
       style="stroke-width:0.0152644;fill:#ffffff" />
  </g>
</svg>
&nbsp;
                Email
            </a>
        </li>
        <hr>

        
        <section>
            <h5>❤️ Show some love</h5>
            <p>
                Creating free ML contents doesn't pay my bills. Support me in creating more free contents like these. 
                Consider buying me a coffee. Your support means a lot to me.
            </p>
            <div style="text-align:center">
                <a href="https://www.buymeacoffee.com/dicksonneoh" target="_blank"><img
                        src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" alt="Buy Me A Coffee"
                        style="height: 60px !important;width: 217px !important;"></a>
            </div>
        </section>
        <hr>
    </ul>
</section>

        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-10 offset-lg-1">
        <nav class="case-details-nav d-flex justify-content-between align-items-start">
          
            <div class="previous">
              <div class="d-flex align-items-center mb-3">
                <div class="icon mr-3">
                  <svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285">
                    <g data-name="Group 1243" fill="#2d2d2d">
                      <path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z" />
                      <path data-name="Path 1455" d="M13.137 1.41L3.39 15.558l-.975-1.415L12.166 0z" />
                    </g>
                  </svg>
                </div>
                <span class="small">Prev blog</span>
              </div>
              <div class="blog-nav-item">
                <div class="blog-nav-thumb">
                  <a href="http://localhost:1313/portfolio/deploy_gpt_hf_models_on_telegram/">
                    <img src="http://localhost:1313/images/portfolio/deploy_gpt_hf_models_on_telegram/thumbnail.gif" alt="post-image">
                  </a>
                </div>
                <h5 class="title"><a class="text-dark" href="http://localhost:1313/portfolio/deploy_gpt_hf_models_on_telegram/">Deploying GPT-J Models on a Telegram Bot with Hugging Face Hub - For Free</a></h5>
              </div>
            </div>
          
          
            <div class="next">
              <div class="d-flex align-items-center justify-content-end mb-3">
                <span class="small">Next blog</span>
                <div class="icon ml-3">
                  <svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285">
                    <g data-name="Group 1244" fill="#2d2d2d">
                      <path data-name="Path 1456" d="M12.162 12.725L2.416 26.87l.978 1.41 9.746-14.138z" />
                      <path data-name="Path 1455" d="M2.416 1.415l9.743 14.141.975-1.414L3.39 0z" />
                    </g>
                  </svg>
                </div>
              </div>
              <div class="blog-nav-item">
                <div class="blog-nav-thumb">
                  <a href="http://localhost:1313/portfolio/fastdup_manage_clean_curate/">
                    <img src="http://localhost:1313/images/portfolio/fastdup_manage_clean_curate/thumbnail.gif" alt="post-image">
                  </a>
                </div>
                <h5 class="title"><a class="text-dark" href="http://localhost:1313/portfolio/fastdup_manage_clean_curate/">fastdup: A Powerful Tool to Manage, Clean &amp; Curate Visual Data at Scale on Your CPU - For Free.</a></h5>
              </div>
            </div>
          
        </nav>
      </div>
    </div>
  </div>
</section>


  </div>
  
  
  
  <script src="/js/toc-highlight.min.b1868ed76863ca9764b9bc4edd7ae04f53476f42dc584e1c6ad1a842c998fa42.js" integrity="sha256-sYaO12hjypdkubxO3XrgT1NHb0LcWE4catGoQsmY&#43;kI="></script>

  
  <section class="footer" id="contact">
	<div class="footer__background_shape">
		<svg viewBox="0 0 1920 79">
			<path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
		</svg>
	</div>
	<div class="container">
		<div class="row">
			<div class="col-lg-12">
				<div class="footer__cta">
					<div class="shape-1">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="shape-2">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="text-light footer__cta_content">
						<span>Contact me</span>
						<h2 class="mb-0 mb-3">Let’s Start a Project</h2>
					</div>
					<div class="footer__cta_action">
						
					</div>
					<a href="https://api.whatsapp.com/send?phone=60133250827" rel="noopener" target="_blank" class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-whatsapp"></i>&nbsp;&nbsp;Chat on WhatsApp</a>
                <a href="https://t.me/dicksonneoh" rel="noopener" target="_blank" class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-telegram"></i>&nbsp;&nbsp;Chat on Telegram</a>
				</div>
			</div>
		</div>
		<div class="row footer__widget">
			<div class="col-lg-4">
				<div class="footer__widget_logo mb-5">
					<img src="http://localhost:1313/images/site-navigation/logo_resized.png" alt="widget-logo">
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_sitemap mb-5">
					<h4 class="base-font">Sitemap</h4>
					<ul class="unstyle-list small">
						
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/#about">About me</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/">Frequently Ask Question</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/">Privacy &amp; Policy</a></li>
						
						<li class="mb-2"><a class="text-light" href="http://localhost:1313/#portfolio">Latest Article</a></li>
						
					</ul>
				</div>
			</div>
			<div class="col-lg-4">
				<div class="text-light footer__widget_address mb-5">
					<h4 class="base-font">Address</h4>
					
					<ul class="fa-ul small">
						<li class="mb-2"><a class="text-light" href="tel:&#43;%2860%29%203%208921%202020"><span class="fa-li"><i
										class="fa fa-phone"></i></span>&#43;(60) 3 8921 2020</a></li>
						<li class="mb-2"><a class="text-light" href="mailto:dickson.neoh@gmail.com"><span class="fa-li"><i
										class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li>
						<li class="mb-2">
							<span class="fa-li"><i class="fa fa-map-marker"></i></span>Kuala Lumpur, Malaysia.</a>
						</li>
					</ul>
				</div>
			</div>
		</div>
		<div class="row footer__footer">
			<div class="col-lg-6">
				<div class="footer__footer_copy text-light">
					<p>All right reserved copyright © Dickson Neoh 2024</p>
				</div>
			</div>
			<div class="col-lg-6">
				<div class="footer__footer_social">
					<ul class="unstyle-list">
						
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://www.linkedin.com/in/dickson-neoh/"><i
									class="fa fa-linkedin-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://twitter.com/dicksonneoh7"><i
									class="fa fa-twitter-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://github.com/dnth"><i
									class="fa fa-github-square"></i></a>
						</li>
						
					</ul>
				</div>
			</div>
		</div>
	</div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script>
<script src="http://localhost:1313/plugins/jQuery/jquery.min.js"></script>
<script src="http://localhost:1313/plugins/bootstrap/bootstrap.min.js"></script>
<script src="http://localhost:1313/plugins/slick/slick.min.js"></script>
<script src="http://localhost:1313/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="http://localhost:1313/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="http://localhost:1313/plugins/tweenmax/TweenMax.min.js"></script>
<script src="http://localhost:1313/plugins/imagesloaded/imagesloaded.min.js"></script>
<script src="http://localhost:1313/plugins/masonry/masonry.min.js"></script>

<script src="http://localhost:1313/js/form-handler.min.js"></script>

<script src="http://localhost:1313/js/script.min.js"></script>

</body>

</html>