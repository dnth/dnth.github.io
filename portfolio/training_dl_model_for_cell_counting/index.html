<!doctype html><html><head><meta charset=utf-8><title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</title><meta property="og:title" content="Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images"><meta property="og:description" content="Leverage hundreds state-of-the-art models on IceVision trained with best practices of Fastai"><meta property="og:type" content="article"><meta property="og:url" content="https://dicksonneoh.com/portfolio/training_dl_model_for_cell_counting/"><meta property="og:image" content="https://dicksonneoh.com/images/portfolio/training_dl_model_for_cell_counting/post_image.png"><meta property="article:section" content="portfolio"><meta property="article:published_time" content="2022-04-11T15:07:15+08:00"><meta property="article:modified_time" content="2022-04-11T15:07:15+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dicksonneoh.com/images/portfolio/training_dl_model_for_cell_counting/post_image.png"><meta name=twitter:title content="Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images"><meta name=twitter:description content="Leverage hundreds state-of-the-art models on IceVision trained with best practices of Fastai"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick-theme.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/magnafic-popup/magnific-popup.css><link href=https://dicksonneoh.com/scss/style.min.css rel=stylesheet><link rel="shortcut icon" href=https://dicksonneoh.com/images/favicon.ico type=image/x-icon><link rel=icon href=https://dicksonneoh.com/images/favicon.png type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-54500366-2')</script></head><body><nav class="navbar navbar-expand-lg fixed-top"><div class=container><a href=https://dicksonneoh.com/ class=navbar-brand><img src=https://dicksonneoh.com/images/site-navigation/logo_dn_resize.png alt=site-logo></a>
<button type=button class="navbar-toggler collapsed" data-toggle=collapse data-target=#navbarCollapse>
<span class=navbar-toggler-icon></span><span class=navbar-toggler-icon></span><span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-between" id=navbarCollapse><ul class="nav navbar-nav main-navigation my-0 mx-auto"><li class=nav-item><a href=https://dicksonneoh.com/#home class="nav-link text-dark text-sm-center p-2">Home</a></li><li class=nav-item><a href=https://dicksonneoh.com/#about class="nav-link text-dark text-sm-center p-2">About</a></li><li class=nav-item><a href=https://dicksonneoh.com/#service class="nav-link text-dark text-sm-center p-2">Services</a></li><li class=nav-item><a href=https://dicksonneoh.com/#portfolio class="nav-link text-dark text-sm-center p-2">Works</a></li><li class=nav-item><a href=https://dicksonneoh.com/#resume class="nav-link text-dark text-sm-center p-2">Resume</a></li><li class=nav-item><a href=https://dicksonneoh.com/#skills class="nav-link text-dark text-sm-center p-2">Skills</a></li><li class=nav-item><a href=https://dicksonneoh.com/#blog class="nav-link text-dark text-sm-center p-2">Blog</a></li><li class=nav-item><a href=https://dicksonneoh.com/#contact class="nav-link text-dark text-sm-center p-2">Contact</a></li></ul><div class=navbar-nav><a href=https://dicksonneoh.com/contact class="btn btn-primary btn-zoom hire_button">Hire Me Now</a></div></div></div></nav><div id=content><header class=breadCrumb><div class=container><div class=row><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><h3 class=breadCrumb__title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</h3><nav aria-label=breadcrumb class="d-flex justify-content-center"><ol class="breadcrumb align-items-center"><li class=breadcrumb-item><a href=https://dicksonneoh.com/>Home</a></li><li class=breadcrumb-item><a href=https://dicksonneoh.com/portfolio>All Post</a></li><li class="breadcrumb-item active" aria-current=page>Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</li></ol></nav></div></div><div class="row p-3"><div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center"><i class="fa fa-calendar"></i>&ensp;
April 11, 2022 &ensp; &ensp;
<i class="fa fa-clock-o"></i>&ensp;
15 mins read</div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-tag"></i>&ensp;
<a href=https://dicksonneoh.com/tags/icevision/>IceVision</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/fast.ai/>Fast.ai</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/labelimg/>labelImg</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/cell-counting/>cell-counting</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/microbiology/>microbiology</a></div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-folder"></i>&ensp;
<a href=https://dicksonneoh.com/categories/modeling/>modeling</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/categories/object-detection/>object-detection</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/categories/tutorial/>tutorial</a></div></div></div></div></div></header><section class="section singleBlog"><div class=svg-img><img src=https://dicksonneoh.com/images/hero/figure-svg.svg alt></div><div class=animate-shape><img src=https://dicksonneoh.com/images/skill/skill-background-shape.svg alt><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600"><defs><linearGradient id="d" x1=".929" y1=".111" x2=".263" y2=".935" gradientUnits="objectBoundingBox"><stop offset="0" stop-color="#f1f6f9"/><stop offset="1" stop-color="#f1f6f9" stop-opacity="0"/></linearGradient></defs><g data-name="blob-shape (3)"><path class="blob" fill="url(#d)" d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z"/></g></svg></div><div class=animate-pattern><img src=https://dicksonneoh.com/images/service/background-pattern.svg alt=background-shape></div><div class=container><div class=row><div class=col-lg-12><div class=singleBlog__feature><img src=https://dicksonneoh.com/images/portfolio/training_dl_model_for_cell_counting/post_image.png alt=feature-image></div></div></div><div class="row mt-5"><div class=col-lg-12><div class=singleBlog__content><hr><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#-motivation>üï∂Ô∏è Motivation</a></li><li><a href=#-installation>‚öôÔ∏è Installation</a></li><li><a href=#-labeling-the-data>üîñ Labeling the data</a></li><li><a href=#-modeling>üåÄ Modeling</a><ul><li><a href=#-preparing-datasets>üéØ Preparing datasets</a></li><li><a href=#-choosing-a-library-model-and-backbone>üóùÔ∏è Choosing a library, model, and backbone</a></li><li><a href=#-metrics-and-training>üèÉ Metrics and Training</a></li><li><a href=#-exporting-model>üì® Exporting model</a></li></ul></li><li><a href=#-inferencing-on-a-new-image>üß≠ Inferencing on a new image</a></li><li><a href=#-wrapping-up>üìñ Wrapping Up</a></li><li><a href=#-comments--feedback>üôè Comments & Feedback</a></li></ul></nav><hr><h3 id=-motivation>üï∂Ô∏è Motivation</h3><p>Numerous biology and medical procedures involve counting cells from images taken with a microscope.
Counting cells reveals the concentration of bacteria and viruses and gives vital information on the progress of a disease.
To accomplish the counting, researchers painstakingly count the cells by hand with the assistance of a device called <a href="https://www.youtube.com/watch?v=WWS9sZbGj6A&ab_channel=ThermoFisherScientific">hemocytometer</a>.
This process is repetitive, tedious, and prone to errors.</p><p>What if we could automate the counting by using an intelligent deep learning algorithm instead?</p><p>In this blog post, I will walk you through how to use the <a href=https://airctic.com/dev/getting_started_object_detection/>IceVision</a> library and train a state-of-the-art deep learning model with <a href=https://github.com/fastai/fastai>Fastai</a> to count microalgae cells.</p><p>Among the things you will learn:</p><ul><li>Installation of the libraries.</li><li>Prepare and label any dataset for object detection.</li><li>Train a high-performance VFNet model with IceVision & Fastai.</li><li>Use the model for inference on new images.</li></ul><p>By the end of the post, you will have an object detection model that will automatically detect microalgae cells from an image.<figure><img src=/portfolio/training_dl_model_for_cell_counting/inference.png srcset="/portfolio/training_dl_model_for_cell_counting/inference_hu4e27c33022e9aff192e501fa00651150_1733072_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/inference_hu4e27c33022e9aff192e501fa00651150_1733072_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/inference_hu4e27c33022e9aff192e501fa00651150_1733072_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>Did I mention that all the tools used in this project are completely open-source and free of charge? Yes!
If you&rsquo;re ready let&rsquo;s begin.</p><h3 id=-installation>‚öôÔ∏è Installation</h3><p>Throughout this post, we will make use a library known as <a href=https://airctic.com/0.12.0/>IceVision</a> - a computer vision-focused library built to work with <a href=https://github.com/fastai/fastai>Fastai</a>. Let&rsquo;s install them first.</p><p>There are many ways to accomplish the installation.
For your convenience, I&rsquo;ve prepared an installation script that simplifies the process into just a few lines of code.</p><p>To get started, let&rsquo;s clone the Git repository by typing the following in your terminal:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>git clone https://github.com/dnth/microalgae-cell-counter-blogpost
</code></pre></div><p>Next, navigate into the directory:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cd microalgae-cell-counter-blogpost/
</code></pre></div><p>Install IceVision and all other libraries used for this post:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>bash icevision_install.sh cuda11 0.12.0
</code></pre></div><p>Depending on your system <code>CUDA</code> version, you may want to change <code>cuda11</code> to <code>cuda10</code>, especially on older systems.
The number following the <code>CUDA</code> version is the version of IceVision.
The version I&rsquo;m using for this blog post is <code>0.12.0</code>.
You can alternatively replace the version number with <code>master</code> to install the bleeding edge version of IceVision from the master branch on Github.</p><p>If you would like to install the CPU version of the library it can be done with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>bash icevision_install.sh cpu 0.12.0
</code></pre></div><style type=text/css>.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:#fff;background:#6ab0de}.notice.warning .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info .notice-title{background:#f0b37e}.notice.info{background:#fff2db}.notice.note .notice-title{background:#6ab0de}.notice.note{background:#e7f2fa}.notice.tip .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>info</p><p>Training an object detection model on a CPU can be many times slower compared to a GPU.
If you do not have an available GPU, use <a href=https://colab.research.google.com/>Google Colab</a>.</p></div><p>The installation may take a few minutes depending on your internet connection speed.
Let the installation complete before proceeding.</p><h3 id=-labeling-the-data>üîñ Labeling the data</h3><p>All deep learning models require data to work.
To construct a model for microalgae cell counting, we require images of microalgae cells to work with.
For the purpose of this post, I&rsquo;ve acquired image samples from a lab.</p><p>The following shows a sample image of the microalgae cells as seen through a microscope.
The cells are colored green.<figure><img src=/portfolio/training_dl_model_for_cell_counting/hemocytometer.jpg srcset="/portfolio/training_dl_model_for_cell_counting/hemocytometer_hu4fbdd0c3cfa1f358477553a8b9dba9d7_69858_360x0_resize_q75_box.jpg 360w, /portfolio/training_dl_model_for_cell_counting/hemocytometer_hu4fbdd0c3cfa1f358477553a8b9dba9d7_69858_720x0_resize_q75_box.jpg 720w, /portfolio/training_dl_model_for_cell_counting/hemocytometer_hu4fbdd0c3cfa1f358477553a8b9dba9d7_69858_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" alt="Can you count how many cells are present in this image?"><figcaption><p style=text-align:center><small>Can you count how many cells are present in this image?</small></p></figcaption></figure></p><p>There are a bunch of other images in the <code>data/not_labeled/</code> folder.<figure><img src=/portfolio/training_dl_model_for_cell_counting/dataset_sample.png srcset="/portfolio/training_dl_model_for_cell_counting/dataset_sample_hu98e8c3c1e143fea035e0e34ff28651e7_150278_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/dataset_sample_hu98e8c3c1e143fea035e0e34ff28651e7_150278_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/dataset_sample_hu98e8c3c1e143fea035e0e34ff28651e7_150278_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>There is only one issue now, and that is the images are not labeled.
Let&rsquo;s label the images with bounding boxes using an open-source image labeling tool <a href=https://github.com/tzutalin/labelImg>labelImg</a>.</p><p>The <code>labelImg</code> app enables us to label images with class names and bounding boxes surrounding the object of interest.
The following figure shows a demo of the app.<figure><img src=/portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot.jpg srcset="/portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot_hu4319d5ff4a4e9c05cba253c9f6e69f4d_211897_360x0_resize_q75_box.jpg 360w, /portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot_hu4319d5ff4a4e9c05cba253c9f6e69f4d_211897_720x0_resize_q75_box.jpg 720w, /portfolio/training_dl_model_for_cell_counting/labelImg_demo_annot_hu4319d5ff4a4e9c05cba253c9f6e69f4d_211897_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>The <code>labelImg</code> app is already installed in the installation step.
To launch the app, type in your terminal:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>labelImg
</code></pre></div><p>A window like the following should appear.<figure><img src=/portfolio/training_dl_model_for_cell_counting/labelimg_start.png srcset="/portfolio/training_dl_model_for_cell_counting/labelimg_start_hua063e19da832683a0ee146d7b42e31f5_56756_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/labelimg_start_hua063e19da832683a0ee146d7b42e31f5_56756_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/labelimg_start_hua063e19da832683a0ee146d7b42e31f5_56756_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>Let&rsquo;s load the <code>data/not_labeled/</code> images folder into <code>labelImg</code> and start labeling them!
To do that, click on the <strong>Open Dir</strong> icon and navigate to the folder.</p><p>An image should now show up in <code>labelImg</code>.
To label, click on the <strong>Create RectBox</strong> icon to start drawing bounding boxes around the microalgae cells.
Next, you will be prompted to enter a label name.
Key in <code>microalgae</code> as the label name.
Once done, a rectangular bounding box should appear on-screen.</p><figure><img src=label.gif width=700></figure><p>Now comes the repetitive part, we will need to draw a bounding box for each microalgae cell for all images in the folder.
To accelerate the process I highly recommend the use of hotkeys keys with <code>labelImg</code>.
The hotkeys are shown below.<figure><img src=/portfolio/training_dl_model_for_cell_counting/hotkeys.png srcset="/portfolio/training_dl_model_for_cell_counting/hotkeys_hu9473a972c5fd24c935bd5378818c6122_49501_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/hotkeys_hu9473a972c5fd24c935bd5378818c6122_49501_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/hotkeys_hu9473a972c5fd24c935bd5378818c6122_49501_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" width=400></figure></p><p>Once done, remember to save the annotations.
The annotations are saved in an <code>XML</code> file with a file name matching to the image file name as shown below.<figure><img src=/portfolio/training_dl_model_for_cell_counting/xml_files.png srcset="/portfolio/training_dl_model_for_cell_counting/xml_files_hudf1c610607a47739799ee163ed5798e4_45665_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/xml_files_hudf1c610607a47739799ee163ed5798e4_45665_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/xml_files_hudf1c610607a47739799ee163ed5798e4_45665_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>It took me a few hours to meticulously label the images.
If you don&rsquo;t feel like spending time labeling all the images (although I recommend doing them at least once), you can find the labeled ones in the <code>data/labeled/</code> folder.</p><h3 id=-modeling>üåÄ Modeling</h3><p>Once the labeling is done, we are now ready to start modeling in a <code>jupyter</code> notebook environment.</p><p>To launch the <code>jupyter</code> notebook run the following in your terminal</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>jupyter lab
</code></pre></div><p>A browser window should pop up.
On the left pane, double click the <code>train.ipynb</code> to open the notebook.
All the codes in this section are inside the notebook.
Here, I will attempt to walk you through just enough details of the code to get you started with modeling on your own data.
If you require further clarifications, the IceVision <a href=https://airctic.com/0.12.0/>documentation</a> is a good starting point.
Or drop me a <a href=https://dicksonneoh.com/contact/>message</a>.</p><p>The first cell in the notebook is the imports.
With IceVision all the necessary components are imported with one line of code:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</code></pre></div><p>If something wasn&rsquo;t properly installed, the imports will raise an error message.
In that event, you must go back to the installation step before proceeding.
If there are no errors, we are ready to dive in further.</p><h4 id=-preparing-datasets>üéØ Preparing datasets</h4><p>After the imports, we must now load the labeled images and bounding boxes into <code>jupyter</code>.
This is also known as <em>data parsing</em> and is accomplished with the following:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>parser <span style=color:#f92672>=</span> parsers<span style=color:#f92672>.</span>VOCBBoxParser(annotations_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;data/labeled&#34;</span>, images_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;data/labeled&#34;</span>)
</code></pre></div><p>The parameter <code>annotations_dir</code> and <code>images_dir</code> are the directories to the images and annotations respectively.
Since both the images and annotations are located in the same directory, they are the same as such in the code.</p><p>Next, we will randomly pick and divide the images and bounding boxes into two groups of data namely <code>train_records</code> and <code>valid_records</code>.
By default, the split will be <code>80:20</code> to <code>train:valid</code> proportion.
You can change the ratio by altering the values in <code>RandomSplitter</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_records, valid_records <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse(data_splitter<span style=color:#f92672>=</span>RandomSplitter([<span style=color:#ae81ff>0.8</span>, <span style=color:#ae81ff>0.2</span>])
</code></pre></div><p>The following code shows the class names from the parsed data:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>parser<span style=color:#f92672>.</span>class_map
</code></pre></div><p>It should output:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>&lt;</span>ClassMap: {<span style=color:#e6db74>&#39;background&#39;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#39;Microalgae&#39;</span>: <span style=color:#ae81ff>1</span>}<span style=color:#f92672>&gt;</span>
</code></pre></div><p>which shows a <code>ClassMap</code> that contains the class name as the key and class index as the value in a Python <a href=https://www.w3schools.com/python/python_dictionaries.asp>dictionary</a>.
The <code>background</code> class is automatically added.
In the data labeling step, we do not need to label the background.</p><p>Next, we will apply basic data augmentation which is a technique used to diversify the training images by applying the random transformation.
Learn more <a href=https://medium.com/analytics-vidhya/image-augmentation-9b7be3972e27>here</a>.</p><p>The following code specifies the kinds of transformations we would like to perform on our images.
Behind the scenes, these transformations are performed with the <a href=https://albumentations.ai/>Albumentations</a> library.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>image_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>640</span>
train_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>aug_tfms(size<span style=color:#f92672>=</span>image_size, presize<span style=color:#f92672>=</span>image_size<span style=color:#f92672>+</span><span style=color:#ae81ff>128</span>), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
valid_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>resize_and_pad(image_size), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
</code></pre></div><p>We must specify the dimensions of the image in <code>image_size = 640</code>.
This value will then be used in <code>tfms.A.aug_tfms</code> that ensures that all images are resized to a <code>640x640</code> resolution and normalized in <code>tfms.A.Normalize()</code>.</p><p>Some models like <code>EfficientDet</code> only work with image size divisible by <code>128</code>.
Other common values you can try are <code>384</code>, <code>512</code>, <code>768</code>, etc.
But beware using a large image size may consume more memory and in some cases halts training.
Starting with a small value like <code>384</code> is probably a good idea.
I found <code>640</code> works best for this dataset.</p><p>Use <code>tfms.A.aug_tfms</code> performs transformations to the image such as varying the lighting, rotation, shifting, flipping, blurring, padding, etc.
The full list of transforms and the parameters can be found in the <code>aug_tfms</code> <a href=https://airctic.com/0.12.0/albumentations_tfms/>documentation</a>.</p><p>In this code snippet, we created two distinct transforms namely <code>train_tfms</code> and <code>valid_tfms</code> that will be used during the training and validation steps respectively.</p><p>Next, we will apply the <code>train_tfms</code> to our <code>train_records</code> and <code>valid_tfms</code> to <code>valid_records</code> with the following snippet.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_ds <span style=color:#f92672>=</span> Dataset(train_records, train_tfms)
valid_ds <span style=color:#f92672>=</span> Dataset(valid_records, valid_tfms)
</code></pre></div><p>This results in the creation of a <code>Dataset</code> object which is a collection of transformed images and bounding boxes.</p><p>To visualize the <code>train_ds</code> we can run:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>samples <span style=color:#f92672>=</span> [train_ds[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
show_samples(samples, ncols<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</code></pre></div><p>This will show us 4 samples from the <code>train_ds</code>.
Note the variations in lighting, translation, and rotation compared to the original images.<figure><img src=/portfolio/training_dl_model_for_cell_counting/show_ds.png srcset="/portfolio/training_dl_model_for_cell_counting/show_ds_hu987ace790d19d4102da18bd5db01d3ee_648447_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/show_ds_hu987ace790d19d4102da18bd5db01d3ee_648447_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/show_ds_hu987ace790d19d4102da18bd5db01d3ee_648447_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>The transformations are applied on the fly.
So each run on the snippet produces slightly different results.</p><h4 id=-choosing-a-library-model-and-backbone>üóùÔ∏è Choosing a library, model, and backbone</h4><p>IceVision supports hundreds of high-quality pre-trained models from <a href=https://github.com/pytorch/vision>Torchvision</a>, Open MMLab&rsquo;s <a href=https://github.com/open-mmlab/mmdetection>MMDetection</a>, Ultralytic&rsquo;s <a href=https://github.com/ultralytics/yolov5>YOLOv5</a> and Ross Wightman&rsquo;s <a href=https://github.com/rwightman/efficientdet-pytorch>EfficientDet</a>.</p><p>Depending on your preference, you may choose the model and backbone from these libraries.
In this post I will choose the <a href=https://arxiv.org/abs/2008.13367>VarifocalNet</a> (VFNet) model from MMDetection which can be accomplished with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_type <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>mmdet<span style=color:#f92672>.</span>vfnet
backbone <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>backbones<span style=color:#f92672>.</span>resnet50_fpn_mstrain_2x
model <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>model(backbone<span style=color:#f92672>=</span>backbone(pretrained<span style=color:#f92672>=</span>True), num_classes<span style=color:#f92672>=</span>len(parser<span style=color:#f92672>.</span>class_map)) 
</code></pre></div><p>There are various ResNet backbones that you can select from such as
<code>resnet50_fpn_1x</code>,
<code>resnet50_fpn_mstrain_2x</code>,
<code>resnet50_fpn_mdconv_c3_c5_mstrain_2x</code>,
<code>resnet101_fpn_1x</code>,
<code>resnet101_fpn_mstrain_2x</code>,
<code>resnet101_fpn_mdconv_c3_c5_mstrain_2x</code>,
<code>resnext101_32x4d_fpn_mdconv_c3_c5_mstrain_2x</code>, and
<code>resnext101_64x4d_fpn_mdconv_c3_c5_mstrain_2x</code>.</p><p>Additionally, IceVision also recently supports state-of-the-art Swin Transformer backbone for the VFNet model
<code>swin_t_p4_w7_fpn_1x_coco</code>,
<code>swin_s_p4_w7_fpn_1x_coco</code>, and
<code>swin_b_p4_w7_fpn_1x_coco</code>.</p><p>Which combination of <code>model_type</code> and <code>backbone</code> that performs best is something you need to experiment with.
Feel free to experiment and swap out the backbone and note the performance of the model.
There are other model types with their respective backbones which you can find <a href=https://github.com/airctic/icevision/blob/master/notebooks/getting_started_object_detection.ipynb>here</a>.</p><h4 id=-metrics-and-training>üèÉ Metrics and Training</h4><p>To start the training, the model needs to take in the images and bounding boxes from the <code>train_ds</code> and <code>valid_ds</code> we created.</p><p>For that, we will need to use a dataloader which will help us iterate over the elements in the dataset we created and load them into the model.
We will construct two separate dataloaders for <code>train_ds</code> and <code>valid_ds</code> respectively.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>train_dl(train_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>True)
valid_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>valid_dl(valid_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>False)
</code></pre></div><p>Here, we can specify the <code>batch_size</code> parameter which is the number of images and bounding boxes given to the model in a single forward pass.
The <code>shuffle</code> parameter specifies if you would like to randomly shuffle the order of the data.
The <code>num_workers</code> parameter specifies how many sub-processes to use to load the data.
Let&rsquo;s keep it at <code>4</code> for now.</p><p>Next, we need to specify a measure of how well our model performs during training.
This measure is specified using a metric - which involves using specific math equations to output a score that tells us if the model is improving or not during training.
Some commonly used metrics include accuracy, error rate, F1 Score, etc.
For object detection tasks the <code>COCOMetric</code> is commonly used.
If you are interested <a href=https://blog.zenggyu.com/en/post/2018-12-16/an-introduction-to-evaluation-metrics-for-object-detection/>this blog</a> explains the math behind the metrics used for object detection.</p><p>Once the metric is defined, we can then load all three components - dataloaders, model, and metric into a Fastai <code>Learner</code> for training.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>metrics <span style=color:#f92672>=</span> [COCOMetric(metric_type<span style=color:#f92672>=</span>COCOMetricType<span style=color:#f92672>.</span>bbox)]
learn <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>fastai<span style=color:#f92672>.</span>learner(dls<span style=color:#f92672>=</span>[train_dl, valid_dl], model<span style=color:#f92672>=</span>model, metrics<span style=color:#f92672>=</span>metrics)
</code></pre></div><p>With deep learning models, there are many hyperparameters that we can configure before we run the training.
One of the most important hyperparameters to get right is the learning rate.
Since IceVision is built to work with Fastai, we have access to a handy tool known as the learning rate finder first proposed by <a href=https://arxiv.org/abs/1506.01186>Leslie Smith</a> and popularized by the Fastai community for its effectiveness.
This is an incredibly simple yet powerful tool to find a range of optimal learning rate values that gives us the best training performance.</p><p>All we need to do is run:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>learn<span style=color:#f92672>.</span>lr_find()
</code></pre></div><p>which outputs:<figure><img src=/portfolio/training_dl_model_for_cell_counting/lr_find.png srcset="/portfolio/training_dl_model_for_cell_counting/lr_find_hu81817669b24bf48609ed9305c3f743a2_85081_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/lr_find_hu81817669b24bf48609ed9305c3f743a2_85081_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/lr_find_hu81817669b24bf48609ed9305c3f743a2_85081_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>The most optimal learning rate value lies in the region where the loss descends most rapidly.
From the figure above, this is somewhere in between <code>1e-4</code> to <code>1e-2</code>.
The orange dot on the plot shows the point where the slope is the steepest and is generally a good value to use as the learning rate.</p><p>Now, let&rsquo;s load this learning rate value of 1e-3 into the <code>fine_tune</code> function and start training.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>1e-3</span>, freeze_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</code></pre></div><p>The first parameter in <code>fine_tune</code> is the number of epochs to train for.
One epoch is defined as a complete iteration over the entire dataset.
In this post, I will only train for 10 epochs.
Training for longer will likely improve the model, so I will leave that to you to experiment with.
The second parameter is the learning rate value we wish to use to train the model.
Let&rsquo;s put the value <code>1e-3</code> from the learning rate finder.</p><p>The above code snippet trains the model for 10 epochs.
By default, this will start the training in two phases.</p><p>In the first phase ‚ûÄ, only the last layer of the model is trained.
The rest of the model is frozen.
In the second phase ‚ûÅ, the entire model is trained end-to-end.
The figure below shows the training output.</p><p>The <code>freeze_epochs</code> parameter specifies the number of <code>epochs</code> to train in ‚ûÄ.</p><figure><img src=/portfolio/training_dl_model_for_cell_counting/train.png srcset="/portfolio/training_dl_model_for_cell_counting/train_huf98f947799718d6575435777906b1a39_139816_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/train_huf98f947799718d6575435777906b1a39_139816_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/train_huf98f947799718d6575435777906b1a39_139816_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><p>During the training, the <code>train_loss</code>, <code>valid_loss</code>, and <code>COCOMetric</code> are printed at every epoch.
Ideally, the losses should decrease, and <code>COCOMetric</code> should increase the longer we train.
As shown above, each epoch only took 2 seconds to complete on a GPU - which is incredibly fast.</p><p>Once the training completes, we can view the performance of the model by showing the inference results on <code>valid_ds</code>.
The following figure shows the output at a detection threshold of <code>0.5</code>.
You can increase the <code>detection_threshold</code> value to only show the bounding boxes with a higher confidence value.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_type<span style=color:#f92672>.</span>show_results(model, valid_ds, detection_threshold<span style=color:#f92672>=.</span><span style=color:#ae81ff>5</span>)
</code></pre></div><figure><img src=/portfolio/training_dl_model_for_cell_counting/show_results.png srcset="/portfolio/training_dl_model_for_cell_counting/show_results_huccd0cb8b6eb6e7b65df90b17e113aa5d_298745_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/show_results_huccd0cb8b6eb6e7b65df90b17e113aa5d_298745_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/show_results_huccd0cb8b6eb6e7b65df90b17e113aa5d_298745_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><p>For completeness, here are the codes for the <em>Modeling</em> section which include steps to load the data, instantiate the model, train the model, and show the results.
That&rsquo;s only 17 lines of code!</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>

parser <span style=color:#f92672>=</span> parsers<span style=color:#f92672>.</span>VOCBBoxParser(annotations_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;data/labeled&#34;</span>, images_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;data/labeled&#34;</span>)
train_records, valid_records <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse()

image_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>640</span>
train_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>aug_tfms(size<span style=color:#f92672>=</span>image_size, presize<span style=color:#f92672>=</span>image_size<span style=color:#f92672>+</span><span style=color:#ae81ff>128</span>), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
valid_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>resize_and_pad(image_size), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])

train_ds <span style=color:#f92672>=</span> Dataset(train_records, train_tfms)
valid_ds <span style=color:#f92672>=</span> Dataset(valid_records, valid_tfms)

model_type <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>mmdet<span style=color:#f92672>.</span>vfnet
backbone <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>backbones<span style=color:#f92672>.</span>resnet50_fpn_mstrain_2x
model <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>model(backbone<span style=color:#f92672>=</span>backbone(pretrained<span style=color:#f92672>=</span>True), num_classes<span style=color:#f92672>=</span>len(parser<span style=color:#f92672>.</span>class_map)) 

train_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>train_dl(train_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>True)
valid_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>valid_dl(valid_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>False)

metrics <span style=color:#f92672>=</span> [COCOMetric(metric_type<span style=color:#f92672>=</span>COCOMetricType<span style=color:#f92672>.</span>bbox)]
learn <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>fastai<span style=color:#f92672>.</span>learner(dls<span style=color:#f92672>=</span>[train_dl, valid_dl], model<span style=color:#f92672>=</span>model, metrics<span style=color:#f92672>=</span>metrics)
learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>1e-3</span>, freeze_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)

model_type<span style=color:#f92672>.</span>show_results(model, valid_ds, detection_threshold<span style=color:#f92672>=.</span><span style=color:#ae81ff>5</span>)
</code></pre></td></tr></table></div></div><h4 id=-exporting-model>üì® Exporting model</h4><p>Once you are satisfied with the performance and quality of the model, we can export all the model configurations (hyperparameters) and weights (parameters) for future use.</p><p>The following code packages the model into a checkpoint and exports it into a local directory.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.models.checkpoint <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
save_icevision_checkpoint(model,
                        model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mmdet.vfnet&#39;</span>, 
                        backbone_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;resnet50_fpn_mstrain_2x&#39;</span>,
                        img_size<span style=color:#f92672>=</span><span style=color:#ae81ff>640</span>,
                        classes<span style=color:#f92672>=</span>parser<span style=color:#f92672>.</span>class_map<span style=color:#f92672>.</span>get_classes(),
                        filename<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./models/model_checkpoint.pth&#39;</span>,
                        meta<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;icevision_version&#39;</span>: <span style=color:#e6db74>&#39;0.12.0&#39;</span>})
</code></pre></div><p>The parameters <code>model_name</code>, <code>backbone_name</code>, and <code>img_size</code> have to match what we used during training.</p><p><code>filename</code> specifies the directory and name of the checkpoint file.</p><p><code>meta</code> is an optional parameter you can use to save all other information about the model.</p><p>Once completed the checkpoint should be saved in the <code>models/</code> folder. We can now use this checkpoint independently outside of the training notebook.</p><h3 id=-inferencing-on-a-new-image>üß≠ Inferencing on a new image</h3><p>To demonstrate that the model checkpoint file can be loaded independently, I created another notebook with the name <code>inference.ipynb</code>.
In this notebook, we are going to load the checkpoint and use it for inference on a brand new image.</p><p>Let&rsquo;s import all the necessary packages:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
<span style=color:#f92672>from</span> icevision.models.checkpoint <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
<span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</code></pre></div><p>And specify the checkpoint path.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>checkpoint_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;./models/model_checkpoint.pth&#34;</span>
</code></pre></div><p>We can load the checkpoint with the function <code>model_from_checkpoint</code>.
From the checkpoint, we can retrieve all other configurations such as the model type, class map, image size, and the transforms.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>checkpoint_and_model <span style=color:#f92672>=</span> model_from_checkpoint(checkpoint_path)
model <span style=color:#f92672>=</span> checkpoint_and_model[<span style=color:#e6db74>&#34;model&#34;</span>]
model_type <span style=color:#f92672>=</span> checkpoint_and_model[<span style=color:#e6db74>&#34;model_type&#34;</span>]
class_map <span style=color:#f92672>=</span> checkpoint_and_model[<span style=color:#e6db74>&#34;class_map&#34;</span>]
img_size <span style=color:#f92672>=</span> checkpoint_and_model[<span style=color:#e6db74>&#34;img_size&#34;</span>]
valid_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>resize_and_pad(img_size), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
</code></pre></div><p>The model is now ready for inference.
Let&rsquo;s try to load an image with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(<span style=color:#e6db74>&#39;data/not_labeled/IMG_20191203_164256.jpg&#39;</span>)
</code></pre></div><p>We can pass the image into the <code>end2end_detect</code> function to run the inference.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pred_dict <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>end2end_detect(img, valid_tfms, model, 
                                      class_map<span style=color:#f92672>=</span>class_map, 
                                      detection_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,
                                      display_label<span style=color:#f92672>=</span>True, 
                                      display_bbox<span style=color:#f92672>=</span>True, 
                                      return_img<span style=color:#f92672>=</span>True, 
                                      font_size<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>, 
                                      label_color<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;#FF59D6&#34;</span>)
</code></pre></div><p>The output <code>pred_dict</code> is a Python dictionary.
To view the output image with the bounding boxes:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pred_dict[<span style=color:#e6db74>&#34;img&#34;</span>]
</code></pre></div><p>which outputs</p><figure><img src=/portfolio/training_dl_model_for_cell_counting/inference.png srcset="/portfolio/training_dl_model_for_cell_counting/inference_hu4e27c33022e9aff192e501fa00651150_1733072_360x0_resize_box_2.png 360w, /portfolio/training_dl_model_for_cell_counting/inference_hu4e27c33022e9aff192e501fa00651150_1733072_720x0_resize_box_2.png 720w, /portfolio/training_dl_model_for_cell_counting/inference_hu4e27c33022e9aff192e501fa00651150_1733072_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><p>To count the number of microalgae cells on the image, we can count the number of bounding boxes on the image by with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>len(pred_dict[<span style=color:#e6db74>&#39;detection&#39;</span>][<span style=color:#e6db74>&#39;bboxes&#39;</span>])
</code></pre></div><p>which outputs <code>29</code> on my computer.</p><p>To save the image with the bounding boxes, you can run:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pred_dict[<span style=color:#e6db74>&#34;img&#34;</span>]<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;inference.png&#34;</span>)
</code></pre></div><p>As you can see, there are some missed detections of the microalgae cells.
But, considering this is our first try, and we only trained for 10 epochs (which took less than 30 seconds to complete), this is an astonishing feat!
Additionally, I&rsquo;ve only used 17 labeled images to train the model.</p><p>In this post, I&rsquo;ve demonstrated that we can train a sophisticated object detection model with only a few images in a very short time.
This outstanding feat is possible thanks to the Fastai library which incorporated all the best practices in training deep learning models.</p><p>At this point, we have not even tuned any hyperparameters (other than learning rate) to optimize performance.
Most hyperparameters are default values in Fastai that worked extremely well out-of-the-box with this dataset and model.</p><p>To improve performance, you may want to experiment by labeling more data and adjusting a few other hyperparameters such as image size, batch size, training epochs, the ratio of training/validation split, different model types, and backbones.</p><h3 id=-wrapping-up>üìñ Wrapping Up</h3><p>Congratulations on making it through this post! It wasn&rsquo;t that hard right?
Hopefully, this post also boosted your confidence that object detection is not as hard as it used to be.
With many high-level open-source packages like IceVision and Fastai, anyone with a computer and a little patience can break into object detection.</p><p>In this post, I&rsquo;ve shown you how you can construct a model that detects microalgae cells.
In reality, the same steps can be used to detect any other cells or any other objects for that matter.
Realizing this is an extremely powerful paradigm shift for me.</p><p>Think about all the problems we can solve by accurately detecting specific objects. Detecting intruders, detecting dangerous objects such as a gun, detecting defects on a production line, detecting smoke/fire, detecting skin cancer, detecting plant disease, and so much more.</p><p>Your creativity and imagination are the limits.
The world is your oyster. Now go out there and use this newly found superpower to make a difference.</p><figure><img src=/portfolio/training_dl_model_for_cell_counting/quote_robert_greene.jpg srcset="/portfolio/training_dl_model_for_cell_counting/quote_robert_greene_hue06465eaba1992211db36b52a131aa5e_64837_360x0_resize_q75_box.jpg 360w, /portfolio/training_dl_model_for_cell_counting/quote_robert_greene_hue06465eaba1992211db36b52a131aa5e_64837_720x0_resize_q75_box.jpg 720w, /portfolio/training_dl_model_for_cell_counting/quote_robert_greene_hue06465eaba1992211db36b52a131aa5e_64837_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>All the codes and data are available on this Github <a href=https://github.com/dnth/microalgae-cell-counter-blogpost>repository</a>.</p></div><h3 id=-comments--feedback>üôè Comments & Feedback</h3><p>If you find this useful, or if you have any questions, comments, or feedback, I would be grateful if you can leave them on the following Twitter post or <a href=https://dicksonneoh.com/contact/>drop me a message</a>.<blockquote class=twitter-tweet><p lang=en dir=ltr>Sharing a blog post on how I trained a deep learning model to count microalgae cells in 17 lines of code with 17 labeled images using IceVision + <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a> <a href="https://twitter.com/hashtag/machinelearning?src=hash&ref_src=twsrc%5Etfw">#machinelearning</a> <a href="https://twitter.com/hashtag/Microbiology?src=hash&ref_src=twsrc%5Etfw">#Microbiology</a> <a href="https://twitter.com/hashtag/DataScientist?src=hash&ref_src=twsrc%5Etfw">#DataScientist</a> <a href="https://twitter.com/hashtag/pythonprogramming?src=hash&ref_src=twsrc%5Etfw">#pythonprogramming</a> <a href="https://twitter.com/hashtag/innovation?src=hash&ref_src=twsrc%5Etfw">#innovation</a> <a href="https://twitter.com/hashtag/CellBiology?src=hash&ref_src=twsrc%5Etfw">#CellBiology</a><a href=https://t.co/AcEtmLS0C9>https://t.co/AcEtmLS0C9</a></p>&mdash; Dickson Neoh (@dicksonneoh7) <a href="https://twitter.com/dicksonneoh7/status/1513478343726809090?ref_src=twsrc%5Etfw">April 11, 2022</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></p><p>So what&rsquo;s next? If you are interested to learn how I deploy this model on Android checkout this <a href=https://dicksonneoh.com/portfolio/microsense_a_deep_learning_powered_cell_counting_app_with_flutter/>post</a>.</p><section class=social-share><ul class=share-icons><hr><h5>Share this post</h5>><li><a href="https://twitter.com/intent/tweet?&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f&text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images" target=_blank rel=noopener aria-label="Share on Twitter" class="share-btn twitter"><svg width="6.3503098mm" height="5.1592798mm" viewBox="0 0 6.3503098 5.1592799" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="twitter_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-38.14813" inkscape:cy="32.360138" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-125.63688,-104.07795)"><path d="m131.98686 104.68815c-.23316.10418-.48452.17363-.74745.20505.26955-.1604.47625-.41672.57216-.72099-.25135.14883-.53082.25797-.82682.31585-.23812-.25136-.57712-.41011-.95084-.41011-.71934.0-1.30308.58374-1.30308 1.30307.0.10253.0116.20175.0331.29601-1.08314-.0546-2.04226-.57216-2.68553-1.36095-.11079.19182-.17528.41672-.17528.65484.0.45145.22985.84997.57877 1.08479-.21332-.007-.41506-.0661-.59035-.16371v.0165c0 .63169.44979 1.15755 1.04511 1.27661-.10914.0298-.2249.0463-.34396.0463-.0843.0-.16537-.008-.24474-.0232.16536.51759.64657.89463 1.21708.90455-.44648.34892-1.00707.55728-1.61726.55728-.10584.0-.20836-.007-.31089-.0182.57712.37041 1.26173.58539 1.9976.58539 2.39614.0 3.70583-1.98438 3.70583-3.70582.0-.0562-.002-.11245-.003-.16868.25466-.18355.47459-.41341.64988-.67468z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g></svg><p>Twitter</p></a></li>&nbsp;<li><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f" target=_blank rel=noopener aria-label="Share on Facebook" class="share-btn facebook"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="facebook_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-7.8050197" inkscape:cy="32.710925" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.36281,-104.14567)"><path d="m130.36281 104.72294v5.19545c0 .3157.26158.57728.57727.57728h5.19546c.3157.0.57727-.26158.57727-.57728v-5.19545c0-.3157-.26157-.57727-.57727-.57727h-5.19546c-.31569.0-.57727.26157-.57727.57727zm5.77273.0v5.19545h-1.4973v-1.94829h.74865l.10824-.86591h-.85689v-.55923c0-.25256.0631-.42394.42393-.42394h.46904v-.78473c-.0794-.0108-.35719-.0271-.67649-.0271-.66567.0-1.11847.40048-1.11847 1.14553v.64943h-.75767v.86591h.75767v1.94829h-2.79617v-5.19545z" id="path1085" style="stroke-width:.0180398;fill:#fff"/></g></svg><p>Facebook</p></a></li>&nbsp;<li><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f&source=https%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f&title=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images&summary=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images" target=_blank rel=noopener aria-label="Share on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg><p>LinkedIn</p></a></li>&nbsp;<li><a href="https://telegram.me/share/url?text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f" target=_blank rel=noopener aria-label="Share on Telegram" class="share-btn telegram"><svg width="7.3503098mm" height="7.1592798mm" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473"><g><path d="M152.531 179.476c-1.48.0-2.95-.438-4.211-1.293l-47.641-32.316-25.552 18.386c-2.004 1.441-4.587 1.804-6.914.972-2.324-.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821 93.928c-2.886-1.104-4.8-3.865-4.821-6.955-.021-3.09 1.855-5.877 4.727-7.02l174.312-69.36c.791-.336 1.628-.53 2.472-.582.302-.018.605-.018.906-.001 1.748.104 3.465.816 4.805 2.13.139.136.271.275.396.42 1.11 1.268 1.72 2.814 1.835 4.389.028.396.026.797-.009 1.198-.024.286-.065.571-.123.854L159.898 173.38c-.473 2.48-2.161 4.556-4.493 5.523C154.48 179.287 153.503 179.476 152.531 179.476zm-47.669-48.897 42.437 28.785L170.193 39.24l-82.687 79.566 17.156 11.638C104.731 130.487 104.797 130.533 104.862 130.579zM69.535 124.178l5.682 21.53 12.242-8.809-16.03-10.874C70.684 125.521 70.046 124.893 69.535 124.178zM28.136 86.782l31.478 12.035c2.255.862 3.957 2.758 4.573 5.092l3.992 15.129c.183-1.745.974-3.387 2.259-4.624L149.227 38.6 28.136 86.782z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg><p>Twitter</p></a></li>&nbsp;<li><a href="whatsapp://send?text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLeverage%20hundreds%20state-of-the-art%20models%20on%20IceVision%20trained%20with%20best%20practices%20of%20Fastai%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f%0a" target=_blank aria-label="Share on WhatsApp" class="share-btn whatsapp"><svg width="6.0324998mm" height="6.05896mm" viewBox="0 0 6.0324997 6.05896" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="whatsapp_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="4.9987205" inkscape:cy="35.692618" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-126.67735,-103.17712)"><path d="m131.83672 104.07671c-.58208-.58209-1.34937-.89959-2.14312-.89959-1.64042.0-2.98979 1.34938-2.98979 3.01625.0.52917.13229 1.03188.39687 1.48167l-.42333 1.56104 1.5875-.42333c.42333.23812.92604.34396 1.42875.34396 1.66687.0 3.01625-1.34938 3.01625-3.01625-.0265-.74084-.3175-1.50813-.87313-2.06375zm-2.14312 4.6302c-.44979.0-.87313-.13229-1.27-.34395l-.10583-.0529-.92605.26458.26459-.89958-.0794-.13229c-.26458-.39688-.37041-.84667-.37041-1.34938.0-1.37583 1.11125-2.48708 2.48708-2.48708.66146.0 1.29646.26458 1.77271.74083.47625.47625.74083 1.11125.74083 1.77271-.0265 1.37583-1.13771 2.48708-2.51354 2.48708zm1.34937-1.87854c-.0794-.0265-.44979-.23812-.5027-.26458-.0794-.0265-.1323-.0265-.18521.0265-.0529.0794-.21167.26458-.23813.29104-.0529.0529-.0794.0529-.15875.0265-.0794-.0265-.3175-.13229-.60854-.37042-.23812-.21167-.37042-.44979-.42333-.52917-.0529-.0794.0-.13229.0265-.15875.0265-.0264.0794-.0794.10583-.13229.0529-.0265.0794-.0794.10583-.13229.0265-.0529.0-.10583.0-.13229.0-.0265-.18521-.39688-.26458-.55563-.0265-.10583-.10583-.0794-.13229-.0794h-.15875s-.10584.0265-.18521.0794c-.0794.0794-.26458.26459-.26458.635.0.37042.26458.74084.29104.79375.0265.0529.52916.82021 1.29646 1.13771.1852.0794.3175.13229.42333.15875.18521.0529.34396.0529.47625.0265.15875-.0265.44979-.18521.50271-.34396.0529-.18521.0529-.3175.0529-.34396-.0264-.0794-.0794-.10583-.15875-.13229z" id="path1793" style="stroke-width:.264583;fill:#fff"/></g></svg><p>Email</p></a></li>&nbsp;<li><a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images.&body=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%20with%2017%20Images%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLeverage%20hundreds%20state-of-the-art%20models%20on%20IceVision%20trained%20with%20best%20practices%20of%20Fastai%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2ftraining_dl_model_for_cell_counting%2f%0a" target=_blank class="share-btn email" aria-label="Share via Email"><svg width="6.3499999mm" height="4.3961601mm" viewBox="0 0 6.3499999 4.3961601" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="email_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.7526575" inkscape:cy="33.4125" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.10375,-103.97942)"><path d="m130.10375 104.22365v3.9077.24423h.24423 5.86154.24423v-.24423-3.9077-.24423h-.24423-5.86154-.24423zm5.29675.24423-2.12175 1.41196-2.12175-1.41196zm-2.25913 1.91569.13738.0839.13738-.0839 2.54916-1.70198v3.20553h-5.37308v-3.20553z" id="path824" style="stroke-width:.0152644;fill:#fff"/></g></svg><p>Email</p></a></li><hr></ul></section></div></div></div><div class=row><div class="col-lg-10 offset-lg-1"><nav class="case-details-nav d-flex justify-content-between align-items-center"><div class="previous d-flex align-items-center"><div class="icon mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1243" fill="#2d2d2d"><path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z"/><path data-name="Path 1455" d="M13.137 1.41 3.39 15.558l-.975-1.415L12.166.0z"/></g></svg></div><div class=content><span class=small>Prev blog</span><h5 class=title><a class=text-dark href=https://dicksonneoh.com/portfolio/realtime_vehicle_and_license_plate_detection_with_openvino/>Real-time Vehicle and License Plate Detection with Tensorflow and OpenVINO</a></h5></div></div><div class="px-4 bg-primary"></div><div class="next d-flex align-items-center"><div class="content text-right"><span class=small>Next blog</span><h5 class=title><a class=text-dark href=https://dicksonneoh.com/portfolio/microsense_a_deep_learning_powered_cell_counting_app_with_flutter/>[WIP] Deploying Object Detection Models on Mobile Devices with Flutter for Busy Engineers</a></h5></div><div class="icon ml-3"><svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1244" fill="#2d2d2d"><path data-name="Path 1456" d="M12.162 12.725 2.416 26.87l.978 1.41 9.746-14.138z"/><path data-name="Path 1455" d="M2.416 1.415l9.743 14.141.975-1.414L3.39.0z"/></g></svg></div></div></nav></div></div></div></section></div><section class=footer id=contact><div class=footer__background_shape><svg viewBox="0 0 1920 79"><path d="M0 0h1920v79L0 0z" data-name="Path 1450"/></svg></div><div class=container><div class=row><div class=col-lg-12><div class=footer__cta><div class=shape-1><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class=shape-2><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class="text-light footer__cta_content"><span>Contact me</span><h2 class=mb-0>Let‚Äôs Start a Project</h2></div><div class=footer__cta_action><a class="btn btn-light btn-zoom" href=https://dicksonneoh.com/contact>Get in
touch</a></div></div></div></div><div class="row footer__widget"><div class=col-lg-4><div class="footer__widget_logo mb-5"><img src=https://dicksonneoh.com/images/site-navigation/logo_dn_resize.png alt=widget-logo></div></div><div class=col-lg-4><div class="text-light footer__widget_sitemap mb-5"><h4 class=base-font>Sitemap</h4><ul class="unstyle-list small"><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>About me</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Frequently Ask Question</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Privacy & Policy</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Latest Article</a></li></ul></div></div><div class=col-lg-4><div class="text-light footer__widget_address mb-5"><h4 class=base-font>Address</h4><ul class="fa-ul small"><li class=mb-2><a class=text-light href=tel:+%2860%29%203%208921%202020><span class=fa-li><i class="fa fa-phone"></i></span>+(60) 3 8921 2020</a></li><li class=mb-2><a class=text-light href=mailto:dickson.neoh@gmail.com><span class=fa-li><i class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li><li class=mb-2><span class=fa-li><i class="fa fa-map-marker"></i></span>Universiti Tenaga Nasional, 43000, Kajang, Malaysia.</a></li></ul></div></div></div><div class="row footer__footer"><div class=col-lg-6><div class="footer__footer_copy text-light"><p>All right reserved copyright ¬© Dickson Neoh 2022</p></div></div><div class=col-lg-6><div class=footer__footer_social><ul class=unstyle-list><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://www.linkedin.com/in/dickson-neoh-3a6984b8/><i class="fa fa-linkedin-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://twitter.com/dicksonneoh7><i class="fa fa-twitter-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://github.com/dnth><i class="fa fa-github-square"></i></a></li></ul></div></div></div></div></section><script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script><script src=https://dicksonneoh.com/plugins/jQuery/jquery.min.js></script><script src=https://dicksonneoh.com/plugins/bootstrap/bootstrap.min.js></script><script src=https://dicksonneoh.com/plugins/slick/slick.min.js></script><script src=https://dicksonneoh.com/plugins/waypoint/jquery.waypoints.min.js></script><script src=https://dicksonneoh.com/plugins/magnafic-popup/jquery.magnific-popup.min.js></script><script src=https://dicksonneoh.com/plugins/tweenmax/TweenMax.min.js></script><script src=https://dicksonneoh.com/plugins/imagesloaded/imagesloaded.min.js></script><script src=https://dicksonneoh.com/plugins/masonry/masonry.min.js></script><script src=https://dicksonneoh.com/js/form-handler.min.js></script><script src=https://dicksonneoh.com/js/script.min.js></script></body></html>