<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dickson Neoh - Personal Portfolio</title><link>https://dicksonneoh.com/portfolio/</link><description>Recent content on Dickson Neoh - Personal Portfolio</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 07 Feb 2023 11:00:15 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/portfolio/index.xml" rel="self" type="application/rss+xml"/><item><title>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</title><link>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</link><pubDate>Tue, 07 Feb 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</guid><description>üî• Motivation With various high-level libraries like Keras, Transformer, and Fastai, the barrier to training SOTA models has never been lower.
On top of that with platforms like Google Colab and Kaggle, pretty much anyone can train a reasonably good model using an old laptop or even a mobile phone (with some patience).
The question is no longer &amp;ldquo;can we train a SOTA model?&amp;rdquo;, but &amp;ldquo;what happens after that?&amp;rdquo;
Unfortunately, after getting the model trained, most people wash their hands off at this point claiming their model works.</description></item><item><title>fastdup: A Powerful Tool to Manage, Clean &amp; Curate Visual Data at Scale on Your CPU - For Free.</title><link>https://dicksonneoh.com/portfolio/fastdup_manage_clean_curate/</link><pubDate>Tue, 03 Jan 2023 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/fastdup_manage_clean_curate/</guid><description>‚úÖ Motivation As a data scientist, you might be tempted to jump into modeling as soon as you can. I mean, that&amp;rsquo;s the fun part, right?
But trust me, if you skip straight to modeling without taking the time to really understand the problem and analyze the data, you&amp;rsquo;re setting yourself up for failure.
I&amp;rsquo;ve been there.
You might feel like a superstar, but you&amp;rsquo;ll have with a model that doesn&amp;rsquo;t work ü§¶‚Äç‚ôÇÔ∏è.</description></item><item><title>Supercharging YOLOv5: How I Got 182.4 FPS Inference Without a GPU</title><link>https://dicksonneoh.com/portfolio/supercharging_yolov5_180_fps_cpu/</link><pubDate>Tue, 07 Jun 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/supercharging_yolov5_180_fps_cpu/</guid><description>üî• Motivation After months of searching, you&amp;rsquo;ve finally found the one.
The one object detection library that just works. No installation hassle, no package version mismatch, and no CUDA errors.
I&amp;rsquo;m talking about the amazingly engineered YOLOv5 object detection library by Ultralytics.
Elated, you quickly find an interesting dataset from Roboflow and finally trained a state-of-the-art (SOTA) YOLOv5 model to detect firearms from image streams.
You ran through a quick checklist &amp;ndash;</description></item><item><title>Deploying GPT-J Models on a Telegram Bot with Hugging Face Hub - For Free</title><link>https://dicksonneoh.com/portfolio/deploy_gpt_hf_models_on_telegram/</link><pubDate>Thu, 19 May 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/deploy_gpt_hf_models_on_telegram/</guid><description>üí• Motivation tip
By the end of this post you will learn how to:
Set up a Telegram bot with a Python wrapper library. Use the Gradio API to access the GPT-J model prediction. Host the Telegram bot on Hugging Face Spaces. By the end of this post, you&amp;rsquo;ll have your own Telegram bot that has access to the GPT-J-6B model. All for free.
Deploying a state-of-the-art (SOTA) GPT-like language model on a chatbot can be tricky.</description></item><item><title>Squeezing the Best Performance Out of YOLOX with Weights and Biases</title><link>https://dicksonneoh.com/portfolio/comparing_yolox_models_weights_and_biases/</link><pubDate>Wed, 11 May 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/comparing_yolox_models_weights_and_biases/</guid><description>üîé Motivation tip
By the end of this post you will learn how to:
Install the Weights and Biases client and log the YOLOX training metrics. Compare training metrics on Weights and Biases dashboard. Picking the best model with mAP and FPS values. &amp;ldquo;So many models, so little time!&amp;rdquo;
As a machine learning engineer, I often hear this phrase thrown around in many variations.
In object detection alone, there are already several hundreds of models out there.</description></item><item><title>Faster than GPU: How to 10x your Object Detection Model and Deploy on CPU at 50+ FPS</title><link>https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</link><pubDate>Sat, 30 Apr 2022 15:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/</guid><description>üö¶ Motivation tip
By the end of this post, you will learn how to:
Train state-of-the-art YOLOX model with your own data. Convert the YOLOX PyTorch model into ONNX and OpenVINO IR format. Run quantization algorithm to 10x your model&amp;rsquo;s inference speed. P/S: The final model runs faster on the CPU than the GPU! üò±
Deep learning (DL), seems to be the magic word that makes anything mundane cool again. We find them everywhere - in news reports, blog posts, articles, research papers, advertisements, and even baby books.</description></item><item><title>How to Deploy Object Detection Models on Android with Flutter</title><link>https://dicksonneoh.com/portfolio/how_to_deploy_od_models_on_android_with_flutter/</link><pubDate>Sun, 17 Apr 2022 15:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/how_to_deploy_od_models_on_android_with_flutter/</guid><description>üöë Deployment: Where ML models go to die In this post, I will outline the basic steps to deploy ML models onto lightweight mobile devices easily, quickly and for free.
tip
By the end of this post, you will learn about:
Leveraging Hugging Face infrastructure to host models. Deploying on any edge device using REST API. Displaying the results on a Flutter Android app. According to Gartner, more than 85% of machine learning (ML) models never made it into production.</description></item><item><title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code with 17 Images</title><link>https://dicksonneoh.com/portfolio/training_dl_model_for_cell_counting/</link><pubDate>Mon, 11 Apr 2022 15:07:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/training_dl_model_for_cell_counting/</guid><description>üï∂Ô∏è Motivation Many biology and medical procedures involve counting cells from images taken with a microscope. Counting cells reveals the concentration of bacteria and viruses and gives vital information on the progress of a disease.
To accomplish the counting, researchers painstakingly count the cells by hand with the assistance of a device called hemocytometer. This process is repetitive, tedious, and prone to errors.
What if we could automate the counting by using an intelligent deep learning algorithm instead?</description></item><item><title>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android &amp; iOS</title><link>https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/</link><pubDate>Sun, 27 Feb 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/</guid><description>info
This blog post is still a work in progress. If you require further clarifications before the contents are finalized, please get in touch with me here, on LinkedIn, or Twitter. üåü Motivation Meet Bob, a data scientist with a passion for computer vision. Bob had been working on a project to build a model that could identify different types of fruits, from apples to pineapples. He spent countless hours training and fine-tuning the model until it could recognize fruits with 98% accuracy.</description></item><item><title>Deploying Object Detection Models on Hugging Face Spaces</title><link>https://dicksonneoh.com/portfolio/deploy_icevision_models_on_huggingface_spaces/</link><pubDate>Thu, 17 Feb 2022 13:42:56 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/deploy_icevision_models_on_huggingface_spaces/</guid><description>Introduction So, you‚Äôve trained a deep learning model that can detect objects from images. Next, how can you share the awesomeness of your model with the rest of the world? You might be a PhD student trying to get some ideas from your peers or supervisors, or a startup founder who wishes to share a minimum viable product to your clients for feedback. But, at the same time you don&amp;rsquo;t wish to go through the hassle of dealing with MLOps.</description></item></channel></rss>