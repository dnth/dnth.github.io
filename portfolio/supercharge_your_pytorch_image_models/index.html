<!doctype html><html><head><meta charset=utf-8><title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations</title>
<meta name=description content="Supercharge your PyTorch image models with ONNX Runtime and TensorRT. Learn step-by-step techniques to achieve up to 8x faster inference speeds, enabling real-time performance for computer vision applications. Optimize TIMM models for deployment using advanced acceleration methods."><meta property="og:url" content="https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/"><meta property="og:site_name" content="Dickson Neoh - Personal Portfolio"><meta property="og:title" content="Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations"><meta property="og:description" content="Supercharge your PyTorch image models with ONNX Runtime and TensorRT. Learn step-by-step techniques to achieve up to 8x faster inference speeds, enabling real-time performance for computer vision applications. Optimize TIMM models for deployment using advanced acceleration methods."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="portfolio"><meta property="article:published_time" content="2024-09-30T09:00:00+08:00"><meta property="article:modified_time" content="2024-09-30T09:00:00+08:00"><meta property="article:tag" content="TIMM"><meta property="article:tag" content="ONNX"><meta property="article:tag" content="TensorRT"><meta property="article:tag" content="ImageNet"><meta property="article:tag" content="Hugging Face"><meta property="article:tag" content="PyTorch"><meta property="og:image" content="https://dicksonneoh.com/images/portfolio/supercharge_your_pytorch_image_models/post_image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dicksonneoh.com/images/portfolio/supercharge_your_pytorch_image_models/post_image.png"><meta name=twitter:title content="Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations"><meta name=twitter:description content="Supercharge your PyTorch image models with ONNX Runtime and TensorRT. Learn step-by-step techniques to achieve up to 8x faster inference speeds, enabling real-time performance for computer vision applications. Optimize TIMM models for deployment using advanced acceleration methods."><meta name=viewport content="width=device-width,initial-scale=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick-theme.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/magnafic-popup/magnific-popup.css><link href=https://dicksonneoh.com/scss/style.min.css rel=stylesheet><link rel="shortcut icon" href=https://dicksonneoh.com/images/favicon-purple.ico type=image/x-icon><link rel=icon href=https://dicksonneoh.com/images/favicon.gif type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-54500366-2")</script></head><body><nav class="navbar navbar-expand-lg fixed-top"><div class=container><a href=https://dicksonneoh.com/ class=navbar-brand><img src=https://dicksonneoh.com/images/site-navigation/logo_resized.png alt=site-logo>
</a><button type=button class="navbar-toggler collapsed" data-toggle=collapse data-target=#navbarCollapse>
<span class=navbar-toggler-icon></span>
<span class=navbar-toggler-icon></span>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarCollapse><div class="d-flex flex-column w-100"><div class="d-flex justify-content-between align-items-center"><ul class="nav navbar-nav main-navigation my-0"><li class=nav-item><a href=https://dicksonneoh.com/#home class="nav-link text-dark text-sm-center p-2">Home</a></li><li class=nav-item><a href=https://dicksonneoh.com/#about class="nav-link text-dark text-sm-center p-2">About</a></li><li class=nav-item><a href=https://dicksonneoh.com/#service class="nav-link text-dark text-sm-center p-2">Services</a></li><li class=nav-item><a href=https://dicksonneoh.com/#portfolio class="nav-link text-dark text-sm-center p-2">Projects</a></li><li class=nav-item><a href=https://dicksonneoh.com/#resume class="nav-link text-dark text-sm-center p-2">Resume</a></li><li class=nav-item><a href=https://dicksonneoh.com/#skills class="nav-link text-dark text-sm-center p-2">Skills</a></li><li class=nav-item><a href=https://dicksonneoh.com/#blog class="nav-link text-dark text-sm-center p-2">Blogs</a></li><li class=nav-item><a href=https://dicksonneoh.com/#contact class="nav-link text-dark text-sm-center p-2">Contact</a></li></ul><div class=navbar-nav><a href=https://dicksonneoh.com/contact class="btn btn-primary btn-zoom hire_button nowrap">Let's Talk</a></div></div><div class="search-row search-row-toggle"><script src=/js/search.js></script><div class="search-container position-relative w-100"><div class="search-input-wrapper position-relative"><i class="fa fa-search search-icon"></i>
<input type=text id=search-input class=form-control placeholder="Search blogs and projects..." autocomplete=off></div><div id=search-results class="search-results-dropdown d-none"></div></div><style>.search-row{padding:1rem 0 0;width:100%}.search-container{width:100%;max-width:100%;padding:0}.search-input-wrapper{position:relative;width:100%}.search-icon{position:absolute;left:15px;top:50%;transform:translateY(-50%);color:#6c757d;z-index:2}#search-input{width:100%;height:38px;padding:8px 20px 8px 40px;font-size:.95rem;color:#495057;background-color:#f8f9fa;border:1px solid #eee;border-radius:20px;transition:all .2s ease-in-out}#search-input:focus{background-color:#fff;border-color:#6c63ff;box-shadow:0 0 0 .2rem rgba(108,99,255,.1);outline:none}#search-input:focus+.search-icon{color:#6c63ff}#search-input::placeholder{color:#6c757d;opacity:.8}.search-results-dropdown{position:absolute;top:calc(100% + 8px);left:15px;right:15px;background:#fff;border:1px solid #eee;border-radius:8px;box-shadow:0 4px 15px rgba(0,0,0,8%);max-height:400px;overflow-y:auto;z-index:1050}.search-result-item{padding:12px 15px;border-bottom:1px solid #eee;cursor:pointer;transition:background-color .2s ease}.search-result-item:hover{background-color:#f8f9fa}.search-result-item:last-child{border-bottom:none}.nowrap{white-space:nowrap}mark{background-color:rgba(108,99,255,.2);color:inherit;padding:0 2px;border-radius:2px}.badge{font-size:.75rem;padding:.25em .6em;border-radius:12px}.bg-primary{background-color:#6c63ff!important}.bg-success{background-color:#28a745!important}@media(max-width:991px){.search-container{padding:0 10px}}</style></div></div></div></div></nav><div id=content><header class=breadCrumb><div class=container><div class=row><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><h3 class=breadCrumb__title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations</h3><nav aria-label=breadcrumb class="d-flex justify-content-center"></nav></div></div><div class="row p-3"><div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center"><i class="fa fa-calendar"></i> &ensp;
September 30, 2024 &ensp; &ensp;
<i class="fa fa-clock-o"></i> &ensp;
15 mins read</div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><div class=portfolio-item-tags-categories><i class="fa fa-tags"></i> &ensp;
<a href=/tags/timm class=tag-category>TIMM</a>
<a href=/tags/onnx class=tag-category>ONNX</a>
<a href=/tags/tensorrt class=tag-category>TensorRT</a>
<a href=/tags/imagenet class=tag-category>ImageNet</a>
<a href=/tags/hugging-face class=tag-category>Hugging Face</a>
<a href=/tags/pytorch class=tag-category>PyTorch</a></div></div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><div class=portfolio-item-tags-categories><i class="fa fa-folder-open"></i> &ensp;
<a href=/categories/inference class=tag-category>inference</a>
<a href=/categories/deployment class=tag-category>deployment</a>
<a href=/categories/image-classification class=tag-category>image-classification</a>
<a href=/categories/optimization class=tag-category>optimization</a>
<a href=/categories/edge-ai class=tag-category>edge-ai</a></div></div></div></div></header><section class="section singleBlog"><div class=svg-img><img src=https://dicksonneoh.com/images/hero/figure-svg.svg alt></div><div class=animate-shape><img src=https://dicksonneoh.com/images/skill/skill-background-shape.svg alt><svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600"><defs><linearGradient id="d" x1=".929" y1=".111" x2=".263" y2=".935" gradientUnits="objectBoundingBox"><stop offset="0" stop-color="#f1f6f9"/><stop offset="1" stop-color="#f1f6f9" stop-opacity="0"/></linearGradient></defs><g data-name="blob-shape (3)"><path class="blob" fill="url(#d)" d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z"/></g></svg></div><div class=animate-pattern><img src=https://dicksonneoh.com/images/service/background-pattern.svg alt=background-shape></div><div class=container><div class=row><div class=col-lg-12><div class=singleBlog__feature><img src=https://dicksonneoh.com/images/portfolio/supercharge_your_pytorch_image_models/post_image.png alt=feature-image></div></div></div><div class="row mt-5"><div class=col-lg-12><div class=singleBlog__content><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a class=toc-link href=#-motivation>🚀 Motivation</a></li><li><a class=toc-link href=#-installation>💻 Installation</a></li><li><a class=toc-link href=#-load-and-infer>🔧 Load and Infer</a></li><li><a class=toc-link href=#-baseline-latency>⏱️ Baseline Latency</a></li><li><a class=toc-link href=#-convert-to-onnx>🔄 Convert to ONNX</a></li><li><a class=toc-link href=#-onnx-runtime-on-cpu>🖥️ ONNX Runtime on CPU</a></li><li><a class=toc-link href=#-onnx-runtime-on-cuda>🖼️ ONNX Runtime on CUDA</a></li><li><a class=toc-link href=#-onnx-runtime-on-tensorrt>📊 ONNX Runtime on TensorRT</a></li><li><a class=toc-link href=#-bake-pre-processing-into-onnx>🎂 Bake pre-processing into ONNX</a></li><li><a class=toc-link href=#-video-inference>🎮 Video Inference</a></li><li><a class=toc-link href=#-conclusion>🚧 Conclusion</a></li></ul></nav><div id=floating-toc class=floating-toc><div id=toc-handle class=toc-handle>☰ Table of Contents</div><nav id=TableOfContents><ul><li><a class=toc-link href=#-motivation>🚀 Motivation</a></li><li><a class=toc-link href=#-installation>💻 Installation</a></li><li><a class=toc-link href=#-load-and-infer>🔧 Load and Infer</a></li><li><a class=toc-link href=#-baseline-latency>⏱️ Baseline Latency</a></li><li><a class=toc-link href=#-convert-to-onnx>🔄 Convert to ONNX</a></li><li><a class=toc-link href=#-onnx-runtime-on-cpu>🖥️ ONNX Runtime on CPU</a></li><li><a class=toc-link href=#-onnx-runtime-on-cuda>🖼️ ONNX Runtime on CUDA</a></li><li><a class=toc-link href=#-onnx-runtime-on-tensorrt>📊 ONNX Runtime on TensorRT</a></li><li><a class=toc-link href=#-bake-pre-processing-into-onnx>🎂 Bake pre-processing into ONNX</a></li><li><a class=toc-link href=#-video-inference>🎮 Video Inference</a></li><li><a class=toc-link href=#-conclusion>🚧 Conclusion</a></li></ul></nav></div><hr><h3 id=-motivation>🚀 Motivation</h3><p>Having real-time inference is crucial for computer vision applications.
In some domains, a 1-second delay in inference could mean life or death.</p><p>Imagine sitting in a self-driving car and the car takes <strong>one full second</strong> to detect an oncoming speeding truck.</p><p>Just one second too late, and you could end up in the clouds 👼👼👼</p><p>Or if you&rsquo;re lucky, you get a very up-close view of the pavement.</p><figure><img src=/portfolio/supercharge_your_pytorch_image_models/banana_peel_robot.gif width=auto></figure><p>I hope that shows how crucial real-time inference is.</p><style>#callout{background:#f0f0f0;padding:1.5em 1.25em;border-radius:3px;display:flex;flex-direction:row;margin-bottom:20px}#callout-inner{margin-left:1em}@media(max-width:767px){#callout{padding:1.5em .75em 1.5em .6em}#callout-inner{margin-left:.5em}}</style><div id=callout><div>📌</div><div id=callout-inner>In many high-stake applications, it&rsquo;s not just about being right - it&rsquo;s about being right, right now.</div></div><p>Thus, having real-time inference capability is paramount and will determine whether a model gets deployed or not.
In many cases, you can pick one or the other:</p><ul><li>A fast model with low accuracy</li><li>A slow model with high accuracy</li></ul><p>But can we have the best of both worlds? I.e. a <strong>fast and accurate</strong> model?</p><p>That&rsquo;s what this post is about.</p><style type=text/css>.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:#fff;background:#6ab0de;text-transform:uppercase}.notice.warning .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info .notice-title{background:#f0b37e}.notice.info{background:#fff2db}.notice.note .notice-title{background:#6ab0de}.notice.note{background:#e7f2fa}.notice.tip .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>By the end of the post you&rsquo;ll learn how to supercharge the inference speed of any image models from <a href=https://huggingface.co/docs/timm/index target=_blank rel="nofollow noopener noreferrer">TIMM</a> with optimized <a href=https://onnxruntime.ai/ target=_blank rel="nofollow noopener noreferrer">ONNX Runtime</a> and <a href=https://developer.nvidia.com/tensorrt target=_blank rel="nofollow noopener noreferrer">TensorRT</a>.</p><p>In short:</p><ul><li>📥 Load any pre-trained model from <a href=https://huggingface.co/docs/timm/index target=_blank rel="nofollow noopener noreferrer">TIMM</a>.</li><li>🔄 Convert the model to ONNX format.</li><li>🖥️ Run inference with <a href=https://onnxruntime.ai/ target=_blank rel="nofollow noopener noreferrer">ONNX Runtime</a> (CPU & CUDA Provider).</li><li>🎮 Run inference with <a href=https://developer.nvidia.com/tensorrt target=_blank rel="nofollow noopener noreferrer">TensorRT</a> provider and optimized runtime parameters.</li><li>🧠 Bake the pre-processing into the ONNX model for faster inference.</li></ul><p>You can find the code for this post on my GitHub repository <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>info</p><p>Are you non-technical?</p><p>Listen to this 10 mins conversation podcast that breaks down the content of this post in an ELI5 manner.</p><style>.custom-audio-player{width:100%;max-width:600px;margin:20px auto;background:#fff;border-radius:10px;padding:20px;box-shadow:0 4px 6px rgba(0,0,0,.1)}.audio-info{margin-bottom:15px}.audio-title{font-size:1em;font-weight:700;color:#333;margin-bottom:5px}.audio-author{font-size:.9em;color:#666}.custom-audio-player audio{width:100%;height:40px;outline:none;border-radius:5px}.custom-audio-player audio::-webkit-media-controls-panel{background-color:#e0e0e0}.custom-audio-player audio::-webkit-media-controls-play-button,.custom-audio-player audio::-webkit-media-controls-mute-button{background-color:#8492ff;border-radius:50%}.custom-audio-player audio::-webkit-media-controls-current-time-display,.custom-audio-player audio::-webkit-media-controls-time-remaining-display{color:#333}.post-image{width:100%;max-height:300px;object-fit:cover;border-radius:5px;margin-bottom:15px}</style><div class=custom-audio-player><div class=audio-header><div class=audio-info><div class=audio-title>Deep Dive - Explain Like I'm 5</div><div class=audio-author>Supercharge Your PyTorch Image Models</div></div></div><audio id=audio controls preload=metadata><source src=notebooklm.wav>Your browser does not support the audio element.</audio></div><p>Note: Conversation generated using NotebookLM.</p></div><p>If you&rsquo;re technical, and this sounds exciting, then let&rsquo;s dive in! 🏊‍♂️</p><h3 id=-installation>💻 Installation</h3><p>Let&rsquo;s begin with the installation.
I will be using a <code>conda</code> environment to install the packages required for this post. Feel free to the environment of your choice.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create -n supercharge_timm_tensorrt python<span style=color:#f92672>=</span>3.11
</span></span><span style=display:flex><span>conda activate supercharge_timm_tensorrt
</span></span></code></pre></div><p>We&rsquo;ll be using the <a href=https://github.com/huggingface/pytorch-image-models target=_blank rel="nofollow noopener noreferrer"><code>timm</code></a> library to load a pre-trained model and run inference. So let&rsquo;s install <code>timm</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install timm
</span></span></code></pre></div><p>At the time of writing, there are over <a href=https://huggingface.co/timm target=_blank rel="nofollow noopener noreferrer">1370 models</a> available in timm. Any of which can be used in this post.</p><h3 id=-load-and-infer>🔧 Load and Infer</h3><p>Let&rsquo;s load a top performing model from the timm <a href=https://huggingface.co/spaces/timm/leaderboard target=_blank rel="nofollow noopener noreferrer">leaderboard</a> - the <code>eva02_large_patch14_448.mim_m38m_ft_in22k_in1k</code> model.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/eva_timm.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/eva_timm.png width=auto style=max-width:100%;height:auto></a></figure><p>The plot above shows the accuracy vs inference speed for the EVA02 model.</p><p>Look closely, the EVA02 model achieves top ImageNet accuracy (90.05% top-1, 99.06% top-5) but is lags in speed.
Check out the model on the <code>timm</code> leaderboard <a href=https://huggingface.co/spaces/timm/leaderboard target=_blank rel="nofollow noopener noreferrer">here</a>.</p><p>So let&rsquo;s get the EVA02 model on our local machine</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> timm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k&#39;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>create_model(model_name, pretrained<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>eval()
</span></span></code></pre></div><p>Get the data config and transformations for the model</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data_config <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>resolve_model_data_config(model)
</span></span><span style=display:flex><span>transforms <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>create_transform(<span style=color:#f92672>**</span>data_config, is_training<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p>And run an inference to get the top 5 predictions</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> urllib.request <span style=color:#f92672>import</span> urlopen
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(urlopen(<span style=color:#e6db74>&#39;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>inference_mode():
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> model(transforms(img)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>top5_probabilities, top5_class_indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>topk(output<span style=color:#f92672>.</span>softmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>100</span>, k<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
</span></span></code></pre></div><p>Next, decode the predictions into class names as a sanity check</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> imagenet_classes <span style=color:#f92672>import</span> IMAGENET2012_CLASSES
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>im_classes <span style=color:#f92672>=</span> list(IMAGENET2012_CLASSES<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>class_names <span style=color:#f92672>=</span> [im_classes[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> top5_class_indices[<span style=color:#ae81ff>0</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, prob <span style=color:#f92672>in</span> zip(class_names, top5_probabilities[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>prob<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/beignets-task-guide.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/beignets-task-guide.png width=400 style=max-width:100%;height:auto></a></figure><p>Top 5 predictions:</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>espresso:&nbsp;26.78%</li><li>eggnog:&nbsp;2.88%</li><li>cup:&nbsp;2.60%</li><li>chocolate&nbsp;sauce,&nbsp;chocolate&nbsp;syrup:&nbsp;2.39%</li><li>bakery,&nbsp;bakeshop,&nbsp;bakehouse:&nbsp;1.48%</li><li></li></ul></div></div><p>Looks like the model is doing it&rsquo;s job.</p><p>Now let&rsquo;s benchmark the inference latency.</p><h3 id=-baseline-latency>⏱️ Baseline Latency</h3><p>We will run the inference 10 times and average the inference time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_benchmark</span>(model, device, num_images<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>inference_mode():
</span></span><span style=display:flex><span>        start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_images):
</span></span><span style=display:flex><span>            input_tensor <span style=color:#f92672>=</span> transforms(img)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>            model(input_tensor)
</span></span><span style=display:flex><span>        end <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    ms_per_image <span style=color:#f92672>=</span> (end <span style=color:#f92672>-</span> start) <span style=color:#f92672>/</span> num_images <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    fps <span style=color:#f92672>=</span> num_images <span style=color:#f92672>/</span> (end <span style=color:#f92672>-</span> start)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;PyTorch model on </span><span style=color:#e6db74>{</span>device<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>ms_per_image<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> ms per image, FPS: </span><span style=color:#e6db74>{</span>fps<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>Let&rsquo;s benchmark on CPU and GPU.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># CPU Benchmark</span>
</span></span><span style=display:flex><span>run_benchmark(model, torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cpu&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># GPU Benchmark </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    run_benchmark(model, torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda&#34;</span>))
</span></span></code></pre></div><p>Alright the benchmarks are in</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>PyTorch&nbsp;model&nbsp;on&nbsp;cpu:&nbsp;1584.379&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;0.63</li><li>PyTorch&nbsp;model&nbsp;on&nbsp;cuda:&nbsp;77.226&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;12.95</li><li></li></ul></div></div><p>Although the performance on the GPU is not bad, 12+ FPS is still not fast enough for real-time inference.
On my reasonably modern CPU, it took over 1.5 seconds to run an inference.</p><p>Definitely not self-driving car material 🤷</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>I&rsquo;m using the following hardware for the benchmarks:</p><ul><li>GPU - NVIDIA RTX 3090</li><li>CPU - 11th Gen Intel® Core™ i9-11900 @ 2.50GHz × 16</li></ul><pre tabindex=0><code></code></pre></div><p>Now let&rsquo;s start to improve the inference time.</p><h3 id=-convert-to-onnx>🔄 Convert to ONNX</h3><p><a href=https://onnx.ai/ target=_blank rel="nofollow noopener noreferrer">ONNX</a> is an open and interoperable format for deep learning models. It lets us deploy models across different frameworks and devices.</p><p>The key advantage of using ONNX is that it lets us deploy models across different frameworks and devices, and offers some performance gains.</p><p>To convert the model to ONNX format, let&rsquo;s first install <code>onnx</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install onnx
</span></span></code></pre></div><p>And export the model to ONNX format</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> timm
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>create_model(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k&#34;</span>, pretrained<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>onnx<span style=color:#f92672>.</span>export(
</span></span><span style=display:flex><span>    model,
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>),
</span></span><span style=display:flex><span>    onnx_filename,
</span></span><span style=display:flex><span>    export_params<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    opset_version<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>,
</span></span><span style=display:flex><span>    do_constant_folding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    input_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;input&#34;</span>],
</span></span><span style=display:flex><span>    output_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;output&#34;</span>],
</span></span><span style=display:flex><span>    dynamic_axes<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;input&#34;</span>: {<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;batch_size&#34;</span>}, 
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;output&#34;</span>: {<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;batch_size&#34;</span>}},
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Here are the descriptions for the arguments you can pass to the <code>torch.onnx.export</code> function:</p><div class=data-table role=region tabindex=0><table class="table %!s(<nil>)" id=863429715><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td><code>torch.randn(1, 3, 448, 448)</code></td><td>A dummy input tensor with the appropriate shape</td></tr><tr><td><code>export_params</code></td><td>Whether to export the model parameters</td></tr><tr><td><code>do_constant_folding</code></td><td>Whether to do constant folding for optimization</td></tr><tr><td><code>input_names</code></td><td>The name of the input node</td></tr><tr><td><code>output_names</code></td><td>The name of the output node</td></tr><tr><td><code>dynamic_axes</code></td><td>Dynamic axes for the input and output nodes</td></tr></tbody></table></div></div><p>If there are no errors, you will end up with a file called <code>eva02_large_patch14_448.onnx</code> in your working directory.</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>Inspect and visualize the ONNX model using the <a href=https://netron.app/ target=_blank rel="nofollow noopener noreferrer">Netron</a> webapp.</p></div><h3 id=-onnx-runtime-on-cpu>🖥️ ONNX Runtime on CPU</h3><p>To run the and inference on the ONNX model, we need to install <code>onnxruntime</code>. This is the &rsquo;engine&rsquo; that will run the ONNX model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install onnxruntime
</span></span></code></pre></div><p>One (major) benefit of using ONNX Runtime is the ability to run the model without PyTorch as a dependency.
This is great for deployment and for running inference in environments where PyTorch is not available.</p><p>The ONNX model we exported earlier only includes the model weights and the graph structure. It does not include the pre-processing transforms.
To run the inference using <code>onnxruntime</code>, we need to replicate the PyTorch transforms.
To find out the transforms that was used, you can print out the <code>transforms</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(transforms)
</span></span></code></pre></div><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>Compose(</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Resize(size=(448,&nbsp;448),&nbsp;</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;interpolation=bicubic,&nbsp;</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_size=None,&nbsp;</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;antialias=True)</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CenterCrop(size=(448,&nbsp;448))</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MaybeToTensor()</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Normalize(mean=tensor([0.4815,&nbsp;0.4578,&nbsp;0.4082]),&nbsp;</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;std=tensor([0.2686,&nbsp;0.2613,&nbsp;0.2758]))</li><li>)</li><li></li></ul></div></div><p>Now let&rsquo;s replicate the transforms using <code>numpy</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transforms_numpy</span>(image: PIL<span style=color:#f92672>.</span>Image<span style=color:#f92672>.</span>Image):
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#39;RGB&#39;</span>)
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>resize((<span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>), Image<span style=color:#f92672>.</span>BICUBIC)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(image)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> img_numpy<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>])<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    std <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>0.225</span>])<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> (img_numpy <span style=color:#f92672>-</span> mean) <span style=color:#f92672>/</span> std
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(img_numpy, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> img_numpy<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> img_numpy
</span></span></code></pre></div><p>Using the <code>numpy</code>, transforms let&rsquo;s run inference with ONNX Runtime.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> onnxruntime <span style=color:#66d9ef>as</span> ort
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create ONNX Runtime session with CPU provider</span>
</span></span><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> ort<span style=color:#f92672>.</span>InferenceSession(
</span></span><span style=display:flex><span>    onnx_filename, 
</span></span><span style=display:flex;background-color:#3c3d38><span>    providers<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;CPUExecutionProvider&#34;</span>] <span style=color:#75715e># Run on CPU</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get input and output names</span>
</span></span><span style=display:flex><span>input_name <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>get_inputs()[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>output_name <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>get_outputs()[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run inference</span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>run([output_name], 
</span></span><span style=display:flex><span>                    {input_name: transforms_numpy(img)})[<span style=color:#ae81ff>0</span>]</span></span></code></pre></div><p>If we inspect the output shape, we can see that it&rsquo;s the same as the number of classes in the ImageNet dataset.</p><p>Let&rsquo;s inspect the <code>output.shape</code>:<div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li>(1,&nbsp;1000)</li><li></li></ul></div></div></p><p>And printing the top 5 predictions:</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>espresso:&nbsp;28.65%</li><li>cup:&nbsp;2.77%</li><li>eggnog:&nbsp;2.28%</li><li>chocolate&nbsp;sauce,&nbsp;chocolate&nbsp;syrup:&nbsp;2.13%</li><li>bakery,&nbsp;bakeshop,&nbsp;bakehouse:&nbsp;1.42%</li><li></li></ul></div></div><p>We get the same results as the PyTorch model with ONNX Runtime. That&rsquo;s a good sign!</p><p>Now let&rsquo;s benchmark the inference latency on ONNX Runtime with a CPU provider (backend).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_images <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_images):
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>run([output_name], {input_name: transforms_numpy(img)})[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>end <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span>time_taken <span style=color:#f92672>=</span> end <span style=color:#f92672>-</span> start
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ms_per_image <span style=color:#f92672>=</span> time_taken <span style=color:#f92672>/</span> num_images <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>fps <span style=color:#f92672>=</span> num_images <span style=color:#f92672>/</span> time_taken
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Onnxruntime CPU: </span><span style=color:#e6db74>{</span>ms_per_image<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> ms per image, FPS: </span><span style=color:#e6db74>{</span>fps<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>Onnxruntime&nbsp;CPU:&nbsp;2002.446&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;0.50</li><li></li></ul></div></div><p>Ouch! That&rsquo;s slower than the PyTorch model. What a bummer!
It may seem like a step back, but we are only getting started.</p><p>Read on.</p><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>info</p><p>The remainder of this post assumes that you have a compatible NVDIA GPU. If you don&rsquo;t, you can still use the CPU for inference by switch to the Intel OpenVINO or AMD backend.</p><p>There are more backends available including for mobile devices like Apple, Android, etc. Check them out <a href=https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html target=_blank rel="nofollow noopener noreferrer">here</a></p><p>These will be covered in a future post.</p></div><h3 id=-onnx-runtime-on-cuda>🖼️ ONNX Runtime on CUDA</h3><p>Other than the CPU, ONNX Runtime offers other backends for inference. We can easily swap to a different backend by changing the provider. In this case we will use the CUDA backend.</p><p>To use the CUDA backend, we need to install the <code>onnxruntime-gpu</code> package.<div class="notice warning"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#warning-notice"/></svg></span>warning</p><p>You <strong>must</strong> uninstall the <code>onnxruntime</code> package before installing the <code>onnxruntime-gpu</code> package.</p><p>Run the following to uninstall the <code>onnxruntime</code> package.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip uninstall onnxruntime
</span></span></code></pre></div><p>Then install the <code>onnxruntime-gpu</code> package.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install onnxruntime-gpu<span style=color:#f92672>==</span>1.19.2
</span></span></code></pre></div><p>The <code>onnxruntime-gpu</code> package requires a compatible CUDA and cuDNN version. I&rsquo;m running on <code>onnxruntime-gpu==1.19.2</code> at the time of writing this post. This version is compatible with CUDA <code>12.x</code> and cuDNN <code>9.x</code>.</p><p>See the compatibility matrix <a href=https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div></p><p>You can install all the CUDA dependencies using conda with the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install -c nvidia cuda<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-tools<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-toolkit<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-version<span style=color:#f92672>=</span>12.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-command-line-tools<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-compiler<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-runtime<span style=color:#f92672>=</span>12.2.2
</span></span></code></pre></div><p>Once done, replace the CPU provider with the CUDA provider.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-python data-lang=python><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> ort<span style=color:#f92672>.</span>InferenceSession(
</span></span><span style=display:flex><span>    onnx_filename, 
</span></span><span style=display:flex;background-color:#3c3d38><span>    providers<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;CUDAExecutionProvider&#34;</span>] <span style=color:#75715e># change the provider </span>
</span></span><span style=display:flex><span>)</span></span></code></pre></div><p>The rest of the code is the same as the CPU inference.</p><p>Just with one line of code change, the benchmarks are as follows:</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>Onnxruntime&nbsp;CUDA&nbsp;numpy&nbsp;transforms:&nbsp;56.430&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;17.72</li><li></li></ul></div></div><p>But that&rsquo;s kinda expected. Running on the GPU, we should expect a speedup.</p><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>info</p><p>If you encounter the following error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Failed to load library libonnxruntime_providers_cuda.so 
</span></span><span style=display:flex><span>with error: libcublasLt.so.12: cannot open shared object 
</span></span><span style=display:flex><span>file: No such file or directory
</span></span></code></pre></div><p>It means that the CUDA library is not in the library path.
You need to export the library path to include the CUDA library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export LD_LIBRARY_PATH<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib:</span>$LD_LIBRARY_PATH<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><p>Replace the <code>/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib</code> with the path to your CUDA library.</p></div><p>Theres is one more trick we can use to squeeze out more performance - using <a href=https://cupy.dev/ target=_blank rel="nofollow noopener noreferrer">CuPy</a> for the transforms instead of NumPy.</p><p>CuPy is a library that lets us run NumPy code on the GPU. It&rsquo;s a drop-in replacement for NumPy, so you can just replace <code>numpy</code> with <code>cupy</code> in your code and it will run on the GPU.</p><p>Let&rsquo;s install CuPy compatible with our CUDA version.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install cupy-cuda12x
</span></span></code></pre></div><p>And we can use it to run the transforms.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transforms_cupy</span>(image: PIL<span style=color:#f92672>.</span>Image<span style=color:#f92672>.</span>Image):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert image to RGB and resize</span>
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#34;RGB&#34;</span>)
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>resize((<span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>), Image<span style=color:#f92672>.</span>BICUBIC)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert to CuPy array and normalize</span>
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>array(image, dtype<span style=color:#f92672>=</span>cp<span style=color:#f92672>.</span>float32) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> img_cupy<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply mean and std normalization</span>
</span></span><span style=display:flex><span>    mean <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>], dtype<span style=color:#f92672>=</span>cp<span style=color:#f92672>.</span>float32)<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    std <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>0.225</span>], dtype<span style=color:#f92672>=</span>cp<span style=color:#f92672>.</span>float32)<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> (img_cupy <span style=color:#f92672>-</span> mean) <span style=color:#f92672>/</span> std
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Add batch dimension</span>
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>expand_dims(img_cupy, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> img_cupy
</span></span></code></pre></div><p>With CuPy, we got a tiny bit of performance improvement:</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>Onnxruntime&nbsp;CUDA&nbsp;cupy&nbsp;transforms:&nbsp;54.267&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;18.43</li><li></li></ul></div></div><p>Using ONNX Runtime with CUDA is a little better than the PyTorch model on the GPU, but still not fast enough for real-time inference.</p><p>We have one more trick up our sleeve.</p><h3 id=-onnx-runtime-on-tensorrt>📊 ONNX Runtime on TensorRT</h3><p>Similar to the CUDA provider, we have the TensorRT provider on ONNX Runtime. This lets us run the model using the TensorRT high performance inference engine by NVIDIA.</p><p>From the <a href=https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements target=_blank rel="nofollow noopener noreferrer">compatibility matrix</a>, we can see that <code>onnxruntime-gpu==1.19.2</code> is compatible with TensorRT 10.1.0.</p><p>To use the TensorRT provider, you need to have TensorRT installed on your system.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install tensorrt<span style=color:#f92672>==</span>10.1.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            tensorrt-cu12<span style=color:#f92672>==</span>10.1.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            tensorrt-cu12-bindings<span style=color:#f92672>==</span>10.1.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            tensorrt-cu12-libs<span style=color:#f92672>==</span>10.1.0
</span></span></code></pre></div><p>Next you need to export library path to include the TensorRT library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export LD_LIBRARY_PATH<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/python3.11/site-packages/tensorrt_libs:</span>$LD_LIBRARY_PATH<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><p>Replace the <code>/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/python3.11/site-packages/tensorrt_libs</code> with the path to your TensorRT library.</p><p>Otherwise you&rsquo;ll encounter the following error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Failed to load library libonnxruntime_providers_tensorrt.so 
</span></span><span style=display:flex><span>with error: libnvinfer.so.10: cannot open shared object file: 
</span></span><span style=display:flex><span>No such file or directory
</span></span></code></pre></div><p>Next we need so set the TensorRT provider options in ONNX Runtime inference code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>providers <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    (
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;TensorrtExecutionProvider&#34;</span>,
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;device_id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_max_workspace_size&#34;</span>: <span style=color:#ae81ff>8589934592</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_fp16_enable&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_engine_cache_enable&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_engine_cache_path&#34;</span>: <span style=color:#e6db74>&#34;./trt_cache&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_force_sequential_engine_build&#34;</span>: <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_max_partition_iterations&#34;</span>: <span style=color:#ae81ff>10000</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_min_subgraph_size&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_builder_optimization_level&#34;</span>: <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_timing_cache_enable&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> ort<span style=color:#f92672>.</span>InferenceSession(onnx_filename, providers<span style=color:#f92672>=</span>providers)
</span></span></code></pre></div><p>The rest of the code is the same as the CUDA inference.</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Here are the parameters and description for the TensorRT provider:</p><div class=data-table role=region tabindex=0><table class="table %!s(<nil>)" id=712895634><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td><code>device_id</code></td><td>The GPU device ID to use. Using the first GPU in the system.</td></tr><tr><td><code>trt_max_workspace_size</code></td><td>Maximum workspace size for TensorRT in bytes (8GB). Allows TensorRT to use up to 8GB of GPU memory for operations.</td></tr><tr><td><code>trt_fp16_enable</code></td><td>Enables FP16 (half-precision) mode. Speeds up inference on supported GPUs while reducing memory usage.</td></tr><tr><td><code>trt_engine_cache_enable</code></td><td>Enables caching of TensorRT engines. Speeds up subsequent runs by avoiding engine rebuilding.</td></tr><tr><td><code>trt_engine_cache_path</code></td><td>Directory where TensorRT engine cache files will be stored.</td></tr><tr><td><code>trt_force_sequential_engine_build</code></td><td>Allows parallel building of TensorRT engines for different subgraphs.</td></tr><tr><td><code>trt_max_partition_iterations</code></td><td>Maximum number of iterations for TensorRT to attempt partitioning the graph.</td></tr><tr><td><code>trt_min_subgraph_size</code></td><td>Minimum number of nodes required for a subgraph to be considered for conversion to TensorRT.</td></tr><tr><td><code>trt_builder_optimization_level</code></td><td>Optimization level for the TensorRT builder. Level 5 is highest, can result in longer build times but potentially better performance.</td></tr><tr><td><code>trt_timing_cache_enable</code></td><td>Enables timing cache. Helps speed up engine building by reusing layer timing information from previous builds.</td></tr></tbody></table></div></div><p>Refer to the <a href=https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html target=_blank rel="nofollow noopener noreferrer">TensorRT ExecutionProvider documentation</a> for more details on the parameters.</p><p>And now let&rsquo;s run the benchmark:</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>TensorRT&nbsp;+&nbsp;numpy:&nbsp;18.852&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;53.04</li><li>TensorRT&nbsp;+&nbsp;cupy:&nbsp;16.892&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;59.20</li><li></li></ul></div></div><p>Running with TensorRT and cupy give us a 4.5x speedup over the PyTorch model on the GPU and 93x speedup over the PyTorch model on the CPU!</p><p>Thank you for reading this far. That&rsquo;s the end of this post.</p><p>Or is it?</p><p>You could stop here and be happy with the results. After all we already got a 93x speedup over the PyTorch model.</p><p>But.. if you&rsquo;re like me and you wonder how much more performance we can squeeze out of the model, there&rsquo;s one final trick up our sleeve.</p><h3 id=-bake-pre-processing-into-onnx>🎂 Bake pre-processing into ONNX</h3><p>If you recall, we did our pre-processing transforms outside of the ONNX model in CuPy or NumPy.</p><p>This incurs some data transfer overhead.
We can avoid this overhead by baking the transforms operations into the ONNX model.</p><p>Okay so how do we do this?</p><p>First, we need to write the preprocessing code as a PyTorch module.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Preprocess</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_shape: List[int]):
</span></span><span style=display:flex><span>        super(Preprocess, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>input_shape <span style=color:#f92672>=</span> tuple(input_shape)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mean <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.4815</span>, <span style=color:#ae81ff>0.4578</span>, <span style=color:#ae81ff>0.4082</span>])<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>std <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.2686</span>, <span style=color:#ae81ff>0.2613</span>, <span style=color:#ae81ff>0.2758</span>])<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x: torch<span style=color:#f92672>.</span>Tensor):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>interpolate(
</span></span><span style=display:flex><span>            input<span style=color:#f92672>=</span>x,
</span></span><span style=display:flex><span>            size<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>input_shape[<span style=color:#ae81ff>2</span>:],
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> (x <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>mean) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>std
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>And now export the <code>Preprocess</code> module to ONNX.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>input_shape <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>]
</span></span><span style=display:flex><span>output_onnx_file <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;preprocessing.onnx&#34;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Preprocess(input_shape<span style=color:#f92672>=</span>input_shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>onnx<span style=color:#f92672>.</span>export(
</span></span><span style=display:flex><span>        model,
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>randn(input_shape),
</span></span><span style=display:flex><span>        output_onnx_file,
</span></span><span style=display:flex><span>        opset_version<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>,
</span></span><span style=display:flex><span>        input_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;input_rgb&#34;</span>],
</span></span><span style=display:flex><span>        output_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;output_prep&#34;</span>],
</span></span><span style=display:flex><span>        dynamic_axes<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;input_rgb&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;batch_size&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>2</span>: <span style=color:#e6db74>&#34;height&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>3</span>: <span style=color:#e6db74>&#34;width&#34;</span>,
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Let&rsquo;s visualize the exported <code>preprocessing.onnx</code> model on Netron.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/preprocess_model.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/preprocess_model.png width=auto style=max-width:100%;height:auto></a></figure><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Note the name of the output node of the <code>preprocessing.onnx</code> model - <code>output_preprocessing</code>.</p></div><p>Next, let&rsquo;s visualize the original <code>eva02_large_patch14_448</code> model on Netron.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/original_model.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/original_model.png width=auto style=max-width:100%;height:auto></a></figure><p>Note the name of the input node of the <code>eva02_large_patch14_448</code> model. We will need this for the merge.
The name of the input node is <code>input</code>.</p><p>Now, we merge the <code>preprocessing.onnx</code> model with the <code>eva02_large_patch14_448</code> model.
To achieve this, we need to merge the output of the <code>preprocessing.onnx</code> model with the input of the <code>eva02_large_patch14_448</code> model.</p><p>To merge the models, we use the <code>compose</code> function from the <code>onnx</code> library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> onnx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the models</span>
</span></span><span style=display:flex;background-color:#3c3d38><span>model1 <span style=color:#f92672>=</span> onnx<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;preprocessing.onnx&#34;</span>)
</span></span><span style=display:flex;background-color:#3c3d38><span>model2 <span style=color:#f92672>=</span> onnx<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Merge the models</span>
</span></span><span style=display:flex><span>merged_model <span style=color:#f92672>=</span> onnx<span style=color:#f92672>.</span>compose<span style=color:#f92672>.</span>merge_models(
</span></span><span style=display:flex><span>    model1,
</span></span><span style=display:flex><span>    model2,
</span></span><span style=display:flex;background-color:#3c3d38><span>    io_map<span style=color:#f92672>=</span>[(<span style=color:#e6db74>&#34;output_preprocessing&#34;</span>, <span style=color:#e6db74>&#34;input&#34;</span>)],
</span></span><span style=display:flex><span>    prefix1<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;preprocessing_&#34;</span>,
</span></span><span style=display:flex><span>    prefix2<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model_&#34;</span>,
</span></span><span style=display:flex><span>    doc_string<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Merged preprocessing and eva02_large_patch14_448 model&#34;</span>,
</span></span><span style=display:flex><span>    producer_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;dickson.neoh@gmail.com using onnx compose&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the merged model</span>
</span></span><span style=display:flex><span>onnx<span style=color:#f92672>.</span>save(merged_model, <span style=color:#e6db74>&#34;merged_model_compose.onnx&#34;</span>)</span></span></code></pre></div><p>Note the <code>io_map</code> parameter. This lets us map the output of the preprocessing model to the input of the original model. You must ensure that the input and output names of the models are correct.</p><p>If there are no errors, you will end up with a file called <code>merged_model_compose.onnx</code> in your working directory.
Let&rsquo;s visualize the merged model on Netron.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/merged_model.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/merged_model.png width=auto style=max-width:100%;height:auto></a></figure><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>The merged model expects an input of size <code>[batch_size, 3, height, width]</code>.
This means that the model can take arbitrary input of size height, width and batch size.</p></div><p>Now using this merged model, let&rsquo;s run the inference benchmark again using the TensorRT provider.</p><p>We&rsquo;ll need to make a small change to how the input tensor is passed to the model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_image</span>(image: Image<span style=color:#f92672>.</span>Image):
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#34;RGB&#34;</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(image)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> img_numpy<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(img_numpy, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> img_numpy
</span></span></code></pre></div><p>Notice we are no longer doing the resize and normalization inside the function. This is because the merged model already includes these operations.</p><p>And the results are in!</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>TensorRT&nbsp;with&nbsp;pre-processing:&nbsp;12.875&nbsp;ms&nbsp;per&nbsp;image,&nbsp;FPS:&nbsp;77.67</li><li></li></ul></div></div><p>That&rsquo;s a 8x improvement over the original PyTorch model on the GPU and a whopping 123x improvement over the PyTorch model on the CPU! 🚀</p><p>Let&rsquo;s do a final sanity check on the predictions.</p><div class=window><div class=titlebar><div class=buttons><div class=bclose><a class=closebutton href=#><span><strong>x</strong></span></a></div><div class=minimize><a class=minimizebutton href=#><span><strong>&ndash;</strong></span></a></div><div class=zoom><a class=zoombutton href=#><span><strong>+</strong></span></a></div></div></div><div class=terminal><ul><li></li><li>espresso:&nbsp;34.48%</li><li>cup:&nbsp;2.16%</li><li>chocolate&nbsp;sauce,&nbsp;chocolate&nbsp;syrup:&nbsp;1.53%</li><li>bakery,&nbsp;bakeshop,&nbsp;bakehouse:&nbsp;1.01%</li><li>eggnog:&nbsp;0.98%</li><li></li></ul></div></div><p>Looks like the predictions tally!</p><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>info</p><p>There are small value differences in the confidence values which is likely due to the precision difference between FP32 and FP16 and the normalization difference between the PyTorch model and the ONNX model.</p></div><h3 id=-video-inference>🎮 Video Inference</h3><p>Just for fun, let&rsquo;s see how fast the merged model runs on a video.</p><video controls preload=auto width=100% autoplay loop playsinline class=html-video>
<source src=/portfolio/supercharge_your_pytorch_image_models/inference_video.mp4 type=video/mp4><span></span></video><p>The video inference code is also provided in the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>.</p><h3 id=-conclusion>🚧 Conclusion</h3><p>In this post we have seen how we can supercharge our TIMM models for faster inference using ONNX Runtime and TensorRT.</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>In this post you&rsquo;ve learned how to:</p><ul><li>📥 Load any pre-trained model from <a href=https://huggingface.co/docs/timm/index target=_blank rel="nofollow noopener noreferrer">TIMM</a></li><li>🔄 Convert the model to ONNX format</li><li>🖥️ Run inference with ONNX Runtime (CPU & GPU)</li><li>🎮 Run inference with TensorRT (GPU)</li><li>🛠️ Tweak the TensorRT parameters for better performance</li><li>🧠 Bake the pre-processing into the ONNX model</li></ul><p>You can find the code for this post on my GitHub repository <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/show_me_the_model.jpg class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/show_me_the_model.jpg width=400 style=max-width:100%;height:auto></a></figure><style>#callout{background:#f0f0f0;padding:1.5em 1.25em;border-radius:3px;display:flex;flex-direction:row;margin-bottom:20px}#callout-inner{margin-left:1em}@media(max-width:767px){#callout{padding:1.5em .75em 1.5em .6em}#callout-inner{margin-left:.5em}}</style><div id=callout><div>🤗</div><div id=callout-inner>I uploaded the final model to Hugging Face. So if you want to try it out, you can get it <a href=https://huggingface.co/dnth/eva02_large_patch14_448/blob/main/merged_model_compose.onnx target=_blank rel="nofollow noopener noreferrer">here</a>.</div></div><p>Or simply check out the Hugging Face Spaces demo below.</p><iframe src=https://dnth-eva02-large-patch14-448.hf.space frameborder=0 width=700 height=900></iframe><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>There are other things that we&rsquo;ve not explored in this post that will likely improve the inference speed. For example,</p><ul><li>Quantization - reducing the precision of the model weights from FP32 to FP8, INT8 or even lower.</li><li>Pruning and Sparsity - removing the redundant components of the model to reduce the model size and improve the inference speed.</li><li>Knowledge distillation - training a smaller and faster model to mimic the original model.</li></ul><p>I will leave these as an exercise for the reader. And let me know if you&rsquo;d like me to write a follow-up post on these topics.</p></div><p>Thank you for reading!
I hope this has been helpful. If you&rsquo;d like to find out how to deploy this model on Android check out the following post.</p><div class=blog-page__item style=margin-bottom:20px><div class=blog-page__item-thumb style=margin-bottom:10px><a href=https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/><img src=https://dicksonneoh.com/images/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/post_image.gif alt=post-image style=width:100%;height:auto;display:block></a></div><span class=small style=display:block;margin-bottom:5px>February 7, 2023</span><h5 style="margin:0 0 10px"><a class=text-dark href=https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</a></h5></div><section class=social-share><ul class=share-icons><hr><h5>🤟 Follow me</h5><p>Don't want to miss any of my future content? Follow me on Twitter and LinkedIn where I share these tips in
bite-size posts.</p><li><a href=https://twitter.com/dicksonneoh7 target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn x"><svg viewBox="0 0 24 24" width="24" height="24"><path fill="#fff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>&nbsp;
Twitter</a></li>&nbsp;<li><a href=https://www.linkedin.com/in/dickson-neoh/ target=_blank rel=noopener aria-label="Follow on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg>
&nbsp;
LinkedIn</a></li>&nbsp;<li><a href=https://github.com/dnth/ target=_blank rel=noopener aria-label="Follow on GitHub" class="share-btn github"><svg width="24" height="24" viewBox="0 0 256 250" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid"><g><path d="M128.00106.0C57.3172926.0.0 57.3066942.0 128.00106c0 56.554221 36.6761997 104.534482 87.534937 121.459839 6.3970853 1.18487999999999 8.745651-2.776734 8.745651-6.157566C96.280588 240.251045 96.1618878 230.167899 96.106777 219.472176 60.4967585 227.215235 52.9826207 204.369712 52.9826207 204.369712c-5.8226623-14.795114-14.2122127-18.729174-14.2122127-18.729174C27.1568785 177.696113 39.6458206 177.859325 39.6458206 177.859325 52.4993419 178.762293 59.267365 191.04987 59.267365 191.04987c11.4164025 19.568553 29.9442103 13.911223 37.2485035 10.640612C97.6647155 193.417512 100.981959 187.77078 104.642583 184.574357 76.211799 181.33766 46.324819 170.362144 46.324819 121.315702c0-13.974813 5.0002398-25.3933338 13.1884247-34.3573083-1.3290169-3.2239785-5.7103208-16.2428317 1.2399917-33.8740301.0.0 10.7487147-3.4401823 35.2094058 13.1205959 10.2103258-2.8360835 21.1604058-4.2583646 32.0384188-4.3071163C138.879073 61.9465949 149.837632 63.368876 160.067033 66.2049595c24.431017-16.5607782 35.164893-13.1205959 35.164893-13.1205959C202.199197 70.715562 197.815773 83.7344152 196.486756 86.9583937 204.694018 95.9223682 209.660343 107.340889 209.660343 121.315702c0 49.163023-29.94421 59.988045-58.447062 63.156912 4.591149 3.97221400000001 8.682061 11.761904 8.682061 23.703979.0 17.126724-.14837399999999 30.910768-.14837399999999 35.12674C159.746968 246.709601 162.05102 250.70089 168.53925 249.443941 219.370432 232.499507 256 184.536204 256 128.00106 256 57.3066942 198.691187.0 128.00106.0zM47.9405593 182.340212C47.6586465 182.976105 46.6581745 183.166873 45.7467277 182.730227 44.8183235 182.312656 44.2968914 181.445722 44.5978808 180.80771 44.8734344 180.152739 45.876026 179.97045 46.8023103 180.409216 47.7328342 180.826786 48.2627451 181.702199 47.9405593 182.340212zm6.2962299 5.618042C53.6263318 188.524199 52.4329723 188.261363 51.6232682 187.366874 50.7860088 186.474504 50.6291553 185.281144 51.2480912 184.70672 51.8776254 184.140775 53.0349512 184.405731 53.8743302 185.298101 54.7115892 186.201069 54.8748019 187.38595 54.2367892 187.958254zM58.5562413 195.146347C57.7719732 195.691096 56.4895886 195.180261 55.6968417 194.042013 54.9125733 192.903764 54.9125733 191.538713 55.713799 190.991845 56.5086651 190.444977 57.7719732 190.936735 58.5753181 192.066505 59.3574669 193.22383 59.3574669 194.58888 58.5562413 195.146347zM65.8613592 203.471174C65.1597571 204.244846 63.6654083 204.03712 62.5716717 202.981538 61.4524999 201.94927 61.1409122 200.484596 61.8446341 199.710926 62.5547146 198.935137 64.0575422 199.15346 65.1597571 200.200564 66.2704506 201.230712 66.6095936 202.705984 65.8613592 203.471174zM75.3025151 206.281542C74.9930474 207.284134 73.553809 207.739857 72.1039724 207.313809 70.6562556 206.875043 69.7087748 205.700761 70.0012857 204.687571 70.302275 203.678621 71.7478721 203.20382 73.2083069 203.659543 74.6539041 204.09619 75.6035048 205.261994 75.3025151 206.281542zM86.046947 207.473627C86.0829806 208.529209 84.8535871 209.404622 83.3316829 209.4237 81.8013 209.457614 80.563428 208.603398 80.5464708 207.564772c0-1.066181 1.20183800000001-1.93311500000002 2.7322209-1.958551C84.8005962 205.576546 86.046947 206.424403 86.046947 207.473627zM96.6021471 207.069023C96.7844366 208.099171 95.7267341 209.156872 94.215428 209.438785 92.7295577 209.710099 91.3539086 209.074206 91.1652603 208.052538 90.9808515 206.996955 92.0576306 205.939253 93.5413813 205.66582 95.054807 205.402984 96.4092596 206.021919 96.6021471 207.069023z" fill="#fff"/></g></svg>&nbsp;
GitHub</a></li>&nbsp;<hr><h5>🔄 Share this post</h5><li><a href="https://twitter.com/intent/tweet?&amp;url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f&amp;text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations @dicksonneoh7" target=_blank rel=noopener aria-label="Share on Twitter" class="share-btn x"><svg viewBox="0 0 24 24" width="24" height="24"><path fill="#fff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>&nbsp;
Twitter</a></li>&nbsp;<li><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f&amp;source=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f&amp;title=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations&amp;summary=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations" target=_blank rel=noopener aria-label="Share on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg>
&nbsp;
LinkedIn</a></li>&nbsp;<li><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f" target=_blank rel=noopener aria-label="Share on Facebook" class="share-btn facebook"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="facebook_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-7.8050197" inkscape:cy="32.710925" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.36281,-104.14567)"><path d="m130.36281 104.72294v5.19545c0 .3157.26158.57728.57727.57728h5.19546c.3157.0.57727-.26158.57727-.57728v-5.19545c0-.3157-.26157-.57727-.57727-.57727h-5.19546c-.31569.0-.57727.26157-.57727.57727zm5.77273.0v5.19545h-1.4973v-1.94829h.74865l.10824-.86591h-.85689v-.55923c0-.25256.0631-.42394.42393-.42394h.46904v-.78473c-.0794-.0108-.35719-.0271-.67649-.0271-.66567.0-1.11847.40048-1.11847 1.14553v.64943h-.75767v.86591h.75767v1.94829h-2.79617v-5.19545z" id="path1085" style="stroke-width:.0180398;fill:#fff"/></g></svg>
&nbsp;
Facebook</a></li>&nbsp;<br><li><a href="https://telegram.me/share/url?text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations&amp;url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f" target=_blank rel=noopener aria-label="Share on Telegram" class="share-btn telegram"><svg width="7.3503098mm" height="7.1592798mm" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473"><g><path d="M152.531 179.476c-1.48.0-2.95-.438-4.211-1.293l-47.641-32.316-25.552 18.386c-2.004 1.441-4.587 1.804-6.914.972-2.324-.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821 93.928c-2.886-1.104-4.8-3.865-4.821-6.955-.021-3.09 1.855-5.877 4.727-7.02l174.312-69.36c.791-.336 1.628-.53 2.472-.582.302-.018.605-.018.906-.001 1.748.104 3.465.816 4.805 2.13.139.136.271.275.396.42 1.11 1.268 1.72 2.814 1.835 4.389.028.396.026.797-.009 1.198-.024.286-.065.571-.123.854L159.898 173.38c-.473 2.48-2.161 4.556-4.493 5.523C154.48 179.287 153.503 179.476 152.531 179.476zm-47.669-48.897 42.437 28.785L170.193 39.24l-82.687 79.566 17.156 11.638C104.731 130.487 104.797 130.533 104.862 130.579zM69.535 124.178l5.682 21.53 12.242-8.809-16.03-10.874C70.684 125.521 70.046 124.893 69.535 124.178zM28.136 86.782l31.478 12.035c2.255.862 3.957 2.758 4.573 5.092l3.992 15.129c.183-1.745.974-3.387 2.259-4.624L149.227 38.6 28.136 86.782z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
&nbsp;
Telegram</a></li>&nbsp;<li><a href="whatsapp://send?text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aSupercharge%20your%20PyTorch%20image%20models%20with%20ONNX%20Runtime%20and%20TensorRT.%20Learn%20step-by-step%20techniques%20to%20achieve%20up%20to%208x%20faster%20inference%20speeds%2c%20enabling%20real-time%20performance%20for%20computer%20vision%20applications.%20Optimize%20TIMM%20models%20for%20deployment%20using%20advanced%20acceleration%20methods.%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f%0a" target=_blank aria-label="Share on WhatsApp" class="share-btn whatsapp"><svg width="6.0324998mm" height="6.05896mm" viewBox="0 0 6.0324997 6.05896" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="whatsapp_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="4.9987205" inkscape:cy="35.692618" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-126.67735,-103.17712)"><path d="m131.83672 104.07671c-.58208-.58209-1.34937-.89959-2.14312-.89959-1.64042.0-2.98979 1.34938-2.98979 3.01625.0.52917.13229 1.03188.39687 1.48167l-.42333 1.56104 1.5875-.42333c.42333.23812.92604.34396 1.42875.34396 1.66687.0 3.01625-1.34938 3.01625-3.01625-.0265-.74084-.3175-1.50813-.87313-2.06375zm-2.14312 4.6302c-.44979.0-.87313-.13229-1.27-.34395l-.10583-.0529-.92605.26458.26459-.89958-.0794-.13229c-.26458-.39688-.37041-.84667-.37041-1.34938.0-1.37583 1.11125-2.48708 2.48708-2.48708.66146.0 1.29646.26458 1.77271.74083.47625.47625.74083 1.11125.74083 1.77271-.0265 1.37583-1.13771 2.48708-2.51354 2.48708zm1.34937-1.87854c-.0794-.0265-.44979-.23812-.5027-.26458-.0794-.0265-.1323-.0265-.18521.0265-.0529.0794-.21167.26458-.23813.29104-.0529.0529-.0794.0529-.15875.0265-.0794-.0265-.3175-.13229-.60854-.37042-.23812-.21167-.37042-.44979-.42333-.52917-.0529-.0794.0-.13229.0265-.15875.0265-.0264.0794-.0794.10583-.13229.0529-.0265.0794-.0794.10583-.13229.0265-.0529.0-.10583.0-.13229.0-.0265-.18521-.39688-.26458-.55563-.0265-.10583-.10583-.0794-.13229-.0794h-.15875s-.10584.0265-.18521.0794c-.0794.0794-.26458.26459-.26458.635.0.37042.26458.74084.29104.79375.0265.0529.52916.82021 1.29646 1.13771.1852.0794.3175.13229.42333.15875.18521.0529.34396.0529.47625.0265.15875-.0265.44979-.18521.50271-.34396.0529-.18521.0529-.3175.0529-.34396-.0264-.0794-.0794-.10583-.15875-.13229z" id="path1793" style="stroke-width:.264583;fill:#fff"/></g></svg>
&nbsp;
WhatsApp</a></li>&nbsp;<li><a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations.&amp;body=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aSupercharge%20your%20PyTorch%20image%20models%20with%20ONNX%20Runtime%20and%20TensorRT.%20Learn%20step-by-step%20techniques%20to%20achieve%20up%20to%208x%20faster%20inference%20speeds%2c%20enabling%20real-time%20performance%20for%20computer%20vision%20applications.%20Optimize%20TIMM%20models%20for%20deployment%20using%20advanced%20acceleration%20methods.%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f%0a" target=_blank class="share-btn email" aria-label="Share via Email"><svg width="6.3499999mm" height="4.3961601mm" viewBox="0 0 6.3499999 4.3961601" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="email_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.7526575" inkscape:cy="33.4125" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.10375,-103.97942)"><path d="m130.10375 104.22365v3.9077.24423h.24423 5.86154.24423v-.24423-3.9077-.24423h-.24423-5.86154-.24423zm5.29675.24423-2.12175 1.41196-2.12175-1.41196zm-2.25913 1.91569.13738.0839.13738-.0839 2.54916-1.70198v3.20553h-5.37308v-3.20553z" id="path824" style="stroke-width:.0152644;fill:#fff"/></g></svg>
&nbsp;
Email</a></li><hr><section><h5>❤️ Show some love</h5><p>Creating free ML contents doesn't pay my bills. Support me in creating more free contents like these.
Consider buying me a coffee. Your support means a lot to me.</p><div style=text-align:center><a href=https://www.buymeacoffee.com/dicksonneoh target=_blank><img src=https://cdn.buymeacoffee.com/buttons/v2/default-blue.png alt="Buy Me A Coffee" style=height:60px!important;width:217px!important></a></div></section><hr></ul></section></div></div></div><div class=row><div class="col-lg-10 offset-lg-1"><nav class="case-details-nav d-flex justify-content-between align-items-start"><div class=previous><div class="d-flex align-items-center mb-3"><div class="icon mr-3"><svg width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1243" fill="#2d2d2d"><path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z"/><path data-name="Path 1455" d="M13.137 1.41 3.39 15.558l-.975-1.415L12.166.0z"/></g></svg></div><span class=small>Prev blog</span></div><div class=blog-nav-item><div class=blog-nav-thumb><a href=https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/><img src=https://dicksonneoh.com/images/portfolio/bringing_high_quality_image_models_to_mobile/thumbnail.gif alt=post-image></a></div><h5 class=title><a class=text-dark href=https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android & iOS</a></h5></div></div></nav></div></div></div></section></div><script src=/js/toc-highlight.min.b1868ed76863ca9764b9bc4edd7ae04f53476f42dc584e1c6ad1a842c998fa42.js integrity="sha256-sYaO12hjypdkubxO3XrgT1NHb0LcWE4catGoQsmY+kI="></script><section class=footer id=contact><div class=footer__background_shape><svg viewBox="0 0 1920 79"><path d="M0 0h1920v79L0 0z" data-name="Path 1450"/></svg></div><div class=container><div class=row><div class=col-lg-12><div class=footer__cta><div class=shape-1><svg width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class=shape-2><svg width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class="text-light footer__cta_content"><span>Contact me</span><h2 class="mb-0 mb-3">Let’s Start a Project</h2></div><div class=footer__cta_action></div><a href="https://api.whatsapp.com/send?phone=60133250827" rel=noopener target=_blank class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-whatsapp"></i>&nbsp;&nbsp;Chat on WhatsApp</a>
<a href=https://t.me/dicksonneoh rel=noopener target=_blank class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-telegram"></i>&nbsp;&nbsp;Chat on Telegram</a></div></div></div><div class="row footer__widget"><div class=col-lg-4><div class="footer__widget_logo mb-5"><img src=https://dicksonneoh.com/images/site-navigation/logo_resized.png alt=widget-logo></div></div><div class=col-lg-4><div class="text-light footer__widget_sitemap mb-5"><h4 class=base-font>Sitemap</h4><ul class="unstyle-list small"><li class=mb-2><a class=text-light href=https://dicksonneoh.com/#about>About me</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Frequently Ask Question</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Privacy & Policy</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/#portfolio>Latest Article</a></li></ul></div></div><div class=col-lg-4><div class="text-light footer__widget_address mb-5"><h4 class=base-font>Address</h4><ul class="fa-ul small"><li class=mb-2><a class=text-light href=tel:+%2860%29%203%208921%202020><span class=fa-li><i class="fa fa-phone"></i></span>+(60) 3 8921 2020</a></li><li class=mb-2><a class=text-light href=mailto:dickson.neoh@gmail.com><span class=fa-li><i class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li><li class=mb-2><span class=fa-li><i class="fa fa-map-marker"></i></span>Kuala Lumpur, Malaysia.</a></li></ul></div></div></div><div class="row footer__footer"><div class=col-lg-6><div class="footer__footer_copy text-light"><p>All right reserved copyright © Dickson Neoh 2024</p></div></div><div class=col-lg-6><div class=footer__footer_social><ul class=unstyle-list><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://www.linkedin.com/in/dickson-neoh/><i class="fa fa-linkedin-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://twitter.com/dicksonneoh7><i class="fa fa-twitter-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://github.com/dnth><i class="fa fa-github-square"></i></a></li></ul></div></div></div></div></section><script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script><script src=https://dicksonneoh.com/plugins/jQuery/jquery.min.js></script><script src=https://dicksonneoh.com/plugins/bootstrap/bootstrap.min.js></script><script src=https://dicksonneoh.com/plugins/slick/slick.min.js></script><script src=https://dicksonneoh.com/plugins/waypoint/jquery.waypoints.min.js></script><script src=https://dicksonneoh.com/plugins/magnafic-popup/jquery.magnific-popup.min.js></script><script src=https://dicksonneoh.com/plugins/tweenmax/TweenMax.min.js></script><script src=https://dicksonneoh.com/plugins/imagesloaded/imagesloaded.min.js></script><script src=https://dicksonneoh.com/plugins/masonry/masonry.min.js></script><script src=https://dicksonneoh.com/js/form-handler.min.js></script><script src=https://dicksonneoh.com/js/script.min.js></script></body></html>