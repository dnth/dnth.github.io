<!doctype html><html><head><meta charset=utf-8><title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations</title><meta name=description content="Learn how to optimize PyTorch image models using ONNX Runtime and TensorRT, achieving up to 8x faster inference speeds for real-time applications."><meta property="og:title" content="Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations"><meta property="og:description" content="Learn how to optimize PyTorch image models using ONNX Runtime and TensorRT, achieving up to 8x faster inference speeds for real-time applications."><meta property="og:type" content="article"><meta property="og:url" content="https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/"><meta property="og:image" content="https://dicksonneoh.com/images/portfolio/supercharge_your_pytorch_image_models/post_image.jpg"><meta property="article:section" content="portfolio"><meta property="article:published_time" content="2024-09-09T09:00:00+08:00"><meta property="article:modified_time" content="2024-09-09T09:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dicksonneoh.com/images/portfolio/supercharge_your_pytorch_image_models/post_image.jpg"><meta name=twitter:title content="Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations"><meta name=twitter:description content="Learn how to optimize PyTorch image models using ONNX Runtime and TensorRT, achieving up to 8x faster inference speeds for real-time applications."><meta name=viewport content="width=device-width,initial-scale=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick-theme.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/magnafic-popup/magnific-popup.css><link href=https://dicksonneoh.com/scss/style.min.css rel=stylesheet><link rel="shortcut icon" href=https://dicksonneoh.com/images/favicon-purple.ico type=image/x-icon><link rel=icon href=https://dicksonneoh.com/images/favicon.gif type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-54500366-2")</script></head><body><nav class="navbar navbar-expand-lg fixed-top"><div class=container><a href=https://dicksonneoh.com/ class=navbar-brand><img src=https://dicksonneoh.com/images/site-navigation/logo_resized.png alt=site-logo></a>
<button type=button class="navbar-toggler collapsed" data-toggle=collapse data-target=#navbarCollapse>
<span class=navbar-toggler-icon></span>
<span class=navbar-toggler-icon></span>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-between" id=navbarCollapse><ul class="nav navbar-nav main-navigation my-0 mx-auto"><li class=nav-item><a href=https://dicksonneoh.com/#home class="nav-link text-dark text-sm-center p-2">Home</a></li><li class=nav-item><a href=https://dicksonneoh.com/#about class="nav-link text-dark text-sm-center p-2">About</a></li><li class=nav-item><a href=https://dicksonneoh.com/#service class="nav-link text-dark text-sm-center p-2">Services</a></li><li class=nav-item><a href=https://dicksonneoh.com/#portfolio class="nav-link text-dark text-sm-center p-2">Projects</a></li><li class=nav-item><a href=https://dicksonneoh.com/#resume class="nav-link text-dark text-sm-center p-2">Resume</a></li><li class=nav-item><a href=https://dicksonneoh.com/#skills class="nav-link text-dark text-sm-center p-2">Skills</a></li><li class=nav-item><a href=https://dicksonneoh.com/#blog class="nav-link text-dark text-sm-center p-2">Blogs</a></li><li class=nav-item><a href=https://dicksonneoh.com/#contact class="nav-link text-dark text-sm-center p-2">Contact</a></li></ul><div class=navbar-nav><a href=https://dicksonneoh.com/contact class="btn btn-primary btn-zoom hire_button">Hire Me Now</a></div></div></div></nav><div id=content><header class=breadCrumb><div class=container><div class=row><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><h3 class=breadCrumb__title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime & Optimizations</h3><nav aria-label=breadcrumb class="d-flex justify-content-center"></nav></div></div><div class="row p-3"><div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center"><i class="fa fa-calendar"></i> &ensp;
September 9, 2024 &ensp; &ensp;
<i class="fa fa-clock-o"></i> &ensp;
16 mins read</div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-tag"></i> &ensp;
<a href=https://dicksonneoh.com/tags/timm/>TIMM</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/tags/onnx/>ONNX</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/tags/tensorrt/>TensorRT</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/tags/imagenet/>ImageNet</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/tags/hugging-face/>Hugging Face</a></div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-folder"></i> &ensp;
<a href=https://dicksonneoh.com/categories/inference/>inference</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/categories/deployment/>deployment</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/categories/image-classification/>image-classification</a>
<span class=separator>•</span>
<a href=https://dicksonneoh.com/categories/optimization/>optimization</a></div></div></div></header><section class="section singleBlog"><div class=svg-img><img src=https://dicksonneoh.com/images/hero/figure-svg.svg alt></div><div class=animate-shape><img src=https://dicksonneoh.com/images/skill/skill-background-shape.svg alt><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600"><defs><linearGradient id="d" x1=".929" y1=".111" x2=".263" y2=".935" gradientUnits="objectBoundingBox"><stop offset="0" stop-color="#f1f6f9"/><stop offset="1" stop-color="#f1f6f9" stop-opacity="0"/></linearGradient></defs><g data-name="blob-shape (3)"><path class="blob" fill="url(#d)" d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z"/></g></svg></div><div class=animate-pattern><img src=https://dicksonneoh.com/images/service/background-pattern.svg alt=background-shape></div><div class=container><div class=row><div class=col-lg-12><div class=singleBlog__feature><img src=https://dicksonneoh.com/images/portfolio/supercharge_your_pytorch_image_models/post_image.png alt=feature-image></div></div></div><div class="row mt-5"><div class=col-lg-12><div class=singleBlog__content><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a class=toc-link href=#-motivation>🚀 Motivation</a></li><li><a class=toc-link href=#-installation>💻 Installation</a></li><li><a class=toc-link href=#-load-and-infer>🔧 Load and Infer</a></li><li><a class=toc-link href=#-baseline-latency>⏱️ Baseline Latency</a></li><li><a class=toc-link href=#-convert-to-onnx>🔄 Convert to ONNX</a></li><li><a class=toc-link href=#-onnx-runtime-on-cpu>🖥️ ONNX Runtime on CPU</a></li><li><a class=toc-link href=#-onnx-runtime-on-cuda>🖼️ ONNX Runtime on CUDA</a></li><li><a class=toc-link href=#-onnx-runtime-on-tensorrt>📊 ONNX Runtime on TensorRT</a></li><li><a class=toc-link href=#-bake-pre-processing-into-onnx>🎂 Bake pre-processing into ONNX</a></li><li><a class=toc-link href=#-conclusion>🚧 Conclusion</a></li></ul></nav><div class=floating-toc><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a class=toc-link href=#-motivation>🚀 Motivation</a></li><li><a class=toc-link href=#-installation>💻 Installation</a></li><li><a class=toc-link href=#-load-and-infer>🔧 Load and Infer</a></li><li><a class=toc-link href=#-baseline-latency>⏱️ Baseline Latency</a></li><li><a class=toc-link href=#-convert-to-onnx>🔄 Convert to ONNX</a></li><li><a class=toc-link href=#-onnx-runtime-on-cpu>🖥️ ONNX Runtime on CPU</a></li><li><a class=toc-link href=#-onnx-runtime-on-cuda>🖼️ ONNX Runtime on CUDA</a></li><li><a class=toc-link href=#-onnx-runtime-on-tensorrt>📊 ONNX Runtime on TensorRT</a></li><li><a class=toc-link href=#-bake-pre-processing-into-onnx>🎂 Bake pre-processing into ONNX</a></li><li><a class=toc-link href=#-conclusion>🚧 Conclusion</a></li></ul></nav></div><hr><h3 id=-motivation>🚀 Motivation</h3><p>Having real time inference is crucial for many computer vision models.
In some applications, a 1-second delay in inference could mean life or death.</p><p>Imagine you&rsquo;re behind the wheels of a self-driving car and the car takes one full second to detect an oncoming truck.
Just one second too late, and you could end up in the clouds 👼👼👼</p><p>Or if you&rsquo;re lucky, on the ground.</p><figure><img src=banana_peel_robot.gif width=480></figure><p>I hope that shows you how crucial this problem is.</p><p>Today (2024), computer vision models are being deployed in all kinds of high-stakes industries like healthcare, finance, and self-driving cars.</p><blockquote class=blockquote><p class=mb-0>We are at a point where it&rsquo;s not just about being right - it&rsquo;s about being right, right now.</p></blockquote><p>With many high-stakes applications, having real-time inference is crucial and will determine whether the model gets deployed or not.
In many cases, you can pick one or the other:</p><ul><li>A fast model with low accuracy</li><li>A slow model with high accuracy</li></ul><p>But can we have the best of both worlds? I.e. a fast and accurate model?</p><p>That&rsquo;s what this post is about.</p><style type=text/css>.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:#fff;background:#6ab0de;text-transform:uppercase}.notice.warning .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info .notice-title{background:#f0b37e}.notice.info{background:#fff2db}.notice.note .notice-title{background:#6ab0de}.notice.note{background:#e7f2fa}.notice.tip .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>By the end of the post you&rsquo;ll learn how to supercharge the inference speed of any image models from <a href=https://huggingface.co/docs/timm/index target=_blank rel="nofollow noopener noreferrer">TIMM</a> with optimized <a href=https://onnxruntime.ai/ target=_blank rel="nofollow noopener noreferrer">ONNX Runtime</a> and <a href=https://developer.nvidia.com/tensorrt target=_blank rel="nofollow noopener noreferrer">TensorRT</a>.</p><p>In short:</p><ul><li>📥 Load any pre-trained model from <a href=https://huggingface.co/docs/timm/index target=_blank rel="nofollow noopener noreferrer">TIMM</a>.</li><li>🔄 Convert the model to ONNX format.</li><li>🖥️ Run inference with <a href=https://onnxruntime.ai/ target=_blank rel="nofollow noopener noreferrer">ONNX Runtime</a> (CPU & CUDA Provider).</li><li>🎮 Run inference with <a href=https://developer.nvidia.com/tensorrt target=_blank rel="nofollow noopener noreferrer">TensorRT</a> provider and optimized runtime parameters.</li><li>🧠 Bake the pre-processing into the ONNX model for faster inference.</li></ul><p>You can find the code for this post on my GitHub repository <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><p>If that sounds exciting let&rsquo;s dive in! 🏊‍♂️</p><h3 id=-installation>💻 Installation</h3><p>Let&rsquo;s begin with the installation.
I will be using a <code>conda</code> environment to install the packages required for this post. Feel free to the environment of your choice.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create -n supercharge_timm_tensorrt python<span style=color:#f92672>=</span>3.11
</span></span><span style=display:flex><span>conda activate supercharge_timm_tensorrt
</span></span></code></pre></div><p>We&rsquo;ll be using the <a href=https://github.com/huggingface/pytorch-image-models target=_blank rel="nofollow noopener noreferrer"><code>timm</code></a> library to load a pre-trained model and run inference. So let&rsquo;s install <code>timm</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install timm
</span></span></code></pre></div><p>At the time of writing, there are over <a href=https://huggingface.co/timm target=_blank rel="nofollow noopener noreferrer">1370 models</a> available in timm. Any of which can be used in this post.</p><h3 id=-load-and-infer>🔧 Load and Infer</h3><p>To prove my point in the motivation section, let&rsquo;s load a top performing model from the timm <a href=https://huggingface.co/spaces/timm/leaderboard target=_blank rel="nofollow noopener noreferrer">leaderboard</a> - the <code>eva02_large_patch14_448.mim_m38m_ft_in22k_in1k</code> model and run an inference to see how it performs.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/eva_timm.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/eva_timm.png srcset="/portfolio/supercharge_your_pytorch_image_models/eva_timm_hu2af30c01d2d0567e0449bf40e4649f57_74208_360x0_resize_box_3.png 360w, /portfolio/supercharge_your_pytorch_image_models/eva_timm_hu2af30c01d2d0567e0449bf40e4649f57_74208_720x0_resize_box_3.png 720w, /portfolio/supercharge_your_pytorch_image_models/eva_timm_hu2af30c01d2d0567e0449bf40e4649f57_74208_1920x0_resize_box_3.png 1920w" sizes="(max-width: auto) 100vw, auto" width=auto style=max-width:100%;height:auto></a></figure><p>The plot above shows the accuracy vs inference speed for the EVA02 model.</p><p>Look closely, the EVA02 model achieves top ImageNet accuracy (90.05% top-1, 99.06% top-5) but is lags in speed.
Check out the model on the <code>timm</code> leaderboard <a href=https://huggingface.co/spaces/timm/leaderboard target=_blank rel="nofollow noopener noreferrer">here</a>.</p><p>So let&rsquo;s get the EVA02 model on our local machine</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> timm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k&#39;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>create_model(model_name, pretrained<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>eval()
</span></span></code></pre></div><p>Get the data config and transformations for the model</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data_config <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>resolve_model_data_config(model)
</span></span><span style=display:flex><span>transforms <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>create_transform(<span style=color:#f92672>**</span>data_config, is_training<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p>And run an inference to get the top 5 predictions</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> urllib.request <span style=color:#f92672>import</span> urlopen
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(urlopen(<span style=color:#e6db74>&#39;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>inference_mode():
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> model(transforms(img)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>top5_probabilities, top5_class_indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>topk(output<span style=color:#f92672>.</span>softmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>100</span>, k<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
</span></span></code></pre></div><p>Next, decode the predictions to get the class names</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> imagenet_classes <span style=color:#f92672>import</span> IMAGENET2012_CLASSES
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>im_classes <span style=color:#f92672>=</span> list(IMAGENET2012_CLASSES<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>class_names <span style=color:#f92672>=</span> [im_classes[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> top5_class_indices[<span style=color:#ae81ff>0</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, prob <span style=color:#f92672>in</span> zip(class_names, top5_probabilities[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>prob<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/beignets-task-guide.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/beignets-task-guide.png srcset="/portfolio/supercharge_your_pytorch_image_models/beignets-task-guide_hua035be20aec9609464fb2533583fb71e_357460_360x0_resize_box_3.png 360w, /portfolio/supercharge_your_pytorch_image_models/beignets-task-guide_hua035be20aec9609464fb2533583fb71e_357460_720x0_resize_box_3.png 720w, /portfolio/supercharge_your_pytorch_image_models/beignets-task-guide_hua035be20aec9609464fb2533583fb71e_357460_1920x0_resize_box_3.png 1920w" sizes="(max-width: 400) 100vw, 400" width=400 style=max-width:100%;height:auto></a></figure><p>Top 5 predictions:</p><pre tabindex=0><code>&gt;&gt;&gt; espresso: 26.78%
&gt;&gt;&gt; eggnog: 2.88%
&gt;&gt;&gt; cup: 2.60%
&gt;&gt;&gt; chocolate sauce, chocolate syrup: 2.39%
&gt;&gt;&gt; bakery, bakeshop, bakehouse: 1.48%
</code></pre><p>The predictions looks fine. Looks like the model is doing it&rsquo;s job.</p><p>Now let&rsquo;s benchmark the inference latency.</p><h3 id=-baseline-latency>⏱️ Baseline Latency</h3><p>We will run the inference 10 times (more is better, but 10 is enough to see the difference) and record the average time on both CPU and GPU.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_benchmark</span>(model, device, num_images<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>inference_mode():
</span></span><span style=display:flex><span>        start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_images):
</span></span><span style=display:flex><span>            input_tensor <span style=color:#f92672>=</span> transforms(img)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>            model(input_tensor)
</span></span><span style=display:flex><span>        end <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    ms_per_image <span style=color:#f92672>=</span> (end <span style=color:#f92672>-</span> start) <span style=color:#f92672>/</span> num_images <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    fps <span style=color:#f92672>=</span> num_images <span style=color:#f92672>/</span> (end <span style=color:#f92672>-</span> start)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;PyTorch model on </span><span style=color:#e6db74>{</span>device<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>ms_per_image<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> ms per image, FPS: </span><span style=color:#e6db74>{</span>fps<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># CPU Benchmark</span>
</span></span><span style=display:flex><span>run_benchmark(model, torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cpu&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># GPU Benchmark </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available():
</span></span><span style=display:flex><span>    run_benchmark(model, torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda&#34;</span>))
</span></span></code></pre></div><p>Alright the benchmarks are in</p><pre tabindex=0><code>&gt;&gt;&gt; PyTorch model on cpu: 1584.379 ms per image, FPS: 0.63
&gt;&gt;&gt; PyTorch model on cuda: 77.226 ms per image, FPS: 12.95
</code></pre><p>Although the performance on the GPU is not bad, 12+ FPS is still not fast enough for real-time inference.
On my reasonably modern CPU, it took over 1.5 seconds to run an inference.</p><p>Definitely not self-driving car material.</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>I&rsquo;m using the following hardware for the benchmarks:</p><ul><li>GPU - NVIDIA RTX 3090</li><li>CPU - 11th Gen Intel® Core™ i9-11900 @ 2.50GHz × 16</li></ul><p>You can find the code for the PyTorch benchmarks on my GitHub repository <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost/blob/main/01_pytorch_latency_benchmark.py target=_blank rel="nofollow noopener noreferrer">here</a>.</p><p>If you&rsquo;ve cloned the repo, you can run the benchmarks by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 01_pytorch_latency_benchmark.py
</span></span></code></pre></div></div><p>Now let&rsquo;s begin the first step to improve the run time.</p><h3 id=-convert-to-onnx>🔄 Convert to ONNX</h3><p><a href=https://onnx.ai/ target=_blank rel="nofollow noopener noreferrer">ONNX</a> is an open and interoperable format for deep learning models. It lets us deploy models across different frameworks and devices.</p><p>The key advantage of using ONNX is that it lets us deploy models across different frameworks and devices.</p><p>To convert the model to ONNX format, let&rsquo;s first install <code>onnx</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install onnx
</span></span></code></pre></div><p>And export the model to ONNX format</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> timm
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> timm<span style=color:#f92672>.</span>create_model(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;eva02_large_patch14_448.mim_m38m_ft_in22k_in1k&#34;</span>, pretrained<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>onnx<span style=color:#f92672>.</span>export(
</span></span><span style=display:flex><span>    model,
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>),
</span></span><span style=display:flex><span>    onnx_filename,
</span></span><span style=display:flex><span>    export_params<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    opset_version<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>,
</span></span><span style=display:flex><span>    do_constant_folding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    input_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;input&#34;</span>],
</span></span><span style=display:flex><span>    output_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;output&#34;</span>],
</span></span><span style=display:flex><span>    dynamic_axes<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;input&#34;</span>: {<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;batch_size&#34;</span>}, 
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;output&#34;</span>: {<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;batch_size&#34;</span>}},
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Here are the descriptions for the arguments you can pass to the <code>torch.onnx.export</code> function:</p><ul><li><code>torch.randn(1, 3, 448, 448)</code>: A dummy input tensor with the appropriate shape.</li><li><code>export_params=True</code>: Whether to export the model parameters.</li><li><code>do_constant_folding=True</code>: Whether to do constant folding for optimization.</li><li><code>input_names=['input']</code>: The name of the input tensor.</li><li><code>output_names=['output']</code>: The name of the output tensor.</li><li><code>dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}</code>: Dynamic axes for the input and output tensors.</li></ul><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the ONNX conversion by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 02_convert_to_onnx.py
</span></span></code></pre></div></div><p>If there are no errors, you will end up with a file called <code>eva02_large_patch14_448.onnx</code> in your working directory.</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>You can inspect and visualize the ONNX model using the <a href=https://netron.app/ target=_blank rel="nofollow noopener noreferrer">Netron</a> webapp.</p></div><h3 id=-onnx-runtime-on-cpu>🖥️ ONNX Runtime on CPU</h3><p>To run the and inference on the ONNX model, we need to install <code>onnxruntime</code>. This is the &rsquo;engine&rsquo; that will run the ONNX model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install onnxruntime
</span></span></code></pre></div><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>One (major) benefit of using ONNX Runtime is the ability to run the model without PyTorch as a dependency.</p><p>This is great for deployment and for running inference in environments where PyTorch is not available.</p></div><p>The ONNX model we exported earlier only includes the model weights and the graph structure. It does not include the pre-processing transforms.
To run the inference using <code>onnxruntime</code>, we need to replicate the PyTorch transforms.</p><p>To find out the transforms that was used, you can print out the <code>transforms</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(transforms)
</span></span></code></pre></div><pre tabindex=0><code>&gt;&gt;&gt; Compose(
&gt;&gt;&gt;     Resize(size=(448, 448), interpolation=bicubic, max_size=None, antialias=True)
&gt;&gt;&gt;     CenterCrop(size=(448, 448))
&gt;&gt;&gt;     MaybeToTensor()
&gt;&gt;&gt;     Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
&gt;&gt;&gt; )
</code></pre><p>Now let&rsquo;s replicate the transforms using <code>numpy</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transforms_numpy</span>(image: PIL<span style=color:#f92672>.</span>Image<span style=color:#f92672>.</span>Image):
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#39;RGB&#39;</span>)
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>resize((<span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>), Image<span style=color:#f92672>.</span>BICUBIC)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(image)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> img_numpy<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>])<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    std <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>0.225</span>])<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> (img_numpy <span style=color:#f92672>-</span> mean) <span style=color:#f92672>/</span> std
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(img_numpy, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> img_numpy<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> img_numpy
</span></span></code></pre></div><p>Once the transforms are replicated, let&rsquo;s run inference with ONNX Runtime.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> onnxruntime <span style=color:#66d9ef>as</span> ort
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create ONNX Runtime session with CPU provider</span>
</span></span><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> ort<span style=color:#f92672>.</span>InferenceSession(
</span></span><span style=display:flex><span>    onnx_filename, 
</span></span><span style=display:flex><span>    providers<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;CPUExecutionProvider&#34;</span>]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get input and output names</span>
</span></span><span style=display:flex><span>input_name <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>get_inputs()[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>output_name <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>get_outputs()[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run inference</span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>run([output_name], 
</span></span><span style=display:flex><span>                    {input_name: transforms_numpy(img)})[<span style=color:#ae81ff>0</span>]
</span></span></code></pre></div><p>If we inspect the output shape, we can see that it&rsquo;s the same as the number of classes in the ImageNet dataset.</p><pre tabindex=0><code>output.shape
&gt;&gt;&gt; (1, 1000)
</code></pre><p>And printing the top 5 predictions:</p><pre tabindex=0><code>&gt;&gt;&gt; espresso: 28.65%
&gt;&gt;&gt; cup: 2.77%
&gt;&gt;&gt; eggnog: 2.28%
&gt;&gt;&gt; chocolate sauce, chocolate syrup: 2.13%
&gt;&gt;&gt; bakery, bakeshop, bakehouse: 1.42%
</code></pre><p>While the results aren&rsquo;t an exact match to the PyTorch model, they&rsquo;re sufficiently similar. This slight variation can be attributed to differences in how normalization is implemented, leading to minor discrepancies in the precise values.</p><p>Now let&rsquo;s benchmark the inference latency on ONNX Runtime with a CPU provider (backend).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_images <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_images):
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>run([output_name], {input_name: transforms_numpy(img)})[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>end <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>perf_counter()
</span></span><span style=display:flex><span>time_taken <span style=color:#f92672>=</span> end <span style=color:#f92672>-</span> start
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ms_per_image <span style=color:#f92672>=</span> time_taken <span style=color:#f92672>/</span> num_images <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>fps <span style=color:#f92672>=</span> num_images <span style=color:#f92672>/</span> time_taken
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Onnxruntime CPU: </span><span style=color:#e6db74>{</span>ms_per_image<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> ms per image, FPS: </span><span style=color:#e6db74>{</span>fps<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><pre tabindex=0><code>&gt;&gt;&gt; Onnxruntime CPU: 2002.446 ms per image, FPS: 0.50
</code></pre><p>Ouch! That&rsquo;s slower than the PyTorch model. What a bummer!
It may seem like a step back, but we are only getting started.</p><p>Read on.</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the ONNX Runtime CPU inference by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 03_onnx_cpu_inference.py
</span></span></code></pre></div></div><h3 id=-onnx-runtime-on-cuda>🖼️ ONNX Runtime on CUDA</h3><p>Other than the CPU, ONNX Runtime offers other backends for inference. We can easily swap to a different backend by changing the provider. In this case we will use the CUDA backend.</p><p>To use the CUDA backend, we need to install the <code>onnxruntime-gpu</code> package.<div class="notice warning"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#warning-notice"/></svg></span>warning</p><p>You <strong>must</strong> uninstall the <code>onnxruntime</code> package before installing the <code>onnxruntime-gpu</code> package.</p><p>Run the following to uninstall the <code>onnxruntime</code> package.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip uninstall onnxruntime
</span></span></code></pre></div><p>Then install the <code>onnxruntime-gpu</code> package.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install onnxruntime-gpu<span style=color:#f92672>==</span>1.19.2
</span></span></code></pre></div><p>The <code>onnxruntime-gpu</code> package requires a compatible CUDA and cuDNN version. I&rsquo;m running on <code>onnxruntime-gpu==1.19.2</code> at the time of writing this post. This version is compatible with CUDA <code>12.x</code> and cuDNN <code>9.x</code>.</p><p>See the compatibility matrix <a href=https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div></p><p>You can install all the CUDA dependencies using conda with the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install -c nvidia cuda<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-tools<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-toolkit<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-version<span style=color:#f92672>=</span>12.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-command-line-tools<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-compiler<span style=color:#f92672>=</span>12.2.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                 cuda-runtime<span style=color:#f92672>=</span>12.2.2
</span></span></code></pre></div><p>Once done, replace the CPU provider with the CUDA provider.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> ort<span style=color:#f92672>.</span>InferenceSession(
</span></span><span style=display:flex><span>    onnx_filename, 
</span></span><span style=display:flex><span>    providers<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;CUDAExecutionProvider&#34;</span>] <span style=color:#75715e># change the provider </span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>The rest of the code is the same as the CPU inference.</p><p>Just with one line of code change, the benchmarks are as follows:</p><pre tabindex=0><code>&gt;&gt;&gt; Onnxruntime CUDA numpy transforms: 56.430 ms per image, FPS: 17.72
</code></pre><p>But that&rsquo;s kinda expected. Running on the GPU, we should expect a speedup.</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>If you encounter the following error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Failed to load library libonnxruntime_providers_cuda.so 
</span></span><span style=display:flex><span>with error: libcublasLt.so.12: cannot open shared object 
</span></span><span style=display:flex><span>file: No such file or directory
</span></span></code></pre></div><p>It means that the CUDA library is not in the library path.
You need to export the library path to include the CUDA library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export LD_LIBRARY_PATH<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib:</span>$LD_LIBRARY_PATH<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><p>Replace the <code>/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib</code> with the path to your CUDA library.</p></div><p>Theres is one more trick we can use to squeeze out more performance - using <a href=https://cupy.dev/ target=_blank rel="nofollow noopener noreferrer">CuPy</a> for the transforms instead of NumPy.</p><p>CuPy is a library that lets us run NumPy code on the GPU. It&rsquo;s a drop-in replacement for NumPy, so you can just replace <code>numpy</code> with <code>cupy</code> in your code and it will run on the GPU.</p><p>Let&rsquo;s install CuPy compatible with our CUDA version.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install cupy-cuda12x
</span></span></code></pre></div><p>And we can use it to run the transforms.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transforms_cupy</span>(image: PIL<span style=color:#f92672>.</span>Image<span style=color:#f92672>.</span>Image):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert image to RGB and resize</span>
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#34;RGB&#34;</span>)
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>resize((<span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>), Image<span style=color:#f92672>.</span>BICUBIC)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert to CuPy array and normalize</span>
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>array(image, dtype<span style=color:#f92672>=</span>cp<span style=color:#f92672>.</span>float32) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> img_cupy<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply mean and std normalization</span>
</span></span><span style=display:flex><span>    mean <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>], dtype<span style=color:#f92672>=</span>cp<span style=color:#f92672>.</span>float32)<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    std <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>0.225</span>], dtype<span style=color:#f92672>=</span>cp<span style=color:#f92672>.</span>float32)<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> (img_cupy <span style=color:#f92672>-</span> mean) <span style=color:#f92672>/</span> std
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Add batch dimension</span>
</span></span><span style=display:flex><span>    img_cupy <span style=color:#f92672>=</span> cp<span style=color:#f92672>.</span>expand_dims(img_cupy, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> img_cupy
</span></span></code></pre></div><p>With CuPy, we got a tiny bit of performance improvement:</p><pre tabindex=0><code>&gt;&gt;&gt; Onnxruntime CUDA cupy transforms: 54.267 ms per image, FPS: 18.43
</code></pre><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the ONNX Runtime CUDA cupy inference by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 04_onnx_cuda_inference.py
</span></span></code></pre></div></div><p>Using ONNX Runtime with CUDA is a little better than the PyTorch model on the GPU, but still not fast enough for real-time inference.</p><p>We have one more trick up our sleeve.</p><h3 id=-onnx-runtime-on-tensorrt>📊 ONNX Runtime on TensorRT</h3><p>Similar to the CUDA provider, we have the TensorRT provider on ONNX Runtime. This lets us run the model using the TensorRT high performance inference engine by NVIDIA.</p><p>From the <a href=https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements target=_blank rel="nofollow noopener noreferrer">compatibility matrix</a>, we can see that <code>onnxruntime-gpu==1.19.2</code> is compatible with TensorRT 10.1.0.</p><p>To use the TensorRT provider, you need to have TensorRT installed on your system.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install tensorrt<span style=color:#f92672>==</span>10.1.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            tensorrt-cu12<span style=color:#f92672>==</span>10.1.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            tensorrt-cu12-bindings<span style=color:#f92672>==</span>10.1.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            tensorrt-cu12-libs<span style=color:#f92672>==</span>10.1.0
</span></span></code></pre></div><p>Next you need to export library path to include the TensorRT library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export LD_LIBRARY_PATH<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib:</span>$LD_LIBRARY_PATH<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><p>Replace the <code>/home/dnth/mambaforge-pypy3/envs/supercharge_timm_tensorrt/lib</code> with the path to your TensorRT library.</p><p>Otherwise you&rsquo;ll encounter the following error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Failed to load library libonnxruntime_providers_tensorrt.so 
</span></span><span style=display:flex><span>with error: libnvinfer.so.10: cannot open shared object file: 
</span></span><span style=display:flex><span>No such file or directory
</span></span></code></pre></div><p>Next we need so set the TensorRT provider options in ONNX Runtime inference code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>providers <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    (
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;TensorrtExecutionProvider&#34;</span>,
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;device_id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_max_workspace_size&#34;</span>: <span style=color:#ae81ff>8589934592</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_fp16_enable&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_engine_cache_enable&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_engine_cache_path&#34;</span>: <span style=color:#e6db74>&#34;./trt_cache&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_force_sequential_engine_build&#34;</span>: <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_max_partition_iterations&#34;</span>: <span style=color:#ae81ff>10000</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_min_subgraph_size&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_builder_optimization_level&#34;</span>: <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trt_timing_cache_enable&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>onnx_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> ort<span style=color:#f92672>.</span>InferenceSession(onnx_filename, providers<span style=color:#f92672>=</span>providers)
</span></span></code></pre></div><p>The rest of the code is the same as the CUDA inference.</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Here are the descriptions for the arguments you can pass to the <code>TensorrtExecutionProvider</code>:</p><ul><li><code>device_id</code>: 0 - This specifies the GPU device ID to use. In this case, it&rsquo;s set to 0, which typically refers to the first GPU in the system.</li><li><code>trt_max_workspace_size</code>: 8589934592 - This sets the maximum workspace size for TensorRT in bytes. Here, it&rsquo;s set to 8GB, which allows TensorRT to use up to 8GB of GPU memory for its operations.</li><li><code>trt_fp16_enable</code>: True - This enables FP16 (half-precision) mode, which can significantly speed up inference on supported GPUs while reducing memory usage.</li><li><code>trt_engine_cache_enable</code>: True - This enables caching of TensorRT engines, which can speed up subsequent runs by avoiding engine rebuilding.</li><li><code>trt_engine_cache_path</code>: <code>./trt_cache</code> - This specifies the directory where TensorRT engine cache files will be stored.</li><li><code>trt_force_sequential_engine_build</code>: False - When set to False, it allows parallel building of TensorRT engines for different subgraphs.</li><li><code>trt_max_partition_iterations</code>: 10000 - This sets the maximum number of iterations for TensorRT to attempt partitioning the graph.</li><li><code>trt_min_subgraph_size</code>: 1 - This specifies the minimum number of nodes required for a subgraph to be considered for conversion to TensorRT.</li><li><code>trt_builder_optimization_level</code>: 5 - This sets the optimization level for the TensorRT builder. Level 5 is the highest optimization level, which can result in longer build times but potentially better performance.</li><li><code>trt_timing_cache_enable</code>: True - This enables the timing cache, which can help speed up engine building by reusing layer timing information from previous builds.</li></ul></div><p>And now let&rsquo;s run the benchmark:</p><pre tabindex=0><code>&gt;&gt;&gt; TensorRT + numpy: 18.852 ms per image, FPS: 53.04
&gt;&gt;&gt; TensorRT + cupy: 16.892 ms per image, FPS: 59.20
</code></pre><p>Running with TensorRT and cupy give us a 4.5x speedup over the PyTorch model on the GPU and 93x speedup over the PyTorch model on the CPU!</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the ONNX Runtime TensorRT inference by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 05_onnx_trt_inference.py
</span></span></code></pre></div></div><p>That&rsquo;s the end of this post. Or is it?</p><p>You could stop here and be happy with the results. After all we already got a 93x speedup over the PyTorch model.</p><p>But.. if you&rsquo;re like me and you want to squeeze out every last bit of performance, there&rsquo;s one final trick up our sleeve.</p><h3 id=-bake-pre-processing-into-onnx>🎂 Bake pre-processing into ONNX</h3><p>If you recall, we did our pre-processing transforms outside of the ONNX model in CuPy or NumPy.</p><p>This incurs some overhead because we need to transfer the data to and from the GPU for the transforms.</p><p>We can avoid this overhead by baking the transforms operations into the ONNX model. This lets us run the inference faster because we don&rsquo;t need to do the transforms separately.</p><p>To do this we need to write some custom code to convert the transforms to an ONNX model.
If you recall, the numpy transforms we used earlier uses the resize and normalization operations. These operations are supported in ONNX and we can add them to the model.</p><p>To do this we need to write the preprocessing code as a PyTorch model and export it to ONNX.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Preprocess</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, input_shape: List[int]):
</span></span><span style=display:flex><span>        super(Preprocess, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>input_shape <span style=color:#f92672>=</span> tuple(input_shape)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mean <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>])<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>std <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>0.225</span>])<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x: torch<span style=color:#f92672>.</span>Tensor):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Resize the image to the input shape</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>interpolate(input<span style=color:#f92672>=</span>x, 
</span></span><span style=display:flex><span>                                           size<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>input_shape[<span style=color:#ae81ff>2</span>:], 
</span></span><span style=display:flex><span>                                           mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bicubic&#39;</span>, 
</span></span><span style=display:flex><span>                                           align_corners<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Normalize the image</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> (x <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>mean) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>std
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>And now export the <code>Preprocess</code> module to ONNX.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>input_shape <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>448</span>, <span style=color:#ae81ff>448</span>]
</span></span><span style=display:flex><span>output_onnx_file <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;preprocessing.onnx&#34;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Preprocess(input_shape<span style=color:#f92672>=</span>input_shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>onnx<span style=color:#f92672>.</span>export(
</span></span><span style=display:flex><span>        model,
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>randn(input_shape),
</span></span><span style=display:flex><span>        output_onnx_file,
</span></span><span style=display:flex><span>        opset_version<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>,
</span></span><span style=display:flex><span>        input_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;input_rgb&#34;</span>],
</span></span><span style=display:flex><span>        output_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;output_prep&#34;</span>],
</span></span><span style=display:flex><span>        dynamic_axes<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;input_rgb&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;batch_size&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>2</span>: <span style=color:#e6db74>&#34;height&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>3</span>: <span style=color:#e6db74>&#34;width&#34;</span>,
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Let&rsquo;s visualize the exported <code>preprocessing.onnx</code> model on Netron.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/preprocess_model.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/preprocess_model.png srcset="/portfolio/supercharge_your_pytorch_image_models/preprocess_model_hu4a4a3b5e6cba6ebdb9f1bac4ae7da7fa_64640_360x0_resize_box_3.png 360w, /portfolio/supercharge_your_pytorch_image_models/preprocess_model_hu4a4a3b5e6cba6ebdb9f1bac4ae7da7fa_64640_720x0_resize_box_3.png 720w, /portfolio/supercharge_your_pytorch_image_models/preprocess_model_hu4a4a3b5e6cba6ebdb9f1bac4ae7da7fa_64640_1920x0_resize_box_3.png 1920w" sizes="(max-width: auto) 100vw, auto" width=auto style=max-width:100%;height:auto></a></figure><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the export of the preprocessing model by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 06_export_preprocessing_onnx.py
</span></span></code></pre></div><p>Note the name of the output node of the <code>preprocessing.onnx</code> model - <code>output_preprocessing</code>.</p></div><p>Now we need to merge the output of the <code>preprocessing.onnx</code> model with the input of the <code>eva02_large_patch14_448</code> model.</p><p>Let&rsquo;s visualize the original <code>eva02_large_patch14_448</code> model on Netron.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/original_model.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/original_model.png srcset="/portfolio/supercharge_your_pytorch_image_models/original_model_hu48e70b40c812c0099676aaf83783985b_50258_360x0_resize_box_3.png 360w, /portfolio/supercharge_your_pytorch_image_models/original_model_hu48e70b40c812c0099676aaf83783985b_50258_720x0_resize_box_3.png 720w, /portfolio/supercharge_your_pytorch_image_models/original_model_hu48e70b40c812c0099676aaf83783985b_50258_1920x0_resize_box_3.png 1920w" sizes="(max-width: auto) 100vw, auto" width=auto style=max-width:100%;height:auto></a></figure><p>Note the name of the input node of the <code>eva02_large_patch14_448</code> model. We will need this for the merge.
The name of the input node is <code>input</code>.</p><p>To merge the models, we use the <code>compose</code> function from the <code>onnx</code> library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> onnx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the models</span>
</span></span><span style=display:flex><span>model1 <span style=color:#f92672>=</span> onnx<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;preprocessing.onnx&#34;</span>)
</span></span><span style=display:flex><span>model2 <span style=color:#f92672>=</span> onnx<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;eva02_large_patch14_448.onnx&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Merge the models</span>
</span></span><span style=display:flex><span>merged_model <span style=color:#f92672>=</span> onnx<span style=color:#f92672>.</span>compose<span style=color:#f92672>.</span>merge_models(
</span></span><span style=display:flex><span>    model1,
</span></span><span style=display:flex><span>    model2,
</span></span><span style=display:flex><span>    io_map<span style=color:#f92672>=</span>[(<span style=color:#e6db74>&#34;output_preprocessing&#34;</span>, <span style=color:#e6db74>&#34;input&#34;</span>)],
</span></span><span style=display:flex><span>    prefix1<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;preprocessing_&#34;</span>,
</span></span><span style=display:flex><span>    prefix2<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model_&#34;</span>,
</span></span><span style=display:flex><span>    doc_string<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Merged preprocessing and eva02_large_patch14_448 model&#34;</span>,
</span></span><span style=display:flex><span>    producer_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;dickson.neoh@gmail.com using onnx compose&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the merged model</span>
</span></span><span style=display:flex><span>onnx<span style=color:#f92672>.</span>save(merged_model, <span style=color:#e6db74>&#34;merged_model_compose.onnx&#34;</span>)
</span></span></code></pre></div><p>Note the <code>io_map</code> parameter. This lets us map the output of the preprocessing model to the input of the original model. You must ensure that the input and output names of the models are correct.</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the merge of the models by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 07_onnx_compose_merge.py
</span></span></code></pre></div></div><p>If there are no errors, you will end up with a file called <code>merged_model_compose.onnx</code> in your working directory.
Let&rsquo;s visualize the merged model on Netron.</p><figure style=text-align:center><a href=/portfolio/supercharge_your_pytorch_image_models/merged_model.png class=image-popup><img src=/portfolio/supercharge_your_pytorch_image_models/merged_model.png srcset="/portfolio/supercharge_your_pytorch_image_models/merged_model_hu7a7141bb47d6eea0aa1a1400614ac396_73885_360x0_resize_box_3.png 360w, /portfolio/supercharge_your_pytorch_image_models/merged_model_hu7a7141bb47d6eea0aa1a1400614ac396_73885_720x0_resize_box_3.png 720w, /portfolio/supercharge_your_pytorch_image_models/merged_model_hu7a7141bb47d6eea0aa1a1400614ac396_73885_1920x0_resize_box_3.png 1920w" sizes="(max-width: auto) 100vw, auto" width=auto style=max-width:100%;height:auto></a></figure><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Note the input to the merged model is <code>[batch_size, 3, height, width]</code>. This model can be given any input of size height x width and the batch size can vary. As we&rsquo;ve seen in the Preprocess module earlier, the height and width are resized to 448x448 internally.</p></div><p>Now using this merged model, let&rsquo;s run the inference benchmark again using the TensorRT provider.</p><p>We&rsquo;ll need to make a small change to how the input tensor is passed to the model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_image</span>(image: Image<span style=color:#f92672>.</span>Image):
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#34;RGB&#34;</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(image)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> img_numpy<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    img_numpy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(img_numpy, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> img_numpy
</span></span></code></pre></div><p>Notice we are no longer doing the resize and normalization inside the function. This is because the merged model already includes these operations.</p><p>And the results are in!</p><pre tabindex=0><code>TensorRT with pre-processing: 12.875 ms per image, FPS: 77.67
</code></pre><p>That&rsquo;s a 6x improvement over the original PyTorch model on the GPU and a whopping 123x improvement over the PyTorch model on the CPU! 🚀</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>If you&rsquo;ve cloned the <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>, you can run the merged model inference by executing the following command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python 08_inference_merged_model.py
</span></span></code></pre></div></div><p>Let&rsquo;s do a final sanity check on the predictions.</p><pre tabindex=0><code>&gt;&gt;&gt; espresso: 34.25%
&gt;&gt;&gt; cup: 2.06%
&gt;&gt;&gt; chocolate sauce, chocolate syrup: 1.31%
&gt;&gt;&gt; bakery, bakeshop, bakehouse: 0.97%
&gt;&gt;&gt; coffee mug: 0.85%
</code></pre><p>Looks like the predictions are close to the original model. We can sign off and say that the model is working as expected.</p><h3 id=-conclusion>🚧 Conclusion</h3><p>In this post we have seen how we can supercharge our TIMM models for faster inference using ONNX Runtime and TensorRT.</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>In this post you&rsquo;ve learned how to:</p><ul><li>📥 Load any pre-trained model from <a href=https://huggingface.co/docs/timm/index target=_blank rel="nofollow noopener noreferrer">TIMM</a></li><li>🔄 Convert the model to ONNX format</li><li>🖥️ Run inference with ONNX Runtime (CPU & GPU)</li><li>🎮 Run inference with TensorRT (GPU)</li><li>🛠️ Tweak the TensorRT parameters for better performance</li><li>🧠 Bake the pre-processing into the ONNX model</li></ul><p>You can find the code for this post on my GitHub repository <a href=https://github.com/dnth/supercharge-your-pytorch-image-models-blogpost target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><p>I hope this has been helpful.</p><p>Thank you for reading!</p><section class=social-share><ul class=share-icons><hr><h5>🤟 Follow me</h5><p>Don't want to miss any of my future content? Follow me on Twitter and LinkedIn where I share these tips in
bite-size posts.</p><li><a href=https://twitter.com/dicksonneoh7 target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn x"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path fill="#fff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>&nbsp;
Twitter</a></li>&nbsp;<li><a href=https://www.linkedin.com/in/dickson-neoh/ target=_blank rel=noopener aria-label="Follow on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg>&nbsp;
LinkedIn</a></li>&nbsp;<li><a href=https://github.com/dnth/ target=_blank rel=noopener aria-label="Follow on GitHub" class="share-btn github"><svg width="24" height="24" viewBox="0 0 256 250" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid"><g><path d="M128.00106.0C57.3172926.0.0 57.3066942.0 128.00106c0 56.554221 36.6761997 104.534482 87.534937 121.459839 6.3970853 1.18487999999999 8.745651-2.776734 8.745651-6.157566C96.280588 240.251045 96.1618878 230.167899 96.106777 219.472176 60.4967585 227.215235 52.9826207 204.369712 52.9826207 204.369712c-5.8226623-14.795114-14.2122127-18.729174-14.2122127-18.729174C27.1568785 177.696113 39.6458206 177.859325 39.6458206 177.859325 52.4993419 178.762293 59.267365 191.04987 59.267365 191.04987c11.4164025 19.568553 29.9442103 13.911223 37.2485035 10.640612C97.6647155 193.417512 100.981959 187.77078 104.642583 184.574357 76.211799 181.33766 46.324819 170.362144 46.324819 121.315702c0-13.974813 5.0002398-25.3933338 13.1884247-34.3573083-1.3290169-3.2239785-5.7103208-16.2428317 1.2399917-33.8740301.0.0 10.7487147-3.4401823 35.2094058 13.1205959 10.2103258-2.8360835 21.1604058-4.2583646 32.0384188-4.3071163C138.879073 61.9465949 149.837632 63.368876 160.067033 66.2049595c24.431017-16.5607782 35.164893-13.1205959 35.164893-13.1205959C202.199197 70.715562 197.815773 83.7344152 196.486756 86.9583937 204.694018 95.9223682 209.660343 107.340889 209.660343 121.315702c0 49.163023-29.94421 59.988045-58.447062 63.156912 4.591149 3.97221400000001 8.682061 11.761904 8.682061 23.703979.0 17.126724-.14837399999999 30.910768-.14837399999999 35.12674C159.746968 246.709601 162.05102 250.70089 168.53925 249.443941 219.370432 232.499507 256 184.536204 256 128.00106 256 57.3066942 198.691187.0 128.00106.0zM47.9405593 182.340212C47.6586465 182.976105 46.6581745 183.166873 45.7467277 182.730227 44.8183235 182.312656 44.2968914 181.445722 44.5978808 180.80771 44.8734344 180.152739 45.876026 179.97045 46.8023103 180.409216 47.7328342 180.826786 48.2627451 181.702199 47.9405593 182.340212zm6.2962299 5.618042C53.6263318 188.524199 52.4329723 188.261363 51.6232682 187.366874 50.7860088 186.474504 50.6291553 185.281144 51.2480912 184.70672 51.8776254 184.140775 53.0349512 184.405731 53.8743302 185.298101 54.7115892 186.201069 54.8748019 187.38595 54.2367892 187.958254zM58.5562413 195.146347C57.7719732 195.691096 56.4895886 195.180261 55.6968417 194.042013 54.9125733 192.903764 54.9125733 191.538713 55.713799 190.991845 56.5086651 190.444977 57.7719732 190.936735 58.5753181 192.066505 59.3574669 193.22383 59.3574669 194.58888 58.5562413 195.146347zM65.8613592 203.471174C65.1597571 204.244846 63.6654083 204.03712 62.5716717 202.981538 61.4524999 201.94927 61.1409122 200.484596 61.8446341 199.710926 62.5547146 198.935137 64.0575422 199.15346 65.1597571 200.200564 66.2704506 201.230712 66.6095936 202.705984 65.8613592 203.471174zM75.3025151 206.281542C74.9930474 207.284134 73.553809 207.739857 72.1039724 207.313809 70.6562556 206.875043 69.7087748 205.700761 70.0012857 204.687571 70.302275 203.678621 71.7478721 203.20382 73.2083069 203.659543 74.6539041 204.09619 75.6035048 205.261994 75.3025151 206.281542zM86.046947 207.473627C86.0829806 208.529209 84.8535871 209.404622 83.3316829 209.4237 81.8013 209.457614 80.563428 208.603398 80.5464708 207.564772c0-1.066181 1.20183800000001-1.93311500000002 2.7322209-1.958551C84.8005962 205.576546 86.046947 206.424403 86.046947 207.473627zM96.6021471 207.069023C96.7844366 208.099171 95.7267341 209.156872 94.215428 209.438785 92.7295577 209.710099 91.3539086 209.074206 91.1652603 208.052538 90.9808515 206.996955 92.0576306 205.939253 93.5413813 205.66582 95.054807 205.402984 96.4092596 206.021919 96.6021471 207.069023z" fill="#fff"/></g></svg>&nbsp;
GitHub</a></li>&nbsp;<hr><h5>🔄 Share this post</h5><li><a href="https://twitter.com/intent/tweet?&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f&text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations @dicksonneoh7" target=_blank rel=noopener aria-label="Share on Twitter" class="share-btn x"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path fill="#fff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>&nbsp;
Twitter</a></li>&nbsp;<li><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f&source=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f&title=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations&summary=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations" target=_blank rel=noopener aria-label="Share on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg>&nbsp;
LinkedIn</a></li>&nbsp;<li><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f" target=_blank rel=noopener aria-label="Share on Facebook" class="share-btn facebook"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="facebook_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-7.8050197" inkscape:cy="32.710925" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.36281,-104.14567)"><path d="m130.36281 104.72294v5.19545c0 .3157.26158.57728.57727.57728h5.19546c.3157.0.57727-.26158.57727-.57728v-5.19545c0-.3157-.26157-.57727-.57727-.57727h-5.19546c-.31569.0-.57727.26157-.57727.57727zm5.77273.0v5.19545h-1.4973v-1.94829h.74865l.10824-.86591h-.85689v-.55923c0-.25256.0631-.42394.42393-.42394h.46904v-.78473c-.0794-.0108-.35719-.0271-.67649-.0271-.66567.0-1.11847.40048-1.11847 1.14553v.64943h-.75767v.86591h.75767v1.94829h-2.79617v-5.19545z" id="path1085" style="stroke-width:.0180398;fill:#fff"/></g></svg>&nbsp;
Facebook</a></li>&nbsp;<br><li><a href="https://telegram.me/share/url?text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f" target=_blank rel=noopener aria-label="Share on Telegram" class="share-btn telegram"><svg width="7.3503098mm" height="7.1592798mm" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473"><g><path d="M152.531 179.476c-1.48.0-2.95-.438-4.211-1.293l-47.641-32.316-25.552 18.386c-2.004 1.441-4.587 1.804-6.914.972-2.324-.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821 93.928c-2.886-1.104-4.8-3.865-4.821-6.955-.021-3.09 1.855-5.877 4.727-7.02l174.312-69.36c.791-.336 1.628-.53 2.472-.582.302-.018.605-.018.906-.001 1.748.104 3.465.816 4.805 2.13.139.136.271.275.396.42 1.11 1.268 1.72 2.814 1.835 4.389.028.396.026.797-.009 1.198-.024.286-.065.571-.123.854L159.898 173.38c-.473 2.48-2.161 4.556-4.493 5.523C154.48 179.287 153.503 179.476 152.531 179.476zm-47.669-48.897 42.437 28.785L170.193 39.24l-82.687 79.566 17.156 11.638C104.731 130.487 104.797 130.533 104.862 130.579zM69.535 124.178l5.682 21.53 12.242-8.809-16.03-10.874C70.684 125.521 70.046 124.893 69.535 124.178zM28.136 86.782l31.478 12.035c2.255.862 3.957 2.758 4.573 5.092l3.992 15.129c.183-1.745.974-3.387 2.259-4.624L149.227 38.6 28.136 86.782z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>&nbsp;
Telegram</a></li>&nbsp;<li><a href="whatsapp://send?text=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20how%20to%20optimize%20PyTorch%20image%20models%20using%20ONNX%20Runtime%20and%20TensorRT%2c%20achieving%20up%20to%208x%20faster%20inference%20speeds%20for%20real-time%20applications.%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f%0a" target=_blank aria-label="Share on WhatsApp" class="share-btn whatsapp"><svg width="6.0324998mm" height="6.05896mm" viewBox="0 0 6.0324997 6.05896" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="whatsapp_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="4.9987205" inkscape:cy="35.692618" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-126.67735,-103.17712)"><path d="m131.83672 104.07671c-.58208-.58209-1.34937-.89959-2.14312-.89959-1.64042.0-2.98979 1.34938-2.98979 3.01625.0.52917.13229 1.03188.39687 1.48167l-.42333 1.56104 1.5875-.42333c.42333.23812.92604.34396 1.42875.34396 1.66687.0 3.01625-1.34938 3.01625-3.01625-.0265-.74084-.3175-1.50813-.87313-2.06375zm-2.14312 4.6302c-.44979.0-.87313-.13229-1.27-.34395l-.10583-.0529-.92605.26458.26459-.89958-.0794-.13229c-.26458-.39688-.37041-.84667-.37041-1.34938.0-1.37583 1.11125-2.48708 2.48708-2.48708.66146.0 1.29646.26458 1.77271.74083.47625.47625.74083 1.11125.74083 1.77271-.0265 1.37583-1.13771 2.48708-2.51354 2.48708zm1.34937-1.87854c-.0794-.0265-.44979-.23812-.5027-.26458-.0794-.0265-.1323-.0265-.18521.0265-.0529.0794-.21167.26458-.23813.29104-.0529.0529-.0794.0529-.15875.0265-.0794-.0265-.3175-.13229-.60854-.37042-.23812-.21167-.37042-.44979-.42333-.52917-.0529-.0794.0-.13229.0265-.15875.0265-.0264.0794-.0794.10583-.13229.0529-.0265.0794-.0794.10583-.13229.0265-.0529.0-.10583.0-.13229.0-.0265-.18521-.39688-.26458-.55563-.0265-.10583-.10583-.0794-.13229-.0794h-.15875s-.10584.0265-.18521.0794c-.0794.0794-.26458.26459-.26458.635.0.37042.26458.74084.29104.79375.0265.0529.52916.82021 1.29646 1.13771.1852.0794.3175.13229.42333.15875.18521.0529.34396.0529.47625.0265.15875-.0265.44979-.18521.50271-.34396.0529-.18521.0529-.3175.0529-.34396-.0264-.0794-.0794-.10583-.15875-.13229z" id="path1793" style="stroke-width:.264583;fill:#fff"/></g></svg>&nbsp;
WhatsApp</a></li>&nbsp;<li><a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations.&body=Supercharge%20Your%20PyTorch%20Image%20Models%3a%20Bag%20of%20Tricks%20to%208x%20Faster%20Inference%20with%20ONNX%20Runtime%20%26%20Optimizations%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20how%20to%20optimize%20PyTorch%20image%20models%20using%20ONNX%20Runtime%20and%20TensorRT%2c%20achieving%20up%20to%208x%20faster%20inference%20speeds%20for%20real-time%20applications.%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2fsupercharge_your_pytorch_image_models%2f%0a" target=_blank class="share-btn email" aria-label="Share via Email"><svg width="6.3499999mm" height="4.3961601mm" viewBox="0 0 6.3499999 4.3961601" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="email_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.7526575" inkscape:cy="33.4125" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.10375,-103.97942)"><path d="m130.10375 104.22365v3.9077.24423h.24423 5.86154.24423v-.24423-3.9077-.24423h-.24423-5.86154-.24423zm5.29675.24423-2.12175 1.41196-2.12175-1.41196zm-2.25913 1.91569.13738.0839.13738-.0839 2.54916-1.70198v3.20553h-5.37308v-3.20553z" id="path824" style="stroke-width:.0152644;fill:#fff"/></g></svg>&nbsp;
Email</a></li><hr><section><h5>❤️ Show some love</h5><p>Creating free ML contents doesn't pay my bills. Support me in creating more free contents like these.
Consider buying me a coffee. Your support means a lot to me.</p><div style=text-align:center><a href=https://www.buymeacoffee.com/dicksonneoh target=_blank><img src=https://cdn.buymeacoffee.com/buttons/v2/default-blue.png alt="Buy Me A Coffee" style=height:60px!important;width:217px!important></a></div></section><hr></ul></section></div></div></div><div class=row><div class="col-lg-10 offset-lg-1"><nav class="case-details-nav d-flex justify-content-between align-items-start"><div class=previous><div class="d-flex align-items-center mb-3"><div class="icon mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1243" fill="#2d2d2d"><path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z"/><path data-name="Path 1455" d="M13.137 1.41 3.39 15.558l-.975-1.415L12.166.0z"/></g></svg></div><span class=small>Prev blog</span></div><div class=blog-nav-item><div class=blog-nav-thumb><a href=https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/><img src=https://dicksonneoh.com/images/portfolio/bringing_high_quality_image_models_to_mobile/thumbnail.gif alt=post-image></a></div><h5 class=title><a class=text-dark href=https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android & iOS</a></h5></div></div></nav></div></div></div></section></div><section class=footer id=contact><div class=footer__background_shape><svg viewBox="0 0 1920 79"><path d="M0 0h1920v79L0 0z" data-name="Path 1450"/></svg></div><div class=container><div class=row><div class=col-lg-12><div class=footer__cta><div class=shape-1><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class=shape-2><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class="text-light footer__cta_content"><span>Contact me</span><h2 class="mb-0 mb-3">Let’s Start a Project</h2></div><div class=footer__cta_action></div><a href="https://api.whatsapp.com/send?phone=60133250827" rel=noopener target=_blank class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-whatsapp"></i>&nbsp;&nbsp;Chat on WhatsApp</a>
<a href=https://t.me/dicksonneoh rel=noopener target=_blank class="btn btn-light btn-zoom mr-3 mb-3"><i class="fa fa-telegram"></i>&nbsp;&nbsp;Chat on Telegram</a></div></div></div><div class="row footer__widget"><div class=col-lg-4><div class="footer__widget_logo mb-5"><img src=https://dicksonneoh.com/images/site-navigation/logo_resized.png alt=widget-logo></div></div><div class=col-lg-4><div class="text-light footer__widget_sitemap mb-5"><h4 class=base-font>Sitemap</h4><ul class="unstyle-list small"><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>About me</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Frequently Ask Question</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Privacy & Policy</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Latest Article</a></li></ul></div></div><div class=col-lg-4><div class="text-light footer__widget_address mb-5"><h4 class=base-font>Address</h4><ul class="fa-ul small"><li class=mb-2><a class=text-light href=tel:+%2860%29%203%208921%202020><span class=fa-li><i class="fa fa-phone"></i></span>+(60) 3 8921 2020</a></li><li class=mb-2><a class=text-light href=mailto:dickson.neoh@gmail.com><span class=fa-li><i class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li><li class=mb-2><span class=fa-li><i class="fa fa-map-marker"></i></span>Kuala Lumpur, Malaysia.</a></li></ul></div></div></div><div class="row footer__footer"><div class=col-lg-6><div class="footer__footer_copy text-light"><p>All right reserved copyright © Dickson Neoh 2024</p></div></div><div class=col-lg-6><div class=footer__footer_social><ul class=unstyle-list><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://www.linkedin.com/in/dickson-neoh/><i class="fa fa-linkedin-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://twitter.com/dicksonneoh7><i class="fa fa-twitter-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://github.com/dnth><i class="fa fa-github-square"></i></a></li></ul></div></div></div></div></section><script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script>
<script src=https://dicksonneoh.com/plugins/jQuery/jquery.min.js></script>
<script src=https://dicksonneoh.com/plugins/bootstrap/bootstrap.min.js></script>
<script src=https://dicksonneoh.com/plugins/slick/slick.min.js></script>
<script src=https://dicksonneoh.com/plugins/waypoint/jquery.waypoints.min.js></script>
<script src=https://dicksonneoh.com/plugins/magnafic-popup/jquery.magnific-popup.min.js></script>
<script src=https://dicksonneoh.com/plugins/tweenmax/TweenMax.min.js></script>
<script src=https://dicksonneoh.com/plugins/imagesloaded/imagesloaded.min.js></script>
<script src=https://dicksonneoh.com/plugins/masonry/masonry.min.js></script>
<script src=https://dicksonneoh.com/js/form-handler.min.js></script>
<script src=https://dicksonneoh.com/js/script.min.js></script></body></html>