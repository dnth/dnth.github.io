<!doctype html><html><head><meta charset=utf-8><title>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</title><meta name=description content="Learn and deploy over 900+ cutting edge PyTorch classification models on Android. "><meta property="og:title" content="PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter"><meta property="og:description" content="Learn and deploy over 900+ cutting edge PyTorch classification models on Android. "><meta property="og:type" content="article"><meta property="og:url" content="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/"><meta property="og:image" content="https://dicksonneoh.com/images/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/post_image.gif"><meta property="article:section" content="portfolio"><meta property="article:published_time" content="2023-02-07T11:00:15+08:00"><meta property="article:modified_time" content="2023-02-07T11:00:15+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dicksonneoh.com/images/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/post_image.gif"><meta name=twitter:title content="PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter"><meta name=twitter:description content="Learn and deploy over 900+ cutting edge PyTorch classification models on Android. "><meta name=viewport content="width=device-width,initial-scale=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick-theme.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/magnafic-popup/magnific-popup.css><link href=https://dicksonneoh.com/scss/style.min.css rel=stylesheet><link rel="shortcut icon" href=https://dicksonneoh.com/images/favicon-purple.ico type=image/x-icon><link rel=icon href=https://dicksonneoh.com/images/favicon.gif type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-54500366-2")</script></head><body><nav class="navbar navbar-expand-lg fixed-top"><div class=container><a href=https://dicksonneoh.com/ class=navbar-brand><img src=https://dicksonneoh.com/images/site-navigation/logo_resized.png alt=site-logo></a>
<button type=button class="navbar-toggler collapsed" data-toggle=collapse data-target=#navbarCollapse>
<span class=navbar-toggler-icon></span>
<span class=navbar-toggler-icon></span>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-between" id=navbarCollapse><ul class="nav navbar-nav main-navigation my-0 mx-auto"><li class=nav-item><a href=https://dicksonneoh.com/#home class="nav-link text-dark text-sm-center p-2">Home</a></li><li class=nav-item><a href=https://dicksonneoh.com/#about class="nav-link text-dark text-sm-center p-2">About</a></li><li class=nav-item><a href=https://dicksonneoh.com/#service class="nav-link text-dark text-sm-center p-2">Services</a></li><li class=nav-item><a href=https://dicksonneoh.com/#portfolio class="nav-link text-dark text-sm-center p-2">Projects</a></li><li class=nav-item><a href=https://dicksonneoh.com/#resume class="nav-link text-dark text-sm-center p-2">Resume</a></li><li class=nav-item><a href=https://dicksonneoh.com/#skills class="nav-link text-dark text-sm-center p-2">Skills</a></li><li class=nav-item><a href=https://dicksonneoh.com/#blog class="nav-link text-dark text-sm-center p-2">Blogs</a></li><li class=nav-item><a href=https://dicksonneoh.com/#contact class="nav-link text-dark text-sm-center p-2">Contact</a></li></ul><div class=navbar-nav><a href=https://dicksonneoh.com/contact class="btn btn-primary btn-zoom hire_button">Hire Me Now</a></div></div></div></nav><div id=content><header class=breadCrumb><div class=container><div class=row><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><h3 class=breadCrumb__title>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</h3><nav aria-label=breadcrumb class="d-flex justify-content-center"></nav></div></div><div class="row p-3"><div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center"><i class="fa fa-calendar"></i> &ensp;
February 7, 2023 &ensp; &ensp;
<i class="fa fa-clock-o"></i> &ensp;
10 mins read</div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-tag"></i> &ensp;
<a href=https://dicksonneoh.com/tags/timm/>TIMM</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/tags/torchscript/>TorchScript</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/tags/paddy-disease/>paddy-disease</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/tags/fastai/>Fastai</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/tags/flutter/>Flutter</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/tags/android/>Android</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/tags/edgenext/>EdgeNeXt</a></div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-folder"></i> &ensp;
<a href=https://dicksonneoh.com/categories/deployment/>deployment</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/categories/image-classification/>image-classification</a>
<span class=separator>â€¢</span>
<a href=https://dicksonneoh.com/categories/edge-ai/>edge-ai</a></div></div></div></header><section class="section singleBlog"><div class=svg-img><img src=https://dicksonneoh.com/images/hero/figure-svg.svg alt></div><div class=animate-shape><img src=https://dicksonneoh.com/images/skill/skill-background-shape.svg alt><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600"><defs><linearGradient id="d" x1=".929" y1=".111" x2=".263" y2=".935" gradientUnits="objectBoundingBox"><stop offset="0" stop-color="#f1f6f9"/><stop offset="1" stop-color="#f1f6f9" stop-opacity="0"/></linearGradient></defs><g data-name="blob-shape (3)"><path class="blob" fill="url(#d)" d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z"/></g></svg></div><div class=animate-pattern><img src=https://dicksonneoh.com/images/service/background-pattern.svg alt=background-shape></div><div class=container><div class=row><div class=col-lg-12><div class=singleBlog__feature><img src=https://dicksonneoh.com/images/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/post_image.gif alt=feature-image></div></div></div><div class="row mt-5"><div class=col-lg-12><div class=singleBlog__content><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a class=toc-link href=#-motivation>ğŸ”¥ Motivation</a></li><li><a class=toc-link href=#-dataset>ğŸŒ¿ Dataset</a></li><li><a class=toc-link href=#-pytorch-image-models>ğŸ¥‡ PyTorch Image Models</a></li><li><a class=toc-link href=#-training-with-fastai>ğŸ‹ï¸â€â™€ï¸ Training with Fastai</a></li><li><a class=toc-link href=#-exporting-with-torchscript>ğŸ“€ Exporting with TorchScript</a></li><li><a class=toc-link href=#-inference-in-flutter>ğŸ“² Inference in Flutter</a></li><li><a class=toc-link href=#-comments--feedback>ğŸ™ Comments & Feedback</a></li></ul></nav><div class=floating-toc><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a class=toc-link href=#-motivation>ğŸ”¥ Motivation</a></li><li><a class=toc-link href=#-dataset>ğŸŒ¿ Dataset</a></li><li><a class=toc-link href=#-pytorch-image-models>ğŸ¥‡ PyTorch Image Models</a></li><li><a class=toc-link href=#-training-with-fastai>ğŸ‹ï¸â€â™€ï¸ Training with Fastai</a></li><li><a class=toc-link href=#-exporting-with-torchscript>ğŸ“€ Exporting with TorchScript</a></li><li><a class=toc-link href=#-inference-in-flutter>ğŸ“² Inference in Flutter</a></li><li><a class=toc-link href=#-comments--feedback>ğŸ™ Comments & Feedback</a></li></ul></nav></div><hr><h3 id=-motivation>ğŸ”¥ Motivation</h3><p>With various high-level libraries like <a href=https://keras.io/ target=_blank rel="nofollow noopener noreferrer">Keras</a>, <a href=https://huggingface.co/docs/transformers/index target=_blank rel="nofollow noopener noreferrer">Transformer</a>, and <a href=https://www.fast.ai/ target=_blank rel="nofollow noopener noreferrer">Fastai</a>, the barrier to training SOTA models has never been lower.</p><p>On top of that with platforms like <a href=https://colab.research.google.com/ target=_blank rel="nofollow noopener noreferrer">Google Colab</a> and <a href=https://www.kaggle.com/ target=_blank rel="nofollow noopener noreferrer">Kaggle</a>, pretty much anyone can train a reasonably good model using an old laptop or even a mobile phone (with some patience).</p><blockquote class=blockquote><p class=mb-0>The question is no longer &ldquo;<strong>can we train a SOTA model?</strong>&rdquo;, but &ldquo;<strong>what happens after that?</strong>&rdquo;</p></blockquote><p>Unfortunately, after getting the model trained, most people wash their hands off at this point claiming their model works.
But, what good would SOTA models do if it&rsquo;s just in notebooks and Kaggle leaderboards?</p><p>Unless the model is deployed and put to use, it&rsquo;s of little benefit to anyone out there.</p><figure><a href=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme.jpg class=image-popup><img src=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme.jpg srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_360x0_resize_q75_box.jpg 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_720x0_resize_q75_box.jpg 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: ) 100vw, " style=max-width:100%;height:auto></a></figure><p>But deployment is painful. Running a model on a mobile phone?</p><p>Forget it ğŸ¤·â€â™‚ï¸.</p><p>The frustration is real. I remember spending nights exporting models into <code>ONNX</code> and it still failed me.
Deploying models on mobile for edge inference used to be complex.</p><p>Not anymore.</p><p>In this post, I&rsquo;m going to show you how you can pick from over 900+ SOTA models on <a href=https://github.com/rwightman/pytorch-image-models target=_blank rel="nofollow noopener noreferrer">TIMM</a>, train them using best practices with <a href=https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/ target=_blank rel="nofollow noopener noreferrer">Fastai</a>, and deploy them on Android using <a href=https://flutter.dev/ target=_blank rel="nofollow noopener noreferrer">Flutter</a>.</p><p>âœ… Yes, for free.</p><style type=text/css>.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:#6c757d;background:#e7f2fa}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:#fff;background:#6ab0de;text-transform:uppercase}.notice.warning .notice-title{background:rgba(217,83,79,.9)}.notice.warning{background:#fae2e2}.notice.info .notice-title{background:#f0b37e}.notice.info{background:#fff2db}.notice.note .notice-title{background:#6ab0de}.notice.note{background:#e7f2fa}.notice.tip .notice-title{background:rgba(92,184,92,.8)}.notice.tip{background:#e6f9e6}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>âš¡ By the end of this post you will learn how to:</p><ul><li>Load a SOTA classification model from TIMM and train it with Fastai.</li><li>Export the trained model with TorchScript for inference.</li><li>Create a functional Android app and run the inference on your device.</li></ul><p>ğŸ”¥ The inference time is at <strong>100ms</strong> and below on my Pixel 3 XL! The lowest I got was <strong>37ms</strong>!</p><p>ğŸ’¡ <strong>NOTE</strong>: Code and data for this post are available on my GitHub repo <a href=https://github.com/dnth/timm-flutter-pytorch-lite-blogpost target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><p>Here&rsquo;s a TLDR ğŸ‘‡</p><iframe src=https://www.youtube-nocookie.com/embed/tno2F3Hp5dA title="YouTube video player" onload='(function(e){e.style.height=e.contentWindow.document.body.scrollHeight+"px"})(this)' style=height:500px;width:100%;border:none;overflow:hidden frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><p>If that looks interesting, read on ğŸ‘‡</p><h3 id=-dataset>ğŸŒ¿ Dataset</h3><p>We will be working with the Paddy Disease Classification <a href=https://www.kaggle.com/competitions/paddy-disease-classification target=_blank rel="nofollow noopener noreferrer">dataset</a> from Kaggle.
The dataset consists of <code>10,407</code> labeled images across ten classes (9 diseases and 1 normal):</p><ol><li><code>bacterial_leaf_blight</code></li><li><code>bacterial_leaf_streak</code></li><li><code>bacterial_panicle_blight</code></li><li><code>blast</code></li><li><code>brown_spot</code></li><li><code>dead_heart</code></li><li><code>downy_mildew</code></li><li><code>hispa</code></li><li><code>tungro</code></li><li><code>normal</code></li></ol><p>The task is to classify the paddy images into <code>1</code> of the <code>9</code> diseases or <code>normal</code>.
Few sample images shown below.<figure><a href=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img.jpg class=image-popup><img src=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img.jpg srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_360x0_resize_q75_box.jpg 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_720x0_resize_q75_box.jpg 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: ) 100vw, " style=max-width:100%;height:auto></a></figure></p><p>Next, I download the data locally and organize them in a folder structure.
Here&rsquo;s the structure I have on my computer.</p><pre tabindex=0><code class=language-tree data-lang=tree>â”œâ”€â”€ data
â”‚   â”œâ”€â”€ test_images
â”‚   â””â”€â”€ train_images
â”‚       â”œâ”€â”€ bacterial_leaf_blight 
â”‚       â”œâ”€â”€ bacterial_leaf_streak 
â”‚       â”œâ”€â”€ bacterial_panicle_blight 
â”‚       â”œâ”€â”€ blast 
â”‚       â”œâ”€â”€ brown_spot 
â”‚       â”œâ”€â”€ dead_heart 
â”‚       â”œâ”€â”€ downy_mildew 
â”‚       â”œâ”€â”€ hispa 
â”‚       â”œâ”€â”€ models
â”‚       â”œâ”€â”€ normal 
â”‚       â””â”€â”€ tungro 
â””â”€â”€ train
    â””â”€â”€ train.ipynb
</code></pre><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Descriptions of the folders:</p><ul><li><code>data/</code> - A folder to store train and test images.</li><li><code>train/</code> - A folder to store training-related files and notebooks.</li></ul><p>View the full structure by browsing my GitHub <a href=https://github.com/dnth/timm-flutter-pytorch-lite-blogpost target=_blank rel="nofollow noopener noreferrer">repo</a>.</p></div><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>ğŸ”” If you&rsquo;d like to explore the dataset and excel in the competition, I&rsquo;d encourage you to check out a series of Kaggle notebooks by Jeremy Howard.</p><ul><li><a href=https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1 target=_blank rel="nofollow noopener noreferrer">First Steps.</a> - Setting up, looking at the data and training your first model.</li><li><a href=https://www.kaggle.com/code/jhoward/small-models-road-to-the-top-part-2 target=_blank rel="nofollow noopener noreferrer">Small Models.</a> - Iterate faster with small models, test time augmentation, and then scale up.</li><li><a href=https://www.kaggle.com/code/jhoward/scaling-up-road-to-the-top-part-3 target=_blank rel="nofollow noopener noreferrer">Scaling Up.</a> - Testing various models, Vision Transformers, and Ensembles.</li><li><a href=https://www.kaggle.com/code/jhoward/multi-target-road-to-the-top-part-4 target=_blank rel="nofollow noopener noreferrer">Multi-target.</a> - Train a multi-target model with Fastai.</li></ul><p>I&rsquo;ve personally learned a lot from the notebooks. Part of the codes in the post is adapted from the notebooks.</p></div><p>Now that we&rsquo;ve got the data, let&rsquo;s see how to start building a model out of it</p><p>For that we need ğŸ‘‡</p><h3 id=-pytorch-image-models>ğŸ¥‡ PyTorch Image Models</h3><p>There are many libraries to model computer vision tasks but PyTorch Image Models or <a href=https://github.com/rwightman/pytorch-image-models target=_blank rel="nofollow noopener noreferrer">TIMM</a> by <a href=https://www.linkedin.com/in/wightmanr/ target=_blank rel="nofollow noopener noreferrer">Ross Wightman</a> is arguably the most prominent one today.</p><p>The TIMM repository hosts hundreds of recent SOTA models maintained by Ross.
At this point (January 2023) we have 964 pre-trained models on TIMM and increasing as we speak.</p><p>You can install TIMM by simply:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install timm
</span></span></code></pre></div><p>One line of code, and we&rsquo;d have access to all models on TIMM!</p><p>With such a massive collection, it can be disorienting which model to start from.
Worry not, TIMM provides a function to search for model architectures with a <a href=https://www.delftstack.com/howto/python/python-wildcard/ target=_blank rel="nofollow noopener noreferrer">wildcard</a>.</p><p>Since we will be running the model on a mobile device, let&rsquo;s search for model names that contain the word <em>edge</em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> timm
</span></span><span style=display:flex><span>timm<span style=color:#f92672>.</span>list_models(<span style=color:#e6db74>&#39;*edge*&#39;</span>)
</span></span></code></pre></div><p>This outputs all models that match the wildcard.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span><span style=color:#e6db74>&#39;cs3edgenet_x&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;cs3se_edgenet_x&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;edgenext_base&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;edgenext_small&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;edgenext_small_rw&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;edgenext_x_small&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;edgenext_xx_small&#39;</span><span style=color:#f92672>]</span>
</span></span></code></pre></div><p>Looks like we have something related to the EdgeNeXt model.</p><p>With a simple search and reading through the preprint <a href=https://arxiv.org/abs/2206.10589 target=_blank rel="nofollow noopener noreferrer">EdgeNeXt - Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications</a>, looks like it&rsquo;s a fitting model for our application!</p><p>With the model name, you can now start training.
The TIMM repo provides various utility functions and training scripts. Feel free to use them.</p><p>In this post, I&rsquo;m going to show you an easy way to train a TIMM model using Fastai ğŸ‘‡</p><h3 id=-training-with-fastai>ğŸ‹ï¸â€â™€ï¸ Training with Fastai</h3><p><a href=https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/ target=_blank rel="nofollow noopener noreferrer">Fastai</a> is a deep learning library that provides practitioners with high high-level components that can quickly provide SOTA results.
Under the hood Fastai uses PyTorch but it abstracts away the details and incorporates various best practices in training a model.</p><p>Install Fastai with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install fastai
</span></span></code></pre></div><p>Since, we&rsquo;d run our model on a mobile device, let&rsquo;s select the smallest model we got from the previous section - <code>edgenext_xx_small</code>.</p><p>Let&rsquo;s import all the necessary packages with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> fastai.vision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span></code></pre></div><p>Next, load the images into a <code>DataLoader</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trn_path <span style=color:#f92672>=</span> Path(<span style=color:#e6db74>&#39;../data/train_images&#39;</span>)
</span></span><span style=display:flex><span>dls <span style=color:#f92672>=</span> ImageDataLoaders<span style=color:#f92672>.</span>from_folder(trn_path, seed<span style=color:#f92672>=</span><span style=color:#ae81ff>316</span>, 
</span></span><span style=display:flex><span>                                   valid_pct<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, bs<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>                                   item_tfms<span style=color:#f92672>=</span>[Resize((<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>))], 
</span></span><span style=display:flex><span>                                   batch_tfms<span style=color:#f92672>=</span>aug_transforms(min_scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.75</span>))
</span></span></code></pre></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Parameters for the <code>from_folder</code> method:</p><ul><li><code>trn_path</code> &ndash; A <code>Path</code> to the training images.</li><li><code>valid_pct</code> &ndash; The percentage of dataset to allocate as the validation set.</li><li><code>bs</code> &ndash; Batch size to use during training.</li><li><code>item_tfms</code> &ndash; Transformation applied to each item.</li><li><code>batch_tfms</code> &ndash; Random transformations applied to each batch to augment the dataset. Read more <a href=https://docs.fast.ai/vision.augment.html#aug_transforms target=_blank rel="nofollow noopener noreferrer">here</a>.</li></ul><p>ğŸ“ <strong>NOTE</strong>: Check out the Fastai <a href=https://docs.fast.ai/ target=_blank rel="nofollow noopener noreferrer">docs</a> for more information on the parameters.</p></div><p>You can show a batch of the train images loaded into the <code>DataLoader</code> with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dls<span style=color:#f92672>.</span>train<span style=color:#f92672>.</span>show_batch(max_n<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>, nrows<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><figure><a href=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch.png class=image-popup><img src=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch.png srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_1920x0_resize_box_3.png 1920w" sizes="(max-width: ) 100vw, " style=max-width:100%;height:auto></a></figure><p>Next create a <code>Learner</code> object which stores the model, dataloaders, and loss function to train a model.
Read more about the <code>Learner</code> <a href=https://docs.fast.ai/learner.html#learner target=_blank rel="nofollow noopener noreferrer">here</a>.</p><p>For vision classification tasks we can create a <code>Learner</code> by calling the <code>vision_learner</code> function and providing the necessary parameters:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn <span style=color:#f92672>=</span> vision_learner(dls, <span style=color:#e6db74>&#39;edgenext_xx_small&#39;</span>, metrics<span style=color:#f92672>=</span>accuracy)<span style=color:#f92672>.</span>to_fp16()
</span></span></code></pre></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>Parameters for <code>vision_learner</code>:</p><ul><li><strong>dls</strong> - The <code>Dataloader</code> object.</li><li><strong>edgenext_xx_small</strong> - Model name from TIMM.</li></ul><p>ğŸ“ <strong>NOTE</strong>: Read more on vision_learner <a href=https://docs.fast.ai/vision.learner.html#vision_learner target=_blank rel="nofollow noopener noreferrer">here</a>.</p><p>In Fastai, you can easily incorporate <a href=https://on-demand.gputechconf.com/gtc/2019/video/_/S9143/ target=_blank rel="nofollow noopener noreferrer">Mixed Precision Training</a> by adding the <code>.to_fp16()</code> method. This little trick reduces memory usage and trains your model faster at the cost of precision.</p></div><p>One of my favorite features in Fastai is the learning rate finder.
It lets you estimate the range of learning rate to train the model for the best results.</p><p>Find the best learning rate with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>lr_find()
</span></span></code></pre></div><figure><a href=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find.png class=image-popup><img src=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find.png srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_1920x0_resize_box_3.png 1920w" sizes="(max-width: ) 100vw, " style=max-width:100%;height:auto></a></figure><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>The orange dot ğŸŸ  shows the suggested learning rate which is approximately at <code>2e-3</code>.</p><p>A good learning rate lies at the point where the loss is <strong>decreasing most rapidly</strong>. On the plot, it&rsquo;s anywhere
from the orange dot ğŸŸ  to the point where the loss starts increasing again approximately at <code>1e-1</code>. I&rsquo;ll pick <code>1e-2</code> as my learning rate.</p><p>Read a post by Zach Mueller on <a href=https://walkwithfastai.com/lr_finder target=_blank rel="nofollow noopener noreferrer">how to pick a good learning rate</a>.</p></div><p>Now train the model for 5 <code>epochs</code> and a base learning rate of <code>0.002</code> with the <a href=https://arxiv.org/pdf/1803.09820.pdf target=_blank rel="nofollow noopener noreferrer">1cycle policy</a>.
The <code>ShowGraphCallback</code> callback plots the progress of the training.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>5</span>, base_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span>, cbs<span style=color:#f92672>=</span>[ShowGraphCallback()])
</span></span></code></pre></div><figure><a href=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train.png class=image-popup><img src=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train.png srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_1920x0_resize_box_3.png 1920w" sizes="(max-width: ) 100vw, " style=max-width:100%;height:auto></a></figure><p>With just a few lines of code, we can train a reasonably good model with Fastai.
For completeness, here are the few lines of codes you need to load and train the model:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> fastai.vision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>trn_path <span style=color:#f92672>=</span> Path(<span style=color:#e6db74>&#39;../data/train_images&#39;</span>)
</span></span><span style=display:flex><span>dls <span style=color:#f92672>=</span> ImageDataLoaders<span style=color:#f92672>.</span>from_folder(trn_path, seed<span style=color:#f92672>=</span><span style=color:#ae81ff>316</span>,
</span></span><span style=display:flex><span>                                  valid_pct<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, bs<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>                                  item_tfms<span style=color:#f92672>=</span>[Resize((<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>))], 
</span></span><span style=display:flex><span>                                  batch_tfms<span style=color:#f92672>=</span>aug_transforms(min_scale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.75</span>))
</span></span><span style=display:flex><span>learn <span style=color:#f92672>=</span> vision_learner(dls, <span style=color:#e6db74>&#39;edgenext_xx_small&#39;</span>, metrics<span style=color:#f92672>=</span>accuracy)<span style=color:#f92672>.</span>to_fp16()
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>5</span>, base_lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span>, cbs<span style=color:#f92672>=</span>[ShowGraphCallback()])
</span></span></code></pre></td></tr></table></div></div><p>Â </p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>For demonstration purposes, I&rsquo;ve only with only 5 <code>epochs</code>. You can train for longer to obtain better accuracy and model performance.</p><p>ğŸ“ <strong>NOTE</strong>: View my training notebook <a href=https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/train/train.ipynb target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><p>You can optionally export the <code>Learner</code> object and import it from another script or notebook with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>export(<span style=color:#e6db74>&#34;../../train/export.pkl&#34;</span>)
</span></span></code></pre></div><p>Once done, now it&rsquo;s time we transform the model into a form we can use for mobile inference.</p><p>For that, we&rsquo;ll need ğŸ‘‡</p><h3 id=-exporting-with-torchscript>ğŸ“€ Exporting with TorchScript</h3><p>In this section, we export the model into a form suitable for a mobile device.
We can do that easily with <a href=https://pytorch.org/docs/stable/jit.html target=_blank rel="nofollow noopener noreferrer">TorchScript</a>.</p><blockquote class=blockquote><p class=mb-0>TorchScript is a way to create serializable and optimizable models from PyTorch code on
a variety of platforms, including desktop and mobile devices, without requiring a Python runtime.</p><footer class=blockquote-footer>TorchScript Docs</footer></blockquote><p>With TorchScript, the model&rsquo;s code is converted into a static graph that can be optimized for faster performance, and then saved and loaded as a serialized representation of the model.</p><p>This allows for deployment to a variety of platforms and acceleration with hardware such as GPUs, TPUs, and mobile devices.</p><p>All the models on TIMM can be exported with TorchScript using the following code snippet.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">9
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.mobile_optimizer <span style=color:#f92672>import</span> optimize_for_mobile
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>cpu()
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>example <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>)
</span></span><span style=display:flex><span>traced_script_module <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>jit<span style=color:#f92672>.</span>trace(learn<span style=color:#f92672>.</span>model, example)
</span></span><span style=display:flex><span>optimized_traced_model <span style=color:#f92672>=</span> optimize_for_mobile(traced_script_module)
</span></span><span style=display:flex><span>optimized_traced_model<span style=color:#f92672>.</span>_save_for_lite_interpreter(<span style=color:#e6db74>&#34;torchscript_edgenext_xx_small.pt&#34;</span>)
</span></span></code></pre></td></tr></table></div></div><p>Â </p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>From the snippet above we need to specify a few things:</p><ul><li><code>Line 6</code>: The shape of the input image tensor.</li><li><code>Line 9</code>: &ldquo;torchscript_edgenext_xx_small.pt&rdquo; is the name of the resulting TorchScript serialized model.</li></ul><p>If you already have your own <code>model.pt</code> file, replace <code>Line 4</code> and Line <code>5</code> with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#39;model.pt&#39;</span>, map_location<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>eval()
</span></span></code></pre></div><p>ğŸ“ <strong>NOTE</strong>: View the full notebook from training to exporting the model on my GitHub repo <a href=https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/train/train.ipynb target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><p>Once completed, you&rsquo;ll have a file <code>torchscript_edgenext_xx_small.pt</code> that can be ported to other devices for inference.
In this post, I will be porting it to Android using a framework known as <a href=https://flutter.dev/ target=_blank rel="nofollow noopener noreferrer">Flutter</a>.</p><h3 id=-inference-in-flutter>ğŸ“² Inference in Flutter</h3><p><img src=./vids/flutter.gif alt=img></p><blockquote class=blockquote><p class=mb-0>Flutter is an open-source framework by Google for building beautiful, natively compiled, multi-platform applications from a single codebase.</p><footer class=blockquote-footer>Flutter Webpage</footer></blockquote><p>We can load the <code>torchscript_edgenext_xx_small.pt</code> and use if for inference.
To do so, we will use the <a href=https://github.com/zezo357/pytorch_lite target=_blank rel="nofollow noopener noreferrer">pytorch_lite</a> Flutter package.
The <code>pytorch_lite</code> package supports image classification and detection with TorchScript.</p><p>The following code snippet shows a function to load our serialized model <code>torchscript_edgenext_xx_small.pt</code>.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dart data-lang=dart><span style=display:flex><span>Future loadModel() <span style=color:#66d9ef>async</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>String</span> pathImageModel <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;assets/models/torchscript_edgenext_xx_small.pt&#34;</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>        _imageModel <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> PytorchLite.loadClassificationModel(
</span></span><span style=display:flex><span>            pathImageModel, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>,
</span></span><span style=display:flex><span>            labelPath: <span style=color:#e6db74>&#34;assets/labels/label_classification_paddy.txt&#34;</span>);
</span></span><span style=display:flex><span>    } on PlatformException {
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;only supported for Android&#34;</span>);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></td></tr></table></div></div><p>Â </p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>note</p><p>From the snippet above we need to specify a few things:</p><ul><li><code>Line 2</code>: Path to the serialized model.</li><li><code>Line 5</code>: The input image size - <code>224</code> by <code>224</code> pixels.</li><li><code>Line 6</code>: A text file with labels associated with each class.</li></ul><p>View the full code on my GitHub <a href=https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/flutter_app/lib/main.dart target=_blank rel="nofollow noopener noreferrer">repo</a>.</p></div><p>The following code snippet shows a function to run the inference.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dart data-lang=dart><span style=display:flex><span>Future runClassification() <span style=color:#66d9ef>async</span> {
</span></span><span style=display:flex><span>    <span style=color:#75715e>//pick an image
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>final</span> XFile<span style=color:#f92672>?</span> image <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> _picker.pickImage(<span style=color:#66d9ef>source</span><span style=color:#f92672>:</span> ImageSource.gallery);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (image <span style=color:#f92672>!=</span> <span style=color:#66d9ef>null</span>) {
</span></span><span style=display:flex><span>      <span style=color:#75715e>// run inference
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      <span style=color:#66d9ef>var</span> result <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> _imageModel<span style=color:#f92672>!</span>
</span></span><span style=display:flex><span>          .getImagePredictionResult(<span style=color:#66d9ef>await</span> File(image.path).readAsBytes());
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      setState(() {
</span></span><span style=display:flex><span>        _imagePrediction <span style=color:#f92672>=</span> result[<span style=color:#e6db74>&#39;label&#39;</span>];
</span></span><span style=display:flex><span>        _predictionConfidence <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>            (result[<span style=color:#e6db74>&#39;probability&#39;</span>] <span style=color:#f92672>*</span> <span style=color:#ae81ff>100</span>).toStringAsFixed(<span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>        _image <span style=color:#f92672>=</span> File(image.path);
</span></span><span style=display:flex><span>      });
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span></code></pre></td></tr></table></div></div><p>Those are the two important functions to load and run the TorchScript model.</p><p>The following screen capture shows the Flutter app in action.
The clip runs in real-time and is <strong>NOT sped up</strong>!</p><video controls preload=auto width=400px autoplay loop muted playsinline class=html-video>
<source src=/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/inference_edgenext.mp4 type=video/mp4><span></span></video><p>The compiled <code>.apk</code> file is about <strong>77MB</strong> in size and the inference time is at <strong>100 ms</strong> or below on my Pixel 3 XL!</p><p>Try it out and install the pre-built <code>.apk</code> file on your Android phone <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/app-release.apk?raw=true" target=_blank rel="nofollow noopener noreferrer">here</a>.</p><h3 id=-comments--feedback>ğŸ™ Comments & Feedback</h3><p>That&rsquo;s a wrap! In this post, I&rsquo;ve shown you how you can start from a model, train it, and deploy it on a mobile device for edge inference.</p><div class="notice tip"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"/></svg></span>tip</p><p>âš¡ In short we learned how to:</p><ul><li>Load a SOTA classification model from TIMM and train it with Fastai.</li><li>Export the trained model with TorchScript for inference.</li><li>Create a functional Android app and run the model inference on your device.</li></ul><p>ğŸ“ <strong>NOTE</strong>: View the codes for the entire post on my GitHub repo <a href=https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/ target=_blank rel="nofollow noopener noreferrer">here</a>.</p></div><p>What&rsquo;s next? If you&rsquo;d like to learn about how I deploy a cloud based object detection model on Android, check it out <a href=../how_to_deploy_od_models_on_android_with_flutter/>here</a>.</p><p>I hope you&rsquo;ve learned a thing or two from this blog post.
If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or <a href=https://dicksonneoh.com/contact/ target=_blank rel="nofollow noopener noreferrer">drop me a message</a>. Alternatively you can also comment on this Hacker News <a href="https://news.ycombinator.com/item?id=34799597#34801672" target=_blank rel="nofollow noopener noreferrer">thread</a>.</p><blockquote class=twitter-tweet><p lang=en dir=ltr>Tired of training models that never see the light of day? Don't let your hard work go to waste!<br><br>In this ğŸ§µ, I'll show you how to pick from over 900+ models from TIMM by <a href="https://twitter.com/wightmanr?ref_src=twsrc%5Etfw">@wightmanr</a> , train them with Fastai by <a href="https://twitter.com/jeremyphoward?ref_src=twsrc%5Etfw">@jeremyphoward</a> , and deploy them on Android â€“ all for free. <a href=https://t.co/25pgunaJNM>pic.twitter.com/25pgunaJNM</a></p>&mdash; Dickson Neoh ğŸš€ (@dicksonneoh7) <a href="https://twitter.com/dicksonneoh7/status/1625367344712388609?ref_src=twsrc%5Etfw">February 14, 2023</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script>
<iframe src=https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:7032246822186209280 height=1198 width=504 onload='(function(e){e.style.height=e.contentWindow.document.body.scrollHeight+"px"})(this)' frameborder=0 allowfullscreen title="Embedded post"></iframe><section class=social-share><ul class=share-icons><hr><h5>ğŸ¤Ÿ Follow me</h5><p>Don't want to miss any of my future content? Follow me on Twitter and LinkedIn where I share these tips in
bite-size posts.</p><li><a href=https://twitter.com/dicksonneoh7 target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn x"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path fill="#fff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>&nbsp;
Twitter</a></li>&nbsp;<li><a href=https://www.linkedin.com/in/dickson-neoh/ target=_blank rel=noopener aria-label="Follow on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg>&nbsp;
LinkedIn</a></li>&nbsp;<li><a href=https://github.com/dnth/ target=_blank rel=noopener aria-label="Follow on GitHub" class="share-btn github"><svg width="24" height="24" viewBox="0 0 256 250" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid"><g><path d="M128.00106.0C57.3172926.0.0 57.3066942.0 128.00106c0 56.554221 36.6761997 104.534482 87.534937 121.459839 6.3970853 1.18487999999999 8.745651-2.776734 8.745651-6.157566C96.280588 240.251045 96.1618878 230.167899 96.106777 219.472176 60.4967585 227.215235 52.9826207 204.369712 52.9826207 204.369712c-5.8226623-14.795114-14.2122127-18.729174-14.2122127-18.729174C27.1568785 177.696113 39.6458206 177.859325 39.6458206 177.859325 52.4993419 178.762293 59.267365 191.04987 59.267365 191.04987c11.4164025 19.568553 29.9442103 13.911223 37.2485035 10.640612C97.6647155 193.417512 100.981959 187.77078 104.642583 184.574357 76.211799 181.33766 46.324819 170.362144 46.324819 121.315702c0-13.974813 5.0002398-25.3933338 13.1884247-34.3573083-1.3290169-3.2239785-5.7103208-16.2428317 1.2399917-33.8740301.0.0 10.7487147-3.4401823 35.2094058 13.1205959 10.2103258-2.8360835 21.1604058-4.2583646 32.0384188-4.3071163C138.879073 61.9465949 149.837632 63.368876 160.067033 66.2049595c24.431017-16.5607782 35.164893-13.1205959 35.164893-13.1205959C202.199197 70.715562 197.815773 83.7344152 196.486756 86.9583937 204.694018 95.9223682 209.660343 107.340889 209.660343 121.315702c0 49.163023-29.94421 59.988045-58.447062 63.156912 4.591149 3.97221400000001 8.682061 11.761904 8.682061 23.703979.0 17.126724-.14837399999999 30.910768-.14837399999999 35.12674C159.746968 246.709601 162.05102 250.70089 168.53925 249.443941 219.370432 232.499507 256 184.536204 256 128.00106 256 57.3066942 198.691187.0 128.00106.0zM47.9405593 182.340212C47.6586465 182.976105 46.6581745 183.166873 45.7467277 182.730227 44.8183235 182.312656 44.2968914 181.445722 44.5978808 180.80771 44.8734344 180.152739 45.876026 179.97045 46.8023103 180.409216 47.7328342 180.826786 48.2627451 181.702199 47.9405593 182.340212zm6.2962299 5.618042C53.6263318 188.524199 52.4329723 188.261363 51.6232682 187.366874 50.7860088 186.474504 50.6291553 185.281144 51.2480912 184.70672 51.8776254 184.140775 53.0349512 184.405731 53.8743302 185.298101 54.7115892 186.201069 54.8748019 187.38595 54.2367892 187.958254zM58.5562413 195.146347C57.7719732 195.691096 56.4895886 195.180261 55.6968417 194.042013 54.9125733 192.903764 54.9125733 191.538713 55.713799 190.991845 56.5086651 190.444977 57.7719732 190.936735 58.5753181 192.066505 59.3574669 193.22383 59.3574669 194.58888 58.5562413 195.146347zM65.8613592 203.471174C65.1597571 204.244846 63.6654083 204.03712 62.5716717 202.981538 61.4524999 201.94927 61.1409122 200.484596 61.8446341 199.710926 62.5547146 198.935137 64.0575422 199.15346 65.1597571 200.200564 66.2704506 201.230712 66.6095936 202.705984 65.8613592 203.471174zM75.3025151 206.281542C74.9930474 207.284134 73.553809 207.739857 72.1039724 207.313809 70.6562556 206.875043 69.7087748 205.700761 70.0012857 204.687571 70.302275 203.678621 71.7478721 203.20382 73.2083069 203.659543 74.6539041 204.09619 75.6035048 205.261994 75.3025151 206.281542zM86.046947 207.473627C86.0829806 208.529209 84.8535871 209.404622 83.3316829 209.4237 81.8013 209.457614 80.563428 208.603398 80.5464708 207.564772c0-1.066181 1.20183800000001-1.93311500000002 2.7322209-1.958551C84.8005962 205.576546 86.046947 206.424403 86.046947 207.473627zM96.6021471 207.069023C96.7844366 208.099171 95.7267341 209.156872 94.215428 209.438785 92.7295577 209.710099 91.3539086 209.074206 91.1652603 208.052538 90.9808515 206.996955 92.0576306 205.939253 93.5413813 205.66582 95.054807 205.402984 96.4092596 206.021919 96.6021471 207.069023z" fill="#fff"/></g></svg>&nbsp;
GitHub</a></li>&nbsp;<hr><h5>ğŸ”„ Share this post</h5><li><a href="https://twitter.com/intent/tweet?&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f&text=PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter @dicksonneoh7" target=_blank rel=noopener aria-label="Share on Twitter" class="share-btn x"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path fill="#fff" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>&nbsp;
Twitter</a></li>&nbsp;<li><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f&source=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f&title=PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter&summary=PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter" target=_blank rel=noopener aria-label="Share on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg>&nbsp;
LinkedIn</a></li>&nbsp;<li><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f" target=_blank rel=noopener aria-label="Share on Facebook" class="share-btn facebook"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="facebook_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-7.8050197" inkscape:cy="32.710925" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.36281,-104.14567)"><path d="m130.36281 104.72294v5.19545c0 .3157.26158.57728.57727.57728h5.19546c.3157.0.57727-.26158.57727-.57728v-5.19545c0-.3157-.26157-.57727-.57727-.57727h-5.19546c-.31569.0-.57727.26157-.57727.57727zm5.77273.0v5.19545h-1.4973v-1.94829h.74865l.10824-.86591h-.85689v-.55923c0-.25256.0631-.42394.42393-.42394h.46904v-.78473c-.0794-.0108-.35719-.0271-.67649-.0271-.66567.0-1.11847.40048-1.11847 1.14553v.64943h-.75767v.86591h.75767v1.94829h-2.79617v-5.19545z" id="path1085" style="stroke-width:.0180398;fill:#fff"/></g></svg>&nbsp;
Facebook</a></li>&nbsp;<br><li><a href="https://telegram.me/share/url?text=PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter&url=https%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f" target=_blank rel=noopener aria-label="Share on Telegram" class="share-btn telegram"><svg width="7.3503098mm" height="7.1592798mm" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473"><g><path d="M152.531 179.476c-1.48.0-2.95-.438-4.211-1.293l-47.641-32.316-25.552 18.386c-2.004 1.441-4.587 1.804-6.914.972-2.324-.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821 93.928c-2.886-1.104-4.8-3.865-4.821-6.955-.021-3.09 1.855-5.877 4.727-7.02l174.312-69.36c.791-.336 1.628-.53 2.472-.582.302-.018.605-.018.906-.001 1.748.104 3.465.816 4.805 2.13.139.136.271.275.396.42 1.11 1.268 1.72 2.814 1.835 4.389.028.396.026.797-.009 1.198-.024.286-.065.571-.123.854L159.898 173.38c-.473 2.48-2.161 4.556-4.493 5.523C154.48 179.287 153.503 179.476 152.531 179.476zm-47.669-48.897 42.437 28.785L170.193 39.24l-82.687 79.566 17.156 11.638C104.731 130.487 104.797 130.533 104.862 130.579zM69.535 124.178l5.682 21.53 12.242-8.809-16.03-10.874C70.684 125.521 70.046 124.893 69.535 124.178zM28.136 86.782l31.478 12.035c2.255.862 3.957 2.758 4.573 5.092l3.992 15.129c.183-1.745.974-3.387 2.259-4.624L149.227 38.6 28.136 86.782z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>&nbsp;
Telegram</a></li>&nbsp;<li><a href="whatsapp://send?text=PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20and%20deploy%20over%20900%2b%20cutting%20edge%20PyTorch%20classification%20models%20on%20Android.%20%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f%0a" target=_blank aria-label="Share on WhatsApp" class="share-btn whatsapp"><svg width="6.0324998mm" height="6.05896mm" viewBox="0 0 6.0324997 6.05896" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="whatsapp_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="4.9987205" inkscape:cy="35.692618" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-126.67735,-103.17712)"><path d="m131.83672 104.07671c-.58208-.58209-1.34937-.89959-2.14312-.89959-1.64042.0-2.98979 1.34938-2.98979 3.01625.0.52917.13229 1.03188.39687 1.48167l-.42333 1.56104 1.5875-.42333c.42333.23812.92604.34396 1.42875.34396 1.66687.0 3.01625-1.34938 3.01625-3.01625-.0265-.74084-.3175-1.50813-.87313-2.06375zm-2.14312 4.6302c-.44979.0-.87313-.13229-1.27-.34395l-.10583-.0529-.92605.26458.26459-.89958-.0794-.13229c-.26458-.39688-.37041-.84667-.37041-1.34938.0-1.37583 1.11125-2.48708 2.48708-2.48708.66146.0 1.29646.26458 1.77271.74083.47625.47625.74083 1.11125.74083 1.77271-.0265 1.37583-1.13771 2.48708-2.51354 2.48708zm1.34937-1.87854c-.0794-.0265-.44979-.23812-.5027-.26458-.0794-.0265-.1323-.0265-.18521.0265-.0529.0794-.21167.26458-.23813.29104-.0529.0529-.0794.0529-.15875.0265-.0794-.0265-.3175-.13229-.60854-.37042-.23812-.21167-.37042-.44979-.42333-.52917-.0529-.0794.0-.13229.0265-.15875.0265-.0264.0794-.0794.10583-.13229.0529-.0265.0794-.0794.10583-.13229.0265-.0529.0-.10583.0-.13229.0-.0265-.18521-.39688-.26458-.55563-.0265-.10583-.10583-.0794-.13229-.0794h-.15875s-.10584.0265-.18521.0794c-.0794.0794-.26458.26459-.26458.635.0.37042.26458.74084.29104.79375.0265.0529.52916.82021 1.29646 1.13771.1852.0794.3175.13229.42333.15875.18521.0529.34396.0529.47625.0265.15875-.0265.44979-.18521.50271-.34396.0529-.18521.0529-.3175.0529-.34396-.0264-.0794-.0794-.10583-.15875-.13229z" id="path1793" style="stroke-width:.264583;fill:#fff"/></g></svg>&nbsp;
WhatsApp</a></li>&nbsp;<li><a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter.&body=PyTorch%20at%20the%20Edge%3a%20Deploying%20Over%20964%20TIMM%20Models%20on%20Android%20with%20TorchScript%20and%20Flutter%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLearn%20and%20deploy%20over%20900%2b%20cutting%20edge%20PyTorch%20classification%20models%20on%20Android.%20%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fportfolio%2fpytorch_at_the_edge_timm_torchscript_flutter%2f%0a" target=_blank class="share-btn email" aria-label="Share via Email"><svg width="6.3499999mm" height="4.3961601mm" viewBox="0 0 6.3499999 4.3961601" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="email_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.7526575" inkscape:cy="33.4125" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.10375,-103.97942)"><path d="m130.10375 104.22365v3.9077.24423h.24423 5.86154.24423v-.24423-3.9077-.24423h-.24423-5.86154-.24423zm5.29675.24423-2.12175 1.41196-2.12175-1.41196zm-2.25913 1.91569.13738.0839.13738-.0839 2.54916-1.70198v3.20553h-5.37308v-3.20553z" id="path824" style="stroke-width:.0152644;fill:#fff"/></g></svg>&nbsp;
Email</a></li><hr><section><h5>â¤ï¸ Show some love</h5><p>Creating free ML contents doesn't pay my bills. Support me in creating more free contents like these.
Consider buying me a coffee. Your support means a lot to me.</p><div style=text-align:center><a href=https://www.buymeacoffee.com/dicksonneoh target=_blank><img src=https://cdn.buymeacoffee.com/buttons/v2/default-blue.png alt="Buy Me A Coffee" style=height:60px!important;width:217px!important></a></div></section><hr></ul></section></div></div></div><div class=row><div class="col-lg-10 offset-lg-1"><nav class="case-details-nav d-flex justify-content-between align-items-start"><div class=previous><div class="d-flex align-items-center mb-3"><div class="icon mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1243" fill="#2d2d2d"><path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z"/><path data-name="Path 1455" d="M13.137 1.41 3.39 15.558l-.975-1.415L12.166.0z"/></g></svg></div><span class=small>Prev blog</span></div><div class=blog-nav-item><div class=blog-nav-thumb><a href=https://dicksonneoh.com/portfolio/fastdup_manage_clean_curate/><img src=https://dicksonneoh.com/images/portfolio/fastdup_manage_clean_curate/thumbnail.gif alt=post-image></a></div><h5 class=title><a class=text-dark href=https://dicksonneoh.com/portfolio/fastdup_manage_clean_curate/>fastdup: A Powerful Tool to Manage, Clean & Curate Visual Data at Scale on Your CPU - For Free.</a></h5></div></div><div class=next><div class="d-flex align-items-center justify-content-end mb-3"><span class=small>Next blog</span><div class="icon ml-3"><svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1244" fill="#2d2d2d"><path data-name="Path 1456" d="M12.162 12.725 2.416 26.87l.978 1.41 9.746-14.138z"/><path data-name="Path 1455" d="M2.416 1.415l9.743 14.141.975-1.414L3.39.0z"/></g></svg></div></div><div class=blog-nav-item><div class=blog-nav-thumb><a href=https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/><img src=https://dicksonneoh.com/images/portfolio/bringing_high_quality_image_models_to_mobile/thumbnail.gif alt=post-image></a></div><h5 class=title><a class=text-dark href=https://dicksonneoh.com/portfolio/bringing_high_quality_image_models_to_mobile/>Bringing High-Quality Image Models to Mobile: Hugging Face TIMM Meets Android & iOS</a></h5></div></div></nav></div></div></div></section></div><section class=footer id=contact><div class=footer__background_shape><svg viewBox="0 0 1920 79"><path d="M0 0h1920v79L0 0z" data-name="Path 1450"/></svg></div><div class=container><div class=row><div class=col-lg-12><div class=footer__cta><div class=shape-1><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class=shape-2><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class="text-light footer__cta_content"><span>Contact me</span><h2 class="mb-0 mb-3">Letâ€™s Start a Project</h2></div><div class=footer__cta_action></div><a href="https://api.whatsapp.com/send?phone=60133250827" rel=noopener target=_blank class="btn btn-light btn-zoom mr-3 mb-3 fa fa-whatsapp my-float">&nbsp&nbspChat on WhatsApp</a>
<a href=https://t.me/dicksonneoh rel=noopener target=_blank class="btn btn-light btn-zoom mr-3 mb-3 fa fa-telegram my-float">&nbsp&nbspChat on Telegram</a>
<a class="btn btn-light btn-zoom mb-3 fa fa-envelope" href=https://dicksonneoh.com/contact>&nbsp&nbspSend me a message</a></div></div></div><div class="row footer__widget"><div class=col-lg-4><div class="footer__widget_logo mb-5"><img src=https://dicksonneoh.com/images/site-navigation/logo_resized.png alt=widget-logo></div></div><div class=col-lg-4><div class="text-light footer__widget_sitemap mb-5"><h4 class=base-font>Sitemap</h4><ul class="unstyle-list small"><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>About me</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Frequently Ask Question</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Privacy & Policy</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Latest Article</a></li></ul></div></div><div class=col-lg-4><div class="text-light footer__widget_address mb-5"><h4 class=base-font>Address</h4><ul class="fa-ul small"><li class=mb-2><a class=text-light href=tel:+%2860%29%203%208921%202020><span class=fa-li><i class="fa fa-phone"></i></span>+(60) 3 8921 2020</a></li><li class=mb-2><a class=text-light href=mailto:dickson.neoh@gmail.com><span class=fa-li><i class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li><li class=mb-2><span class=fa-li><i class="fa fa-map-marker"></i></span>Kuala Lumpur, Malaysia.</a></li></ul></div></div></div><div class="row footer__footer"><div class=col-lg-6><div class="footer__footer_copy text-light"><p>All right reserved copyright Â© Dickson Neoh 2024</p></div></div><div class=col-lg-6><div class=footer__footer_social><ul class=unstyle-list><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://www.linkedin.com/in/dickson-neoh/><i class="fa fa-linkedin-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://twitter.com/dicksonneoh7><i class="fa fa-twitter-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://github.com/dnth><i class="fa fa-github-square"></i></a></li></ul></div></div></div></div></section><script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script>
<script src=https://dicksonneoh.com/plugins/jQuery/jquery.min.js></script>
<script src=https://dicksonneoh.com/plugins/bootstrap/bootstrap.min.js></script>
<script src=https://dicksonneoh.com/plugins/slick/slick.min.js></script>
<script src=https://dicksonneoh.com/plugins/waypoint/jquery.waypoints.min.js></script>
<script src=https://dicksonneoh.com/plugins/magnafic-popup/jquery.magnific-popup.min.js></script>
<script src=https://dicksonneoh.com/plugins/tweenmax/TweenMax.min.js></script>
<script src=https://dicksonneoh.com/plugins/imagesloaded/imagesloaded.min.js></script>
<script src=https://dicksonneoh.com/plugins/masonry/masonry.min.js></script>
<script src=https://dicksonneoh.com/js/form-handler.min.js></script>
<script src=https://dicksonneoh.com/js/script.min.js></script></body></html>