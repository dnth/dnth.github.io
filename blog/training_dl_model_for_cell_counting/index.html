<!doctype html><html><head><meta charset=utf-8><title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code</title><meta property="og:title" content="Training a Deep Learning Model for Cell Counting in 17 Lines of Code"><meta property="og:description" content="Leveraging state-of-the-art models on IceVision and Fastai"><meta property="og:type" content="article"><meta property="og:url" content="https://dicksonneoh.com/blog/training_dl_model_for_cell_counting/"><meta property="og:image" content="https://dicksonneoh.com/images/blog/training_dl_model_for_cell_counting/post_image.jpg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-04-07T15:07:15+08:00"><meta property="article:modified_time" content="2022-04-07T15:07:15+08:00"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/slick/slick-theme.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/font-awesome/css/font-awesome.min.css><link rel=stylesheet href=https://dicksonneoh.com/plugins/magnafic-popup/magnific-popup.css><link href=https://dicksonneoh.com/scss/style.min.css rel=stylesheet><link rel="shortcut icon" href=https://dicksonneoh.com/images/favicon.ico type=image/x-icon><link rel=icon href=https://dicksonneoh.com/images/favicon.png type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=UA-54500366-2"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-54500366-2')</script></head><body><nav class="navbar navbar-expand-lg fixed-top"><div class=container><a href=https://dicksonneoh.com/ class=navbar-brand><img src=https://dicksonneoh.com/images/site-navigation/logo_dn_resize.png alt=site-logo></a>
<button type=button class="navbar-toggler collapsed" data-toggle=collapse data-target=#navbarCollapse>
<span class=navbar-toggler-icon></span><span class=navbar-toggler-icon></span><span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-between" id=navbarCollapse><ul class="nav navbar-nav main-navigation my-0 mx-auto"><li class=nav-item><a href=https://dicksonneoh.com/#home class="nav-link text-dark text-sm-center p-2">Home</a></li><li class=nav-item><a href=https://dicksonneoh.com/#about class="nav-link text-dark text-sm-center p-2">About</a></li><li class=nav-item><a href=https://dicksonneoh.com/#service class="nav-link text-dark text-sm-center p-2">Services</a></li><li class=nav-item><a href=https://dicksonneoh.com/#portfolio class="nav-link text-dark text-sm-center p-2">Works</a></li><li class=nav-item><a href=https://dicksonneoh.com/#resume class="nav-link text-dark text-sm-center p-2">Resume</a></li><li class=nav-item><a href=https://dicksonneoh.com/#skills class="nav-link text-dark text-sm-center p-2">Skills</a></li><li class=nav-item><a href=https://dicksonneoh.com/#blog class="nav-link text-dark text-sm-center p-2">Blog</a></li><li class=nav-item><a href=https://dicksonneoh.com/#contact class="nav-link text-dark text-sm-center p-2">Contact</a></li></ul><div class=navbar-nav><a href=https://dicksonneoh.com/contact class="btn btn-primary btn-zoom hire_button">Hire Me Now</a></div></div></div></nav><div id=content><header class=breadCrumb><div class=container><div class=row><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><h3 class=breadCrumb__title>Training a Deep Learning Model for Cell Counting in 17 Lines of Code</h3><nav aria-label=breadcrumb class="d-flex justify-content-center"><ol class="breadcrumb align-items-center"><li class=breadcrumb-item><a href=https://dicksonneoh.com/>Home</a></li><li class=breadcrumb-item><a href=https://dicksonneoh.com/blog>All Post</a></li><li class="breadcrumb-item active" aria-current=page>Training a Deep Learning Model for Cell Counting in 17 Lines of Code</li></ol></nav></div></div><div class="row p-3"><div class="col-lg-10 col-md-10 offset-lg-1 offset-md-0 text-center"><i class="fa fa-calendar"></i>&ensp;
April 7, 2022 &ensp; &ensp;
<i class="fa fa-clock-o"></i>&ensp;
11 mins read</div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-tag"></i>&ensp;
<a href=https://dicksonneoh.com/tags/icevision/>IceVision</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/fast.ai/>Fast.ai</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/counting/>counting</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/tags/cell/>cell</a></div><div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center"><i class="fa fa-folder"></i>&ensp;
<a href=https://dicksonneoh.com/categories/modeling/>modeling</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/categories/object-detection/>object-detection</a>
<span class=separator>‚Ä¢</span>
<a href=https://dicksonneoh.com/categories/biology/>biology</a></div></div></div></div></div></header><section class="section singleBlog"><div class=svg-img><img src=https://dicksonneoh.com/images/hero/figure-svg.svg alt></div><div class=animate-shape><img src=https://dicksonneoh.com/images/skill/skill-background-shape.svg alt><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 600 600"><defs><linearGradient id="d" x1=".929" y1=".111" x2=".263" y2=".935" gradientUnits="objectBoundingBox"><stop offset="0" stop-color="#f1f6f9"/><stop offset="1" stop-color="#f1f6f9" stop-opacity="0"/></linearGradient></defs><g data-name="blob-shape (3)"><path class="blob" fill="url(#d)" d="M455.4 151.1c43.1 36.7 73.4 92.8 60.8 136.3-12.7 43.5-68.1 74.4-111.3 119.4-43.1 45-74 104.1-109.8 109-35.9 5-76.7-44.2-111.8-89.2-35.2-45-64.7-85.8-70.8-132.6-6-46.8 11.6-99.6 46.7-136.3 35.2-36.6 88-57.2 142.4-58.8 54.5-1.7 110.6 15.6 153.8 52.2z"/></g></svg></div><div class=animate-pattern><img src=https://dicksonneoh.com/images/service/background-pattern.svg alt=background-shape></div><div class=container><div class=row><div class=col-lg-12><div class=singleBlog__feature><img src=https://dicksonneoh.com/images/blog/training_dl_model_for_cell_counting/post_image.jpg alt=feature-image></div></div></div><div class="row mt-5"><div class=col-lg-12><div class=singleBlog__content><hr><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#-motivation>üï∂Ô∏è Motivation</a></li><li><a href=#-step-1-installation>‚öôÔ∏è Step 1: Installation</a></li><li><a href=#-step-2-labeling-the-data>üîñ Step 2: Labeling the data</a></li><li><a href=#-step-3-modeling>üèÉ Step 3: Modeling</a><ul><li><a href=#preparing-datasets>Preparing datasets</a></li><li><a href=#choosing-library-model-and-backbone>Choosing library, model and backbone</a></li><li><a href=#metrics-and-training>Metrics and Training</a></li><li><a href=#exporting-model>Exporting model</a></li></ul></li><li><a href=#-step-4-inferencing-on-a-new-image>üß≠ Step 4: Inferencing on a new image</a></li></ul></nav><hr><h3 id=-motivation>üï∂Ô∏è Motivation</h3><p>Numerous biology and medical procedure involve counting cells from images taken with microscope.
Counting cells reveals the concentration of bacteria and viruses and gives vital information on the progress of a disease.
To accomplish the counting, a device known as the hemocytometer or a counting chamber is used.
The hemocytometer creates volumetric grid to divide each region on the image for accurate counting.</p><p>The following YouTube video illustrates the counting process using a hemocytometer.<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/WWS9sZbGj6A style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>As shown in the video, each cell in the image has to be manually and meticulously counted.
This process may be slow and is prone to human error.
What if we could automate the counting by using an intelligent deep learning model?</p><p>In this blog post, we will see how easy it gets for anyone to use the IceVision computer vision library and quickly train a sophisticated deep learning model to count microalgae cells.</p><p>For the purpose of this post, I&rsquo;ve acquired image samples from a lab with a colony of microalgae cells.
The following image shows a sample image of the cells as seen from a microscope.
The microalgae cells are in green.<figure><img src=/blog/training_dl_model_for_cell_counting/hemocytometer.jpg srcset="/blog/training_dl_model_for_cell_counting/hemocytometer_hu4fbdd0c3cfa1f358477553a8b9dba9d7_69858_360x0_resize_q75_box.jpg 360w, /blog/training_dl_model_for_cell_counting/hemocytometer_hu4fbdd0c3cfa1f358477553a8b9dba9d7_69858_720x0_resize_q75_box.jpg 720w, /blog/training_dl_model_for_cell_counting/hemocytometer_hu4fbdd0c3cfa1f358477553a8b9dba9d7_69858_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>By the end of this blog post, you should be able to train your own microalgae cell (or any other objects really) counter with the steps that I will walk you through.
Did I mention that all the tools used in this project are open-source and free of charge? Yes!!
If you&rsquo;re ready let&rsquo;s begin.</p><h3 id=-step-1-installation>‚öôÔ∏è Step 1: Installation</h3><p>We will be using a computer vision library known as IceVision - a computer vision focused library built to work with Fastai.
I highly recommend that you use a virtual environment like Anaconda to install the package.
<a href=https://www.geeksforgeeks.org/set-up-virtual-environment-for-python-using-anaconda/>Here</a> is how to set it up.</p><p>All the codes you will need to replicate this post is open-sourced on this Github <a href=https://github.com/dnth/microalgae-cell-counter-blogpost>repository</a>.
To get started, clone the Git repository:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>git clone https://github.com/dnth/microalgae-cell-counter-blogpost
</code></pre></div><p>Next, navigate into the directory:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cd microalgae-cell-counter-blogpost/
</code></pre></div><p>Install IceVision:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>bash icevision_install.sh cuda11 0.12.0
</code></pre></div><p>Depending on your system <code>CUDA</code> version, you may want to change <code>cuda11</code> to <code>cuda10</code> on older systems.
The number following the cuda version is the version of IceVision we are installing.
The version I&rsquo;m using for this blog post is <code>0.12.0</code>.
You can alternatively specify <code>master</code> to install the bleeding edge version of IceVision from the master branch on Github.</p><p>The installation may take a couple of minutes depending on the speed of your internet connection.
Allow the installation to complete before proceeding to the next step.</p><h3 id=-step-2-labeling-the-data>üîñ Step 2: Labeling the data</h3><p>Before embarking on any machine learning work, we must ensure that we have a dataset to work on.
Our task at hand is to construct a model that can count microalgaes.
Since there are no public dataset available, we will have to curate our own dataset.</p><p>The figure below shows a dozen of collected microalgae cell images in the <code>images/not_labeled/</code> folder.<figure><img src=/blog/training_dl_model_for_cell_counting/dataset_sample.png srcset="/blog/training_dl_model_for_cell_counting/dataset_sample_hu98e8c3c1e143fea035e0e34ff28651e7_150278_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/dataset_sample_hu98e8c3c1e143fea035e0e34ff28651e7_150278_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/dataset_sample_hu98e8c3c1e143fea035e0e34ff28651e7_150278_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>There is only one issue now, and that is the images are not annotated.
We will have to annotate all the images with an open source image labeling tool known as <a href=https://github.com/tzutalin/labelImg>labelImg</a>.</p><p><code>labelImg</code> app enables us to annotate any image with class labels and bounding boxes surrounding the object of interest.
The following figure shows a demo of <code>labelImg</code> taken from the GitHub repository.<figure><img src=/blog/training_dl_model_for_cell_counting/labelimg_demo.jpg srcset="/blog/training_dl_model_for_cell_counting/labelimg_demo_hu9475e40123017607cf3e014e405fbc8c_91238_360x0_resize_q75_box.jpg 360w, /blog/training_dl_model_for_cell_counting/labelimg_demo_hu9475e40123017607cf3e014e405fbc8c_91238_720x0_resize_q75_box.jpg 720w, /blog/training_dl_model_for_cell_counting/labelimg_demo_hu9475e40123017607cf3e014e405fbc8c_91238_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>Type <code>labelImg</code> in your terminal to launch the <code>labelImg</code> app.
A window like the following should appear.<figure><img src=/blog/training_dl_model_for_cell_counting/labelimg_start.png srcset="/blog/training_dl_model_for_cell_counting/labelimg_start_hua063e19da832683a0ee146d7b42e31f5_56756_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/labelimg_start_hua063e19da832683a0ee146d7b42e31f5_56756_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/labelimg_start_hua063e19da832683a0ee146d7b42e31f5_56756_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>Now, let&rsquo;s load the folder that contains the microalgae images into <code>labelImg</code> and annotate them!
To do that, click on the <strong>Open Dir</strong> icon and navigate to the folder containing the images at <code>images/not_labeled/</code>.
An image should now show up in <code>labelImg</code>.
Next click on the <strong>Create RectBox</strong> icon to start drawing bounding boxes around the microalgaes. Next you will be prompted to enter a label name.
Key in microalgae as the label name. Once done, a rectangular bounding box should appear on-screen.</p><figure><img src=/blog/training_dl_model_for_cell_counting/labelimg_loaded.png srcset="/blog/training_dl_model_for_cell_counting/labelimg_loaded_hu08bb15feff40daa8fd9c05d0efc18ecd_490968_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/labelimg_loaded_hu08bb15feff40daa8fd9c05d0efc18ecd_490968_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/labelimg_loaded_hu08bb15feff40daa8fd9c05d0efc18ecd_490968_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><p>Now comes the repetitive part, we will need to draw a bounding box for each microalgae cell for all images in the folder.
To accelerate the process I highly recommend the use of Hotkeys keys with <code>labelImg</code>.<figure><img src=/blog/training_dl_model_for_cell_counting/hotkeys.png srcset="/blog/training_dl_model_for_cell_counting/hotkeys_hu9473a972c5fd24c935bd5378818c6122_49501_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/hotkeys_hu9473a972c5fd24c935bd5378818c6122_49501_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/hotkeys_hu9473a972c5fd24c935bd5378818c6122_49501_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px" width=400></figure></p><p>Once done, remember to save the annotations. The annotations are saved in <code>XML</code> file with a filename matching to image as shown below.<figure><img src=/blog/training_dl_model_for_cell_counting/xml_files.png srcset="/blog/training_dl_model_for_cell_counting/xml_files_hudf1c610607a47739799ee163ed5798e4_45665_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/xml_files_hudf1c610607a47739799ee163ed5798e4_45665_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/xml_files_hudf1c610607a47739799ee163ed5798e4_45665_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>Once all images are labelled, we will partition the image and annotations into three sets namely train set, validation set and test set.
These will be used to train and evaluate our model in the next section.</p><p>It took a few hours to meticulously label the images.
The labeled images can be found in the <code>images/labeled/</code> folder in case you didn&rsquo;t want to label them as I did.</p><h3 id=-step-3-modeling>üèÉ Step 3: Modeling</h3><p>Once the labeling is done, we are now ready to start modeling.
The modeling will be done in a <code>jupyter</code> notebook environment.</p><p>To launch jupyter notebook run</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>jupyter lab
</code></pre></div><p>A browser window should pop up.
On the left pane, double click the <code>train.ipynb</code> to open the notebook.</p><p>All the codes in this section are in the <code>train.ipynb</code> notebook.
Here, I will attempt to walk you through just enough details of the code to get you started with modeling on your own data.
If you require further clarifications, the IceVision <a href=https://airctic.com/0.12.0/>documentation</a> is a good starting point.
Alternatively, <a href=https://dicksonneoh.com/contact/>drop me a message</a>.</p><p>The first cell in the notebook are the imports.
With IceVision all the necessary components are imported with one line of code</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</code></pre></div><p>If something wasn&rsquo;t properly installed, the imports will raise an error message.
In that event, you must go back to the installation (Step 1) before proceeding further.
If there are no errors, we are ready to dive in further.</p><h4 id=preparing-datasets>Preparing datasets</h4><p>After the imports, we must now read the labeled images and bounding boxes into the <code>jupyter</code> notebook.
This is also known as parsing the data and can be accomplished with the following</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>parser <span style=color:#f92672>=</span> parsers<span style=color:#f92672>.</span>VOCBBoxParser(annotations_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;images/labeled&#34;</span>, images_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;images/labeled&#34;</span>)
</code></pre></div><p>The argument <code>annotations_dir</code> and <code>images_dir</code> are the directory to the images and annotations respectively.
Since we had both the images and annotations in the same directory, they are the same as specified in the code.</p><p>Next we will divide the images and bounding boxes into two groups of data namely <code>train_records</code> and <code>valid_records</code>.
By default, the split will be <code>80:20</code> to <code>train:valid</code> proportion.
You can change the ratio by altering the value in <code>RandomSplitter</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_records, valid_records <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse(data_splitter<span style=color:#f92672>=</span>RandomSplitter([<span style=color:#ae81ff>0.8</span>, <span style=color:#ae81ff>0.2</span>])
</code></pre></div><p>If you would like to check what the class names from the parsed data try running</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>parser<span style=color:#f92672>.</span>class_map
</code></pre></div><p>It should output</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>&lt;</span>ClassMap: {<span style=color:#e6db74>&#39;background&#39;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#39;Microalgae&#39;</span>: <span style=color:#ae81ff>1</span>}<span style=color:#f92672>&gt;</span>
</code></pre></div><p>which shows the <code>ClassMap</code> that contains the class name as the key and class number as the value in a Python <a href=https://www.w3schools.com/python/python_dictionaries.asp>dictionary</a>.
The <code>background</code> class is automatically added.
In Step 2 we do not need to label the background.</p><p>Next, we will apply basic data augmentation which is a technique used to diversify the training images by applying random transformation.
Learn more <a href=https://medium.com/analytics-vidhya/image-augmentation-9b7be3972e27>here</a>.</p><p>The following code specifies the kinds of transformation we would like to perform on our images.
Behind the scenes these transformations are performed with the <a href=https://albumentations.ai/>Albumentations</a> library.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>image_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>640</span>
train_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>aug_tfms(size<span style=color:#f92672>=</span>image_size, presize<span style=color:#f92672>=</span>image_size<span style=color:#f92672>+</span><span style=color:#ae81ff>128</span>), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
valid_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>resize_and_pad(image_size), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
</code></pre></div><p>We must specify the dimensions of the image in <code>image_size = 640</code>.
This value will then be used in <code>tfms.A.aug_tfms</code> that ensures that all images are resized to a <code>640x640</code> resolution and normalized in <code>tfms.A.Normalize()</code>.</p><p>Some models like <code>EfficientDet</code> only works with image size divisible by <code>128</code>.
Other common values you may try are <code>384</code>,<code>512</code>,<code>768</code>, etc.
But beware using large image size may consume more memory and in some cases halts training.
Starting with a small value like <code>384</code> is probably a good idea.
I found that for this blog post <code>640</code> works best.</p><p>Use <code>tfms.A.aug_tfms</code> also performs other transformations to the image such as varying the lighting, rotation, shifting, flipping, blurring, padding, etc.
The full list of transforms that and the arguments can be found in the <code>aug_tfms</code> <a href=https://airctic.com/0.12.0/albumentations_tfms/>documentation</a>.</p><p>In this code snippet we created two distinct transforms namely <code>train_tfms</code> and <code>valid_tfms</code> that will be used during the training and validation steps.</p><p>Next, we will apply the <code>train_tfms</code> to our <code>train_records</code> and <code>valid_tfms</code> to <code>valid_records</code> with the following snippet.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_ds <span style=color:#f92672>=</span> Dataset(train_records, train_tfms)
valid_ds <span style=color:#f92672>=</span> Dataset(valid_records, valid_tfms)
</code></pre></div><p>This results in the creation of a <code>Dataset</code> object which is a collection of transformed images and bounding boxes.</p><p>To visualize the <code>train_ds</code> we can run</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>samples <span style=color:#f92672>=</span> [train_ds[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
show_samples(samples, ncols<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</code></pre></div><p>This will show us 4 samples from the <code>train_ds</code>.
Note the variations in lighting, translation, and rotation compared to the original images.<figure><img src=/blog/training_dl_model_for_cell_counting/show_ds.png srcset="/blog/training_dl_model_for_cell_counting/show_ds_hu987ace790d19d4102da18bd5db01d3ee_648447_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/show_ds_hu987ace790d19d4102da18bd5db01d3ee_648447_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/show_ds_hu987ace790d19d4102da18bd5db01d3ee_648447_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>The transformations are applied on-the-fly.
So each run on the snippet produces different results.</p><h4 id=choosing-library-model-and-backbone>Choosing library, model and backbone</h4><p>IceVision supports hundreds of high-quality pre-trained models from <a href=https://github.com/pytorch/vision>Torchvision</a>, Open MMLab&rsquo;s <a href=https://github.com/open-mmlab/mmdetection>MMDetection</a>, Ultralytic&rsquo;s <a href=https://github.com/ultralytics/yolov5>YOLOv5</a> and Ross Wightman&rsquo;s <a href=https://github.com/rwightman/efficientdet-pytorch>EfficientDet</a>.</p><p>Depending on your preference, you may choose the model and backbone from these libraries.
In this post I will choose the <a href=https://arxiv.org/abs/2008.13367>VarifocalNet</a> (VFNet) model from MMDetection which can be accomplished with</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_type <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>mmdet<span style=color:#f92672>.</span>vfnet
backbone <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>backbones<span style=color:#f92672>.</span>resnet50_fpn_mstrain_2x
model <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>model(backbone<span style=color:#f92672>=</span>backbone(pretrained<span style=color:#f92672>=</span>True), num_classes<span style=color:#f92672>=</span>len(parser<span style=color:#f92672>.</span>class_map)) 
</code></pre></div><p>There are various ResNet backbones that you can select from such as
<code>resnet50_fpn_1x</code>,
<code>resnet50_fpn_mstrain_2x</code>,
<code>resnet50_fpn_mdconv_c3_c5_mstrain_2x</code>,
<code>resnet101_fpn_1x</code>,
<code>resnet101_fpn_mstrain_2x</code>,
<code>resnet101_fpn_mdconv_c3_c5_mstrain_2x</code>,
<code>resnext101_32x4d_fpn_mdconv_c3_c5_mstrain_2x</code>, and
<code>resnext101_64x4d_fpn_mdconv_c3_c5_mstrain_2x</code>.</p><p>Additionally, IceVsion also recently supports state-of-the-art Swin Transformer backbone for the VFNet model
<code>swin_t_p4_w7_fpn_1x_coco</code>,
<code>swin_s_p4_w7_fpn_1x_coco</code>, and
<code>swin_b_p4_w7_fpn_1x_coco</code>.</p><p>Which combination of <code>model_type</code> and <code>backbone</code> that performs best is something you need to experiment with.
Feel free to experiment and swap out the backbone and note the performance of the model.</p><h4 id=metrics-and-training>Metrics and Training</h4><p>In order to start the training, the model needs to ingest the images and bounding boxes from the <code>train_ds</code> and <code>valid_ds</code> we created.
This is the role that dataloaders play.</p><p>We will therefore need to construct the dataloaders from the <code>train_ds</code> and <code>valid_ds</code> respectively</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>train_dl(train_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>True)
valid_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>valid_dl(valid_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>False)
</code></pre></div><p>Here, we can specify the <code>batch_size</code> which is the number of images and bounding boxes to be passed to the model in a single forward pass.
The <code>batch_size</code> is a hyperparameter that be tuned to improve performance.
The <code>num_workers</code> argument specifies the number of CPU cores to be used - the more cores, the faster.</p><p>Next, we need to specify the metric we use for the training.
Metric is a measure of how good the model is at the task we are trying to train the model for.
Some commonly used metrics include accuracy, F1 Score, etc.
For object detection tasks the <code>COCOMetric</code> is commonly used.</p><p>One of the most important hyperparameter to get right is the learning rate.
Since IceVision is built to work with Fastai, we have access to a handy tool known as the learning rate finder first proposed by Leslie Smith and popularized by the Fastai community for its effectiveness.
This is an incredibly easy to use tool to find a range of optimal learning rate with this dataset.</p><p>All we need to do is run</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>learn<span style=color:#f92672>.</span>lr_find()
</code></pre></div><p>which outputs<figure><img src=/blog/training_dl_model_for_cell_counting/lr_find.png srcset="/blog/training_dl_model_for_cell_counting/lr_find_hu6561684f31e2a74860f9f6c313a6ebd2_21303_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/lr_find_hu6561684f31e2a74860f9f6c313a6ebd2_21303_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/lr_find_hu6561684f31e2a74860f9f6c313a6ebd2_21303_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure></p><p>The most optimal learning rate value is where the loss descends most rapidly as can be seen in values between <code>1e-4</code> to <code>1e-3</code>.
The orange dot on the plot shows the point where the slope is the steepest.</p><p>With this learning rate value, we can pass it into the fine_tune function to start training.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>metrics <span style=color:#f92672>=</span> [COCOMetric(metric_type<span style=color:#f92672>=</span>COCOMetricType<span style=color:#f92672>.</span>bbox)]
learn <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>fastai<span style=color:#f92672>.</span>learner(dls<span style=color:#f92672>=</span>[train_dl, valid_dl], model<span style=color:#f92672>=</span>model, metrics<span style=color:#f92672>=</span>metrics)
learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>3e-4</span>, freeze_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</code></pre></div><p>The first argument to in <code>fine_tune</code> is the number of epochs to train for. In this post I will train for 10 epochs for demonstration.
Training for longer will likely to improve the model.
The second argument is the base learning rate value which we found using the <code>learn.lr_find()</code></p><p>The above code snippet trains the model for 10 <code>epochs</code>.
The <code>freeze_epochs</code> specifies the number of <code>epochs</code> to train while the backbone of the model is frozen.</p><p>The figure below shows the training output.
In ‚ûÄ, only the last layer of the model was trained.
The remaining parts of the model are frozen.
In ‚ûÅ, the entire mode is trained end-to-end.</p><figure><img src=/blog/training_dl_model_for_cell_counting/train.png srcset="/blog/training_dl_model_for_cell_counting/train_hu2226ca9d5c8f11c1359a7df633969155_44566_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/train_hu2226ca9d5c8f11c1359a7df633969155_44566_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/train_hu2226ca9d5c8f11c1359a7df633969155_44566_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><p>During the training, the <code>train_loss</code>, <code>valid_loss</code> and <code>COCOMetric</code>is printed every epoch.
Ideally the losses should decrease and <code>COCOMetric</code> increase the longer we train.
As shown above, each epoch only took 2 seconds to complete on a GPU - which is incredibly fast.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_type<span style=color:#f92672>.</span>show_results(model, valid_ds, detection_threshold<span style=color:#f92672>=.</span><span style=color:#ae81ff>5</span>)
</code></pre></div><figure><img src=/blog/training_dl_model_for_cell_counting/show_results.png srcset="/blog/training_dl_model_for_cell_counting/show_results_huccd0cb8b6eb6e7b65df90b17e113aa5d_298745_360x0_resize_box_2.png 360w, /blog/training_dl_model_for_cell_counting/show_results_huccd0cb8b6eb6e7b65df90b17e113aa5d_298745_720x0_resize_box_2.png 720w, /blog/training_dl_model_for_cell_counting/show_results_huccd0cb8b6eb6e7b65df90b17e113aa5d_298745_1920x0_resize_box_2.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"></figure><p>For completeness here are all the codes in Step 3 to load the data, instantiate the model, training and showing the results.
That is only 17 lines of code if you remove the spaces in between!</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.all <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>

parser <span style=color:#f92672>=</span> parsers<span style=color:#f92672>.</span>VOCBBoxParser(annotations_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;images/labeled&#34;</span>, images_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;images/labeled&#34;</span>)
train_records, valid_records <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse()

image_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>640</span>
train_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>aug_tfms(size<span style=color:#f92672>=</span>image_size, presize<span style=color:#f92672>=</span>image_size<span style=color:#f92672>+</span><span style=color:#ae81ff>128</span>), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])
valid_tfms <span style=color:#f92672>=</span> tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Adapter([<span style=color:#f92672>*</span>tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>resize_and_pad(image_size), tfms<span style=color:#f92672>.</span>A<span style=color:#f92672>.</span>Normalize()])

train_ds <span style=color:#f92672>=</span> Dataset(train_records, train_tfms)
valid_ds <span style=color:#f92672>=</span> Dataset(valid_records, valid_tfms)

model_type <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>mmdet<span style=color:#f92672>.</span>vfnet
backbone <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>backbones<span style=color:#f92672>.</span>resnet50_fpn_mstrain_2x
model <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>model(backbone<span style=color:#f92672>=</span>backbone(pretrained<span style=color:#f92672>=</span>True), num_classes<span style=color:#f92672>=</span>len(parser<span style=color:#f92672>.</span>class_map)) 

train_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>train_dl(train_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>True)
valid_dl <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>valid_dl(valid_ds, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, shuffle<span style=color:#f92672>=</span>False)

metrics <span style=color:#f92672>=</span> [COCOMetric(metric_type<span style=color:#f92672>=</span>COCOMetricType<span style=color:#f92672>.</span>bbox)]
learn <span style=color:#f92672>=</span> model_type<span style=color:#f92672>.</span>fastai<span style=color:#f92672>.</span>learner(dls<span style=color:#f92672>=</span>[train_dl, valid_dl], model<span style=color:#f92672>=</span>model, metrics<span style=color:#f92672>=</span>metrics)
learn<span style=color:#f92672>.</span>fine_tune(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>3e-4</span>, freeze_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)

model_type<span style=color:#f92672>.</span>show_results(model, valid_ds, detection_threshold<span style=color:#f92672>=.</span><span style=color:#ae81ff>5</span>)
</code></pre></td></tr></table></div></div><h4 id=exporting-model>Exporting model</h4><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> icevision.models.checkpoint <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
save_icevision_checkpoint(model,
                        model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mmdet.vfnet&#39;</span>, 
                        backbone_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;resnet50_fpn_mstrain_2x&#39;</span>,
                        img_size<span style=color:#f92672>=</span>image_size,
                        classes<span style=color:#f92672>=</span>parser<span style=color:#f92672>.</span>class_map<span style=color:#f92672>.</span>get_classes(),
                        filename<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./models/model_checkpoint.pth&#39;</span>,
                        meta<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;icevision_version&#39;</span>: <span style=color:#e6db74>&#39;0.12.0&#39;</span>})
</code></pre></div><h3 id=-step-4-inferencing-on-a-new-image>üß≠ Step 4: Inferencing on a new image</h3><p>Inference on a local machine
[inference.png not found]</p><p>Figure illustrates the raw detection of cells from microscope image. The model is a RetinaNet with a ResNet50 backbone trained using <a href=https://github.com/airctic/icevision>IceVision</a>.
[detection.png not found]</p><section class=social-share><ul class=share-icons><hr><h5>Share this post</h5>><li><a href="https://twitter.com/intent/tweet?&url=https%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f&text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code" target=_blank rel=noopener aria-label="Share on Twitter" class="share-btn twitter"><svg width="6.3503098mm" height="5.1592798mm" viewBox="0 0 6.3503098 5.1592799" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="twitter_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-38.14813" inkscape:cy="32.360138" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-125.63688,-104.07795)"><path d="m131.98686 104.68815c-.23316.10418-.48452.17363-.74745.20505.26955-.1604.47625-.41672.57216-.72099-.25135.14883-.53082.25797-.82682.31585-.23812-.25136-.57712-.41011-.95084-.41011-.71934.0-1.30308.58374-1.30308 1.30307.0.10253.0116.20175.0331.29601-1.08314-.0546-2.04226-.57216-2.68553-1.36095-.11079.19182-.17528.41672-.17528.65484.0.45145.22985.84997.57877 1.08479-.21332-.007-.41506-.0661-.59035-.16371v.0165c0 .63169.44979 1.15755 1.04511 1.27661-.10914.0298-.2249.0463-.34396.0463-.0843.0-.16537-.008-.24474-.0232.16536.51759.64657.89463 1.21708.90455-.44648.34892-1.00707.55728-1.61726.55728-.10584.0-.20836-.007-.31089-.0182.57712.37041 1.26173.58539 1.9976.58539 2.39614.0 3.70583-1.98438 3.70583-3.70582.0-.0562-.002-.11245-.003-.16868.25466-.18355.47459-.41341.64988-.67468z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g></svg><p>Twitter</p></a></li>&nbsp;<li><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f" target=_blank rel=noopener aria-label="Share on Facebook" class="share-btn facebook"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="facebook_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-7.8050197" inkscape:cy="32.710925" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.36281,-104.14567)"><path d="m130.36281 104.72294v5.19545c0 .3157.26158.57728.57727.57728h5.19546c.3157.0.57727-.26158.57727-.57728v-5.19545c0-.3157-.26157-.57727-.57727-.57727h-5.19546c-.31569.0-.57727.26157-.57727.57727zm5.77273.0v5.19545h-1.4973v-1.94829h.74865l.10824-.86591h-.85689v-.55923c0-.25256.0631-.42394.42393-.42394h.46904v-.78473c-.0794-.0108-.35719-.0271-.67649-.0271-.66567.0-1.11847.40048-1.11847 1.14553v.64943h-.75767v.86591h.75767v1.94829h-2.79617v-5.19545z" id="path1085" style="stroke-width:.0180398;fill:#fff"/></g></svg><p>Facebook</p></a></li>&nbsp;<li><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f&source=https%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f&title=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code&summary=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code" target=_blank rel=noopener aria-label="Share on LinkedIn" class="share-btn linkedin"><svg width="6.3499999mm" height="6.3499999mm" viewBox="0 0 6.3499999 6.3499999" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="linkedin_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.2264764" inkscape:cy="40.603642" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-129.66672,-101.87176)"><path d="m129.66672 102.59335v4.90682c0 .39507.32652.72159.72159.72159h4.90682c.39507.0.72159-.32652.72159-.72159v-4.90682c0-.39507-.32652-.72159-.72159-.72159h-4.90682c-.39507.0-.72159.32652-.72159.72159zm5.62841-.14432c.083.0.14432.0613.14432.14432v4.90682c0 .083-.0613.14431-.14432.14431h-4.90682c-.083.0-.14432-.0613-.14432-.14431v-4.90682c0-.083.0613-.14432.14432-.14432zm-4.55504.99219c0 .2742.22189.49609.49609.49609.27421.0.4961-.22189.4961-.49609.0-.27421-.22189-.4961-.4961-.4961-.2742.0-.49609.22189-.49609.4961zm2.30007 1.26278h-.018v-.37883h-.81179v2.74204h.84787v-1.35298c0-.35719.0703-.70355.51413-.70355.43657.0.44198.40409.44198.72159v1.33494h.84787v-1.50632c0-.73783-.15695-1.29886-1.01925-1.29886-.41492.0-.68912.2273-.80277.44197zm-2.21889 2.36321h.85689v-2.74204h-.85689z" id="path1321" style="stroke-width:.0180398;fill:#fff"/></g></svg><p>LinkedIn</p></a></li>&nbsp;<li><a href="https://telegram.me/share/url?text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code&url=https%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f" target=_blank rel=noopener aria-label="Share on Telegram" class="share-btn telegram"><svg width="7.3503098mm" height="7.1592798mm" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 189.473 189.473" style="enable-background:new 0 0 189.473 189.473"><g><path d="M152.531 179.476c-1.48.0-2.95-.438-4.211-1.293l-47.641-32.316-25.552 18.386c-2.004 1.441-4.587 1.804-6.914.972-2.324-.834-4.089-2.759-4.719-5.146l-12.83-48.622L4.821 93.928c-2.886-1.104-4.8-3.865-4.821-6.955-.021-3.09 1.855-5.877 4.727-7.02l174.312-69.36c.791-.336 1.628-.53 2.472-.582.302-.018.605-.018.906-.001 1.748.104 3.465.816 4.805 2.13.139.136.271.275.396.42 1.11 1.268 1.72 2.814 1.835 4.389.028.396.026.797-.009 1.198-.024.286-.065.571-.123.854L159.898 173.38c-.473 2.48-2.161 4.556-4.493 5.523C154.48 179.287 153.503 179.476 152.531 179.476zm-47.669-48.897 42.437 28.785L170.193 39.24l-82.687 79.566 17.156 11.638C104.731 130.487 104.797 130.533 104.862 130.579zM69.535 124.178l5.682 21.53 12.242-8.809-16.03-10.874C70.684 125.521 70.046 124.893 69.535 124.178zM28.136 86.782l31.478 12.035c2.255.862 3.957 2.758 4.573 5.092l3.992 15.129c.183-1.745.974-3.387 2.259-4.624L149.227 38.6 28.136 86.782z" id="path1039" style="fill:#fff;stroke-width:.0165365"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg><p>Twitter</p></a></li>&nbsp;<li><a href="whatsapp://send?text=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLeveraging%20state-of-the-art%20models%20on%20IceVision%20and%20Fastai%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f%0a" target=_blank aria-label="Share on WhatsApp" class="share-btn whatsapp"><svg width="6.0324998mm" height="6.05896mm" viewBox="0 0 6.0324997 6.05896" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="whatsapp_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="4.9987205" inkscape:cy="35.692618" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-126.67735,-103.17712)"><path d="m131.83672 104.07671c-.58208-.58209-1.34937-.89959-2.14312-.89959-1.64042.0-2.98979 1.34938-2.98979 3.01625.0.52917.13229 1.03188.39687 1.48167l-.42333 1.56104 1.5875-.42333c.42333.23812.92604.34396 1.42875.34396 1.66687.0 3.01625-1.34938 3.01625-3.01625-.0265-.74084-.3175-1.50813-.87313-2.06375zm-2.14312 4.6302c-.44979.0-.87313-.13229-1.27-.34395l-.10583-.0529-.92605.26458.26459-.89958-.0794-.13229c-.26458-.39688-.37041-.84667-.37041-1.34938.0-1.37583 1.11125-2.48708 2.48708-2.48708.66146.0 1.29646.26458 1.77271.74083.47625.47625.74083 1.11125.74083 1.77271-.0265 1.37583-1.13771 2.48708-2.51354 2.48708zm1.34937-1.87854c-.0794-.0265-.44979-.23812-.5027-.26458-.0794-.0265-.1323-.0265-.18521.0265-.0529.0794-.21167.26458-.23813.29104-.0529.0529-.0794.0529-.15875.0265-.0794-.0265-.3175-.13229-.60854-.37042-.23812-.21167-.37042-.44979-.42333-.52917-.0529-.0794.0-.13229.0265-.15875.0265-.0264.0794-.0794.10583-.13229.0529-.0265.0794-.0794.10583-.13229.0265-.0529.0-.10583.0-.13229.0-.0265-.18521-.39688-.26458-.55563-.0265-.10583-.10583-.0794-.13229-.0794h-.15875s-.10584.0265-.18521.0794c-.0794.0794-.26458.26459-.26458.635.0.37042.26458.74084.29104.79375.0265.0529.52916.82021 1.29646 1.13771.1852.0794.3175.13229.42333.15875.18521.0529.34396.0529.47625.0265.15875-.0265.44979-.18521.50271-.34396.0529-.18521.0529-.3175.0529-.34396-.0264-.0794-.0794-.10583-.15875-.13229z" id="path1793" style="stroke-width:.264583;fill:#fff"/></g></svg><p>Email</p></a></li>&nbsp;<li><a href="mailto:?subject=Dickson%20Neoh%20-%20Personal%20Portfolio - Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code.&body=Training%20a%20Deep%20Learning%20Model%20for%20Cell%20Counting%20in%2017%20Lines%20of%20Code%2c%20by%20Dickson%20Neoh%20-%20Personal%20Portfolio%0aLeveraging%20state-of-the-art%20models%20on%20IceVision%20and%20Fastai%0a%0ahttps%3a%2f%2fdicksonneoh.com%2fblog%2ftraining_dl_model_for_cell_counting%2f%0a" target=_blank class="share-btn email" aria-label="Share via Email"><svg width="6.3499999mm" height="4.3961601mm" viewBox="0 0 6.3499999 4.3961601" id="svg5" inkscape:version="1.1.2 (b8e25be833, 2022-02-05)" sodipodi:docname="email_white.svg" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><sodipodi:namedview id="namedview7" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" inkscape:document-units="mm" showgrid="false" inkscape:zoom="5.701459" inkscape:cx="-6.7526575" inkscape:cy="33.4125" inkscape:window-width="1920" inkscape:window-height="972" inkscape:window-x="1920" inkscape:window-y="1107" inkscape:window-maximized="1" inkscape:current-layer="layer1"/><defs id="defs2"/><g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1" transform="translate(-130.10375,-103.97942)"><path d="m130.10375 104.22365v3.9077.24423h.24423 5.86154.24423v-.24423-3.9077-.24423h-.24423-5.86154-.24423zm5.29675.24423-2.12175 1.41196-2.12175-1.41196zm-2.25913 1.91569.13738.0839.13738-.0839 2.54916-1.70198v3.20553h-5.37308v-3.20553z" id="path824" style="stroke-width:.0152644;fill:#fff"/></g></svg><p>Email</p></a></li><hr></ul></section></div></div></div><div class=row><div class="col-lg-10 offset-lg-1"><nav class="case-details-nav d-flex justify-content-between align-items-center"><div class="previous d-flex align-items-center"><div class="icon mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="15.556" height="28.285" viewBox="0 0 15.556 28.285"><g data-name="Group 1243" fill="#2d2d2d"><path data-name="Path 1456" d="M3.391 12.728l9.75 14.142-.982 1.414-9.742-14.142z"/><path data-name="Path 1455" d="M13.137 1.41 3.39 15.558l-.975-1.415L12.166.0z"/></g></svg></div><div class=content><span class=small>Prev blog</span><h5 class=title><a class=text-dark href=https://dicksonneoh.com/blog/contributing_to_open_source_lessons_learned/>Contributing to open-source: Lessons learned</a></h5></div></div></nav></div></div></div></section></div><section class=footer id=contact><div class=footer__background_shape><svg viewBox="0 0 1920 79"><path d="M0 0h1920v79L0 0z" data-name="Path 1450"/></svg></div><div class=container><div class=row><div class=col-lg-12><div class=footer__cta><div class=shape-1><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class=shape-2><svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029"><path data-name="Path 1449" d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z" transform="translate(217.489 188.626)"/></svg></div><div class="text-light footer__cta_content"><span>Contact me</span><h2 class=mb-0>Let‚Äôs Start a Project</h2></div><div class=footer__cta_action><a class="btn btn-light btn-zoom" href=https://dicksonneoh.com/contact>Get in
touch</a></div></div></div></div><div class="row footer__widget"><div class=col-lg-4><div class="footer__widget_logo mb-5"><img src=https://dicksonneoh.com/images/site-navigation/logo_dn_resize.png alt=widget-logo></div></div><div class=col-lg-4><div class="text-light footer__widget_sitemap mb-5"><h4 class=base-font>Sitemap</h4><ul class="unstyle-list small"><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>About me</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Frequently Ask Question</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Privacy & Policy</a></li><li class=mb-2><a class=text-light href=https://dicksonneoh.com/>Latest Article</a></li></ul></div></div><div class=col-lg-4><div class="text-light footer__widget_address mb-5"><h4 class=base-font>Address</h4><ul class="fa-ul small"><li class=mb-2><a class=text-light href=tel:+%2860%29%203%208921%202020><span class=fa-li><i class="fa fa-phone"></i></span>+(60) 3 8921 2020</a></li><li class=mb-2><a class=text-light href=mailto:dickson.neoh@gmail.com><span class=fa-li><i class="fa fa-envelope"></i></span>dickson.neoh@gmail.com</a></li><li class=mb-2><span class=fa-li><i class="fa fa-map-marker"></i></span>Universiti Tenaga Nasional, 43000, Kajang, Malaysia.</a></li></ul></div></div></div><div class="row footer__footer"><div class=col-lg-6><div class="footer__footer_copy text-light"><p>All right reserved copyright ¬© Dickson Neoh 2022</p></div></div><div class=col-lg-6><div class=footer__footer_social><ul class=unstyle-list><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://www.linkedin.com/in/dickson-neoh-3a6984b8/><i class="fa fa-linkedin-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://twitter.com/dicksonneoh7><i class="fa fa-twitter-square"></i></a></li><li class="d-inline-block mx-2"><a class=text-light target=_blank href=https://github.com/dnth><i class="fa fa-github-square"></i></a></li></ul></div></div></div></div></section><script src="https://maps.googleapis.com/maps/api/js?key=YOUR%20GOOGLE%20MAP%20API&libraries=geometry"></script><script src=https://dicksonneoh.com/plugins/jQuery/jquery.min.js></script><script src=https://dicksonneoh.com/plugins/bootstrap/bootstrap.min.js></script><script src=https://dicksonneoh.com/plugins/slick/slick.min.js></script><script src=https://dicksonneoh.com/plugins/waypoint/jquery.waypoints.min.js></script><script src=https://dicksonneoh.com/plugins/magnafic-popup/jquery.magnific-popup.min.js></script><script src=https://dicksonneoh.com/plugins/tweenmax/TweenMax.min.js></script><script src=https://dicksonneoh.com/plugins/imagesloaded/imagesloaded.min.js></script><script src=https://dicksonneoh.com/plugins/masonry/masonry.min.js></script><script src=https://dicksonneoh.com/js/form-handler.min.js></script><script src=https://dicksonneoh.com/js/script.min.js></script></body></html>