<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TIMM on Dickson Neoh - Personal Portfolio</title><link>https://dicksonneoh.com/tags/timm/</link><description>Recent content in TIMM on Dickson Neoh - Personal Portfolio</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 18 Apr 2022 11:00:15 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/tags/timm/index.xml" rel="self" type="application/rss+xml"/><item><title>PyTorch at the Edge: Deploying Over 964 TIMM Models on Android with TorchScript and Flutter</title><link>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</link><pubDate>Mon, 18 Apr 2022 11:00:15 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/</guid><description>ðŸ”¥ Motivation You finally got into a Kaggle competition. You found a getting-started notebook written by a Kaggle Grandmaster and immediately trained a state-of-the-art (SOTA) image classification model.
After some fiddling, you found yourself in the leaderboard topping the charts with 99.9851247% accuracy on the test set ðŸ˜Ž!
Proud of your achievement you reward yourself to some rest and a good night&amp;rsquo;s sleep.
And then..
With various high level libraries like Keras, Transformer and Fastai, the barrier to SOTA models have never been lower.</description></item></channel></rss>