<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorRT on Dickson Neoh - Personal Portfolio</title><link>https://dicksonneoh.com/tags/tensorrt/</link><description>Recent content in TensorRT on Dickson Neoh - Personal Portfolio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 09 Sep 2024 09:00:00 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/tags/tensorrt/index.xml" rel="self" type="application/rss+xml"/><item><title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime &amp; Optimizations</title><link>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</link><pubDate>Mon, 09 Sep 2024 09:00:00 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</guid><description>ðŸš€ Motivation Having real time inference is crucial for many computer vision applications. In some domain, a 1-second delay in inference could mean life or death.
Imagine you&amp;rsquo;re sitting in a self-driving car and the car takes one full second to detect an oncoming truck.
Just one second too late, and you could end up in the clouds ðŸ‘¼ðŸ‘¼ðŸ‘¼
Or if youâ€™re really lucky, you get a very up-close view of the pavement.</description></item></channel></rss>