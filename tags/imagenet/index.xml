<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ImageNet on Dickson Neoh - Personal Portfolio</title><link>https://dicksonneoh.com/tags/imagenet/</link><description>Recent content in ImageNet on Dickson Neoh - Personal Portfolio</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 09 Sep 2024 09:00:00 +0800</lastBuildDate><atom:link href="https://dicksonneoh.com/tags/imagenet/index.xml" rel="self" type="application/rss+xml"/><item><title>Supercharge Your PyTorch Image Models: Bag of Tricks to 8x Faster Inference with ONNX Runtime &amp; Optimizations</title><link>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</link><pubDate>Mon, 09 Sep 2024 09:00:00 +0800</pubDate><guid>https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/</guid><description>ðŸš€ Motivation Real time inference speed is crucial for many applications in production. Some could mean life or death. ðŸ’€
Imagine you&amp;rsquo;re behind the wheels of a self-driving car and the car takes one second to detect an oncoming truck.
Just one second too late, and you could end up in the clouds ðŸ‘¼ðŸ‘¼ðŸ‘¼
Or if you&amp;rsquo;re lucky, on the ground.
I hope that shows you how crucial this problem is.</description></item></channel></rss>